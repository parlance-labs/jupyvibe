---
title: LiteLLM multimodal image support discovery
threadId: T-019b72d6-f13f-75bd-811b-e34bc8c75e39
created: 2025-12-31T05:17:27.743Z
agentMode: smart
---

# LiteLLM multimodal image support discovery

## User

<attached_files>
```file:///Users/hamel/git/ai-jup/ai_jup/handlers.py
1: """Server handlers for AI prompt processing."""
2: import json
3: import os
4: import re
5: 
6: from jupyter_server.base.handlers import APIHandler
7: from jupyter_server.utils import url_path_join
8: from tornado.web import authenticated
9: from tornado.iostream import StreamClosedError
10: 
... (493 more lines)
```
```file:///Users/hamel/git/ai-jup/src/tokens.ts
1: /**
2:  * Dependency injection tokens for ai-jup extension.
3:  * 
4:  * Following JupyterLab's plugin architecture pattern, these tokens
5:  * allow loose coupling between components and enable testing/mocking.
6:  */
7: 
8: import { Token } from '@lumino/coreutils';
9: import { ISignal } from '@lumino/signaling';
10: import { NotebookPanel } from '@jupyterlab/notebook';
... (263 more lines)
```
```file:///Users/hamel/git/ai-jup/src/promptModel.ts
1: /**
2:  * Signal-based prompt model for streaming AI responses.
3:  * 
4:  * Uses Lumino Signals for reactive updates, following jupyter-ai patterns.
5:  */
6: 
7: import { Signal, ISignal } from '@lumino/signaling';
8: import { PageConfig } from '@jupyterlab/coreutils';
9: import type {
10:   IPromptModel,
... (270 more lines)
```
```file:///Users/hamel/git/ai-jup/src/promptCell.ts
1: /**
2:  * Prompt cell management and execution.
3:  */
4: 
5: import { NotebookPanel, NotebookActions } from '@jupyterlab/notebook';
6: import { Cell, ICellModel, MarkdownCell } from '@jupyterlab/cells';
7: import type {
8:   IPromptCellManager,
9:   IKernelConnector,
10:   IPromptModel,
... (477 more lines)
```
```file:///Users/hamel/git/ai-jup/src/kernelConnector.ts
1: /**
2:  * Kernel connector for variable and function introspection.
3:  */
4: 
5: import { ISessionContext } from '@jupyterlab/apputils';
6: import { KernelMessage } from '@jupyterlab/services';
7: import {
8:   IKernelConnector,
9:   IKernelConnectorFactory,
10:   IVariableInfo,
... (257 more lines)
```
```file:///Users/hamel/git/ai-jup/src/index.ts
1: /**
2:  * AI-powered Jupyter Lab extension with prompt cells.
3:  * 
4:  * Features:
5:  * - $variable syntax to reference kernel variables in prompts
6:  * - &function syntax to give AI access to kernel functions as tools
7:  * - Prompt cells that see all preceding cells and kernel state
8:  */
9: 
10: import {
... (248 more lines)
```
```file:///Users/hamel/git/ai-jup/schema/plugin.json
1: {
2:   "title": "AI-Jup Settings",
3:   "description": "Settings for the AI-Jup extension",
4:   "type": "object",
5:   "jupyter.lab.setting-icon": "ui-components:add",
6:   "jupyter.lab.setting-icon-label": "AI-Jup",
7:   "properties": {
8:     "defaultModel": {
9:       "type": "string",
10:       "title": "Default Model",
... (36 more lines)
```
</attached_files>

Continuing work from thread T-019b72c1-da2a-76ff-8900-d3c7d1769322. When you lack specific information you can use read_thread to get it.

@ai_jup/handlers.py @src/tokens.ts @src/promptModel.ts @src/promptCell.ts @src/kernelConnector.ts @src/index.ts @schema/plugin.json

I refactored ai-jup to use a modular plugin architecture with dependency injection tokens following jupyter-ai patterns:

- Created tokens in src/tokens.ts for IKernelConnector, IPromptCellManager, IPromptModel, IExtensionSettings
- Split src/index.ts into multiple plugins: settingsPlugin, kernelConnectorPlugin, promptCellManagerPlugin, mainPlugin
- Created src/promptModel.ts with signal-based streaming using Lumino Signals (streamEvent, outputChanged, stateChanged)
- Created src/settings.ts with SettingsManager that integrates with JupyterLab's ISettingRegistry
- Created schema/plugin.json for settings UI (defaultModel, maxToolSteps, showConvertButton)

From studying jupyter-ai (github.com/jupyterlab/jupyter-ai), I learned:
- They use LiteLLM via jupyter-ai-litellm for provider abstraction supporting 100+ models
- Their architecture is highly modular with separate repos: jupyter-chat (UI), jupyter-ai-router (message routing), jupyter-ai-litellm (LLM abstraction)
- They have an abstract model pattern with base classes and registries for extensibility

Current LLM integration:
- Backend in ai_jup/handlers.py uses Anthropic SDK directly (hardcoded)
- PromptHandler._build_system_prompt() constructs prompts with preceding_code, variables, functions context
- Streaming via SSE with tool loop support (max_steps parameter)

For multimodal/image support, the key consideration is:
- Prompt cells gather context from preceding cells via _gatherContext() in src/promptCell.ts
- Currently only extracts code source text, not cell outputs or images
- Would need to capture image outputs from executed cells and pass to LLM in multimodal format

ok for the lite-llm integration idea that allows you to integrate whatever models you want, but its important to me that the model is multimodal, so that the llms can see images in previous cells.  Can we make sure this is happening?  I think this should be possible with lite-llm but not sure.  Please do some discovery first on this and let me know what you find.  You have somewhat of the scaffolding worked out potentially from studying the official jupyter-ai extension https://github.com/jupyterlab/jupyter-ai

## Assistant

I'll research LiteLLM's multimodal capabilities and how jupyter-ai handles image outputs from cells.

**Tool Use:** `web_search`

```json
{
  "objective": "Find LiteLLM documentation on multimodal image support - how to pass images in messages to vision-capable models like Claude, GPT-4V",
  "search_queries": [
    "litellm multimodal image vision",
    "litellm image input messages"
  ]
}
```

**Tool Use:** `librarian`

```json
{
  "query": "How does jupyter-ai handle image outputs from notebook cells? Look at how they extract and pass images to LLMs for multimodal support. I want to understand the code that captures cell outputs (especially images/plots) and sends them to the AI model.",
  "context": "I'm building a JupyterLab extension (ai-jup) with AI prompt cells. I need to understand how jupyter-ai captures image outputs from executed cells and passes them to vision-capable LLMs."
}
```

## User

*(interrupted)*

**Tool Result:** `toolu_vrtx_018vbtmSnegmZhXMGDX6kUM6`

```
[
  {
    "title": "Using Vision Models - LiteLLM",
    "url": "https://docs.litellm.ai/docs/completion/vision",
    "excerpts": [
      "[Skip to main content]()\n\n* [](/)\n* Guides\n* Using Vision Models\n\nOn this page\n\n# Using Vision Models\n\n## Quick Start [‚Äã]( \"Direct link to Quick Start\")\n\nExample passing images to a model\n\n* LiteLLMPython SDK\n* LiteLLM Proxy Server\n\n```\nimport  os   \n from  litellm  import  completion  \n   \n os . environ [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n # openai call  \n response  =  completion (  \n     model  = \"gpt-4-vision-preview\" ,  \n     messages = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : [  \n {  \n \"type\" : \"text\" ,  \n \"text\" : \"What‚Äôs in this image?\"  \n } ,  \n {  \n \"type\" : \"image_url\" ,  \n \"image_url\" : {  \n \"url\" : \"https://awsmp-logos.s3.amazonaws.com/seller-xw5kijmvmzasy/c233c9ade2ccb5491072ae232c814942.png\"  \n }  \n }  \n ]  \n }  \n ] ,  \n )\n```\n\n1. Define vision models on config.yaml\n\n```\nmodel_list :  \n - model_name :  gpt - 4 - vision - preview  # OpenAI gpt-4-vision-preview  \n litellm_params :  \n model :  openai/gpt - 4 - vision - preview  \n api_key :  os.environ/OPENAI_API_KEY  \n - model_name :  llava - hf           # Custom OpenAI compatible model  \n litellm_params :  \n model :  openai/llava - hf/llava - v1.6 - vicuna - 7b - hf  \n api_base :  http : //localhost : 8000  \n api_key :  fake - key  \n model_info :  \n supports_vision : True # set supports_vision to True so /model/info returns this attribute as True\n```\n\n1. Run proxy server\n\n```\nlitellm --config config.yaml\n```\n\n1. Test it using the OpenAI Python SDK\n\n```\nimport  os   \n from  openai  import  OpenAI  \n   \n client  =  OpenAI (  \n     api_key = \"sk-1234\" , # your litellm proxy api key  \n )  \n   \n response  =  client . chat . completions . create (  \n     model  = \"gpt-4-vision-preview\" , # use model=\"llava-hf\" to test your custom OpenAI endpoint  \n     messages = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : [  \n {  \n \"type\" : \"text\" ,  \n \"text\" : \"What‚Äôs in this image?\"  \n } ,  \n {  \n \"type\" : \"image_url\" ,  \n \"image_url\" : {  \n \"url\" : \"https://awsmp-logos.s3.amazonaws.com/seller-xw5kijmvmzasy/c233c9ade2ccb5491072ae232c814942.png\"  \n }  \n }  \n ]  \n }  \n ] ,  \n )\n```\n\n## Checking if a model supports `vision` [‚Äã]( \"Direct link to checking-if-a-model-supports-vision\")\n\n* LiteLLM Python SDK\n* LiteLLM Proxy Server\n\nUse `litellm.supports_vision(model=\"\")` \\-> returns `True` if model supports `vision` and `False` if not\n\n```\nassert  litellm . supports_vision ( model = \"openai/gpt-4-vision-preview\" ) == True  \n assert  litellm . supports_vision ( model = \"vertex_ai/gemini-1.0-pro-vision\" ) == True  \n assert  litellm . supports_vision ( model = \"openai/gpt-3.5-turbo\" ) == False  \n assert  litellm . supports_vision ( model = \"xai/grok-2-vision-latest\" ) == True  \n assert  litellm . supports_vision ( model = \"xai/grok-2-latest\" ) == False\n```\n\n1. Define vision models on config.yaml\n\n```\nmodel_list :  \n - model_name :  gpt - 4 - vision - preview  # OpenAI gpt-4-vision-preview  \n litellm_params :  \n model :  openai/gpt - 4 - vision - preview  \n api_key :  os.environ/OPENAI_API_KEY  \n - model_name :  llava - hf           # Custom OpenAI compatible model  \n litellm_params :  \n model :  openai/llava - hf/llava - v1.6 - vicuna - 7b - hf  \n api_base :  http : //localhost : 8000  \n api_key :  fake - key  \n model_info :  \n supports_vision : True # set supports_vision to True so /model/info returns this attribute as True\n```\n\n1. Run proxy server\n\n```\nlitellm --config config.yaml\n```\n\n1. Call `/model_group/info` to check if your model supports `vision`\n\n```\ncurl -X 'GET' \\  \n   'http://localhost:4000/model_group/info' \\  \n   -H 'accept: application/json' \\  \n   -H 'x-api-key: [REDACTED:api-key]'\n```\n\nExpected Response\n\n```\n{  \n \"data\" : [  \n {  \n \"model_group\" : \"gpt-4-vision-preview\" ,  \n \"providers\" : [ \"openai\" ] ,  \n \"max_input_tokens\" : 128000 ,  \n \"max_output_tokens\" : 4096 ,  \n \"mode\" : \"chat\" ,  \n \"supports_vision\" : true ,  # üëà supports_vision is  true  \n \"supports_function_calling\" : false  \n } ,  \n {  \n \"model_group\" : \"llava-hf\" ,  \n \"providers\" : [ \"openai\" ] ,  \n \"max_input_tokens\" : null ,  \n \"max_output_tokens\" : null ,  \n \"mode\" : null ,  \n \"supports_vision\" : true ,  # üëà supports_vision is  true  \n \"supports_function_calling\" : false  \n }  \n ]  \n }\n```\n\n## Explicitly specify image type [‚Äã]( \"Direct link to Explicitly specify image type\")\n\nIf you have images without a mime-type, or if litellm is incorrectly inferring the mime type of your image (e.g. calling `gs://` url's with vertex ai), you can set this explicitly via the `format` param.\n\n```\n\"image_url\" : {  \n \"url\" : \"gs://my-gs-image\" ,  \n \"format\" : \"image/jpeg\"  \n }\n```\n\nLiteLLM will use this for any API endpoint, which supports specifying mime-type (e.g. anthropic/bedrock/vertex ai).\n\nFor others (e.g. openai), it will be ignored.\n\n* SDK\n* PROXY\n\n```\nimport  os   \n from  litellm  import  completion  \n   \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n # openai call  \n response  =  completion (  \n     model  = \"claude-3-7-sonnet-latest\" ,  \n     messages = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : [  \n {  \n \"type\" : \"text\" ,  \n \"text\" : \"What‚Äôs in this image?\"  \n } ,  \n {  \n \"type\" : \"image_url\" ,  \n \"image_url\" : {  \n \"url\" : \"https://awsmp-logos.s3.amazonaws.com/seller-xw5kijmvmzasy/c233c9ade2ccb5491072ae232c814942.png\" ,  \n \"format\" : \"image/jpeg\"  \n }  \n }  \n ]  \n }  \n ] ,  \n )\n```\n\n1. Define vision models on config.yaml\n\n```\nmodel_list :  \n - model_name :  gpt - 4 - vision - preview  # OpenAI gpt-4-vision-preview  \n litellm_params :  \n model :  openai/gpt - 4 - vision - preview  \n api_key :  os.environ/OPENAI_API_KEY  \n - model_name :  llava - hf           # Custom OpenAI compatible model  \n litellm_params :  \n model :  openai/llava - hf/llava - v1.6 - vicuna - 7b - hf  \n api_base :  http : //localhost : 8000  \n api_key :  fake - key  \n model_info :  \n supports_vision : True # set supports_vision to True so /model/info returns this attribute as True\n```\n\n1. Run proxy server\n\n```\nlitellm --config config.yaml\n```\n\n1. Test it using the OpenAI Python SDK\n\n```\nimport  os   \n from  openai  import  OpenAI  \n   \n client  =  OpenAI (  \n     api_key = \"sk-1234\" , # your litellm proxy api key  \n )  \n   \n response  =  client . chat . completions . create (  \n     model  = \"gpt-4-vision-preview\" , # use model=\"llava-hf\" to test your custom OpenAI endpoint  \n     messages = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : [  \n {  \n \"type\" : \"text\" ,  \n \"text\" : \"What‚Äôs in this image?\"  \n } ,  \n {  \n \"type\" : \"image_url\" ,  \n \"image_url\" : {  \n \"url\" : \"https://awsmp-logos.s3.amazonaws.com/seller-xw5kijmvmzasy/c233c9ade2ccb5491072ae232c814942.png\" ,  \n \"format\" : \"image/jpeg\"  \n }  \n }  \n ]  \n }  \n ] ,  \n )\n```\n\n## Spec [‚Äã]( \"Direct link to Spec\")\n\n```\n\"image_url\": str  \n   \n OR   \n   \n \"image_url\": {  \n   \"url\": \"url OR base64 encoded str\",  \n   \"detail\": \"openai-only param\",   \n   \"format\": \"specify mime-type of image\"  \n }\n```\n\n[Previous Provider-specific Params](/docs/completion/provider_specific_params) [Next Exception Mapping](/docs/exception_mapping)\n\n* [Quick Start]()\n* [Checking if a model supports `vision`]()\n* [Explicitly specify image type]()\n* [Spec]()\n\nDocs\n\n* [Getting Started](https://docs.litellm.ai/docs/)\n\nCommunity\n\n* [Discord](https://discord.com/invite/wuPM9dRgDw)\n* [Twitter](https://twitter.com/LiteLLM)\n\nMore\n\n* [GitHub](https://github.com/BerriAI/litellm/)\n\nCopyright ¬© 2025 liteLLM\n"
    ]
  },
  {
    "title": "LiteLLM - Getting Started | liteLLM",
    "url": "https://docs.litellm.ai/docs/",
    "excerpts": [
      "[Skip to main content]()\n\n* [](/)\n* Getting Started\n\nOn this page\n\n# LiteLLM - Getting Started\n\n<https://github.com/BerriAI/litellm>\n\n## **Call 100+ LLMs using the OpenAI Input/Output Format** [‚Äã]( \"Direct link to call-100-llms-using-the-openai-inputoutput-format\")\n\n* Translate inputs to provider's endpoints ( `/chat/completions` , `/responses` , `/embeddings` , `/images` , `/audio` , `/batches` , and more)\n* [Consistent output](https://docs.litellm.ai/docs/supported_endpoints) \\- same response format regardless of which provider you use\n* Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n* Track spend & set budgets per project [LiteLLM Proxy Server](https://docs.litellm.ai/docs/simple_proxy)\n\n## How to use LiteLLM [‚Äã]( \"Direct link to How to use LiteLLM\")\n\nYou can use LiteLLM through either the Proxy Server or Python SDK. Both gives you a unified interface to access multiple LLMs (100+ LLMs).\nChoose the option that best fits your needs:\n\n| |**[LiteLLM Proxy Server]()** |**[LiteLLM Python SDK]()** |\n| --- | --- | --- |\n|**Use Case** |Central service (LLM Gateway) to access multiple LLMs |Use LiteLLM directly in your Python code |\n|**Who Uses It? ** |Gen AI Enablement / ML Platform Teams |Developers building LLM projects |\n|**Key Features** |‚Ä¢ Centralized API gateway with authentication & authorization  \n‚Ä¢ Multi-tenant cost tracking and spend management per project/user  \n‚Ä¢ Per-project customization (logging, guardrails, caching)  \n‚Ä¢ Virtual keys for secure access control  \n‚Ä¢ Admin dashboard UI for monitoring and management |‚Ä¢ Direct Python library integration in your codebase  \n‚Ä¢ Router with retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)  \n‚Ä¢ Application-level load balancing and cost tracking  \n‚Ä¢ Exception handling with OpenAI-compatible errors  \n‚Ä¢ Observability callbacks (Lunary, MLflow, Langfuse, etc.)\n|\n\n\n## **LiteLLM Python SDK** [‚Äã]( \"Direct link to litellm-python-sdk\")\n\n### Basic usage [‚Äã]( \"Direct link to Basic usage\")\n\n[](https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb)\n\n```\npip install litellm\n```\n\n* OpenAI\n* Anthropic\n* xAI\n* VertexAI\n* NVIDIA\n* HuggingFace\n* Azure OpenAI\n* Ollama\n* Openrouter\n* Novita AI\n* Vercel AI Gateway\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"openai/gpt-4o\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"anthropic/claude-3-sonnet-20240229\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\"\n, \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"XAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"xai/grok-2-latest\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n # auth: run 'gcloud auth application-default'  \n os . environ [ \"VERTEXAI_PROJECT\" ] = \"hardy-device-386718\"  \n os . environ [ \"VERTEXAI_LOCATION\" ] = \"us-central1\"  \n   \n response  =  completion (  \n   model = \"vertex_ai/gemini-1.5-pro\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"NVIDIA_NIM_API_KEY\" ] = \"nvidia_api_key\"  \n os .\n ... \nenviron [ \"AZURE_API_VERSION\" ] = \"\"  \n   \n # azure call  \n response  =  completion (  \n \"azure/<your_deployment_name>\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n   \n response  =  completion (  \n             model = \"ollama/llama2\" ,  \n             messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n             api_base = \"http://localhost:11434\"  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENROUTER_API_KEY\" ] = \"openrouter_api_key\"  \n   \n response  =  completion (  \n   model = \"openrouter/google/palm-2-chat-bison\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables. Visit https://novita.ai/settings/key-management to get your API key  \n os .\nenviron [ \"NOVITA_API_KEY\" ] = \"novita-api-key\"  \n   \n response  =  completion (  \n   model = \"novita/deepseek/deepseek-r1\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables. Visit https://vercel.com/docs/ai-gateway for insturctions on obtaining a key  \n os . environ [ \"VERCEL_AI_GATEWAY_API_KEY\" ] = \"your-vercel-api-key\"  \n   \n response  =  completion (  \n   model = \"vercel_ai_gateway/openai/gpt-4o\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\"\n ... \n,  \n \"role\" : \"assistant\" ,  \n \"tool_calls\" : null ,  \n \"function_call\" : null  \n }  \n }  \n ] ,  \n \"usage\" : {  \n \"completion_tokens\" : 43 ,  \n \"prompt_tokens\" : 13 ,  \n \"total_tokens\" : 56 ,  \n \"completion_tokens_details\" : null ,  \n \"prompt_tokens_details\" : {  \n \"audio_tokens\" : null ,  \n \"cached_tokens\" : 0  \n } ,  \n \"cache_creation_input_tokens\" : 0 ,  \n \"cache_read_input_tokens\" : 0  \n }  \n }\n```\n\n### Streaming [‚Äã]( \"Direct link to Streaming\")\n\nSet `stream=True` in the `completion` args. * OpenAI\n* Anthropic\n* xAI\n* VertexAI\n* NVIDIA\n* HuggingFace\n* Azure OpenAI\n* Ollama\n* Openrouter\n* Novita AI\n* Vercel AI Gateway\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"openai/gpt-4o\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\"\n, \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"anthropic/claude-3-sonnet-20240229\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"XAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"xai/grok-2-latest\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n # auth: run 'gcloud auth application-default'  \n os . environ [ \"VERTEX_PROJECT\" ] = \"hardy-device-386718\"  \n os .\n ... \nenviron [ \"VERCEL_AI_GATEWAY_API_KEY\" ] = \"your-vercel-api-key\"  \n   \n response  =  completion (  \n   model = \"vercel_ai_gateway/openai/gpt-4o\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n### Streaming Response Format (OpenAI Format) [‚Äã]( \"Direct link to Streaming Response Format (OpenAI Format)\")\n\n```\n{  \n \"id\" : \"chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697\" ,  \n \"created\" : 1734366925 ,  \n \"model\" : \"claude-3-sonnet-20240229\" ,  \n \"object\" : \"chat.completion.chunk\" ,  \n \"system_fingerprint\" : null ,  \n \"choices\" : [  \n {  \n \"finish_reason\" : null ,  \n \"index\" : 0 ,  \n \"delta\" : {  \n \"content\" : \"Hello\" ,  \n \"role\" : \"assistant\" ,  \n \"function_call\" : null ,  \n \"tool_calls\" : null ,  \n \"audio\" : null  \n } ,  \n \"logprobs\" : null  \n }  \n ]  \n }\n```\n\n### Exception handling [‚Äã]( \"Direct link to Exception handling\")\n\nLiteLLM maps exceptions across all supported providers to the OpenAI exceptions.\nAll our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM. ```\nimport  litellm  \n from  litellm  import  completion  \n import  os  \n   \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"bad-key\"  \n try :  \n     completion ( model = \"anthropic/claude-instant-1\" ,  messages = [ { \"role\" : \"user\" , \"content\" : \"Hey, how's it going?\" } ] )  \n except  litellm . AuthenticationError  as  e :  \n # Thrown when the API key is invalid  \n print ( f\"Authentication failed:  { e } \" )  \n except  litellm . RateLimitError  as  e :  \n # Thrown when you've exceeded your rate limit  \n print ( f\"Rate limited:  { e } \" )  \n except  litellm . APIError  as  e :  \n # Thrown for general API errors  \n print ( f\"API error:  { e } \" )\n```\n\n### See How LiteLLM Transforms Your Requests [‚Äã]( \"Direct link to See How LiteLLM Transforms Your Requests\")\n\nWant to understand how LiteLLM parses and normalizes your LLM API requests?\n ... \nsuccess_callback  = [ \"lunary\" , \"mlflow\" , \"langfuse\" , \"helicone\" ] # log input/output to lunary, mlflow, langfuse, helicone  \n   \n #openai call  \n response  =  completion ( model = \"gpt-3.5-turbo\" ,  messages = [ { \"role\" : \"user\" , \"content\" : \"Hi üëã - i'm openai\" } ] )\n```\n\n### Track Costs, Usage, Latency for streaming [‚Äã]( \"Direct link to Track Costs, Usage, Latency for streaming\")\n\nUse a callback function for this - more info on custom callbacks: [https://docs.litellm.ai/docs/observability/custom\\_callback](https://docs.litellm.ai/docs/observability/custom_callback)\n\n```\nimport  litellm  \n   \n # track_cost_callback  \n def track_cost_callback (  \n     kwargs , # kwargs to completion  \n     completion_response , # response from completion  \n     start_time ,  end_time     # start/end time  \n ) :  \n try :  \n       response_cost  =  kwargs . get ( \"response_cost\" , 0 )  \n print ( \"streaming response_cost\" ,  response_cost )  \n except :  \n pass  \n # set callback  \n litellm .\n ... \nCREATE config.yaml\n\nExample `litellm_config.yaml`\n\n```\nmodel_list :  \n - model_name :  gpt - 3.5 - turbo  \n litellm_params :  \n model :  azure/<your - azure - model - deployment >  \n api_base :  os.environ/AZURE_API_BASE  # runs os.getenv(\"AZURE_API_BASE\")  \n api_key :  os.environ/AZURE_API_KEY  # runs os.getenv(\"AZURE_API_KEY\")  \n api_version : \"2023-07-01-preview\"\n```\n\nStep 2. RUN Docker Image\n\n```\ndocker run \\  \n     -v $(pwd)/litellm_config.yaml:/app/config.yaml \\  \n     -e AZURE_API_KEY=d6*********** \\  \n     -e AZURE_API_BASE=https://openai-***********/ \\  \n     -p 4000:4000 \\  \n     docker.litellm.ai/berriai/litellm:main-latest \\  \n     --config /app/config.yaml --detailed_debug\n```\n\n#### Step 2: Make ChatCompletions Request to Proxy [‚Äã]( \"Direct link to Step 2: Make ChatCompletions Request to Proxy\")\n\n```\nimport  openai  # openai v1.0.0+  \n client  =  openai .\nOpenAI ( api_key = \"[REDACTED:api-key]\" , base_url = \"http://0.0.0.0:4000\" ) # set proxy to base_url  \n # request sent to model set on litellm proxy, `litellm --model`  \n response  =  client . chat . completions . create ( model = \"gpt-3.5-turbo\" ,  messages  = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : \"this is a test request, write a short poem\"  \n }  \n ] )  \n   \n print ( response )\n```\n\n## More details [‚Äã]( \"Direct link to More details\")\n\n* [exception mapping](/docs/exception_mapping)\n* [retries + model fallbacks for completion()](/docs/completion/reliable_completions)\n* [proxy virtual keys & spend management](/docs/proxy/virtual_keys)\n* [E2E Tutorial for LiteLLM Proxy Server](/docs/proxy/docker_quick_start)\n\n[Next completion()](/docs/completion/input)\n\nDocs\n\n* [Getting Started](https://docs.litellm.ai/docs/)\n\nCommunity\n\n* [Discord](https://discord.com/invite/wuPM9dRgDw)\n* [Twitter](https://twitter.com/LiteLLM)\n\nMore\n\n* [GitHub](https://github.com/BerriAI/litellm/)\n\nCopyright ¬© 2025 liteLLM"
    ]
  },
  {
    "title": "[Feature]: Add Multi-Modal Output Support for Image and ...",
    "url": "https://github.com/BerriAI/litellm/issues/15177",
    "excerpts": [
      "Oct 3, 2025 ‚Äî Description. LiteLLM currently supports multi-modal inputs (text + images), but lacks support for multi-modal outputs (generating images and ...Read more"
    ]
  },
  {
    "title": "Image Generations | liteLLM",
    "url": "https://docs.litellm.ai/docs/image_generation",
    "excerpts": [
      "[Skip to main content]()\n\n* [](/)\n* LiteLLM Python SDK\n* SDK Functions\n* image\\_generation()\n\nOn this page\n\n# Image Generations\n\n## Overview [‚Äã]( \"Direct link to Overview\")\n\n|Feature |Supported |Notes |\n| --- | --- | --- |\n|Cost Tracking |‚úÖ |Works with all supported models |\n|Logging |‚úÖ |Works across all integrations |\n|End-user Tracking |‚úÖ | |\n|Fallbacks |‚úÖ |Works between supported models |\n|Loadbalancing |‚úÖ |Works between supported models |\n|Guardrails |‚úÖ |Applies to input prompts (non-streaming only) |\n|Supported Providers |OpenAI, Azure, Google AI Studio, Vertex AI, AWS Bedrock, Recraft, Xinference, Nscale | |\n\n\n## Quick Start [‚Äã]( \"Direct link to Quick Start\")\n\n### LiteLLM Python SDK [‚Äã]( \"Direct link to LiteLLM Python SDK\")\n\n```\nfrom  litellm  import  image_generation  \n import  os   \n   \n # set api keys   \n os . environ [ \"OPENAI_API_KEY\" ] = \"\"  \n   \n response  =  image_generation ( prompt = \"A cute baby sea otter\" ,  model = \"dall-e-3\" )  \n   \n print ( f\"response:  { response } \" )\n```\n\n### LiteLLM Proxy [‚Äã]( \"Direct link to LiteLLM Proxy\")\n\n### Setup config.yaml [‚Äã]( \"Direct link to Setup config.yaml\")\n\n```\nmodel_list :  \n - model_name :  gpt - image - 1 ### RECEIVED MODEL NAME ###  \n litellm_params : # all params accepted by litellm.image_generation()  \n model :  azure/gpt - image - 1 ### MODEL NAME sent to `litellm.image_generation()` ###  \n api_base :  https : //my - endpoint - europe - berri - 992.openai.azure.com/  \n api_key : \"os.environ/AZURE_API_KEY_EU\" # does os.getenv(\"AZURE_API_KEY_EU\")\n```\n\n### Start proxy [‚Äã]( \"Direct link to Start proxy\")\n\n```\nlitellm --config /path/to/config.yaml   \n   \n # RUNNING on http://0.0.0.0:4000\n```\n\n### Test [‚Äã]( \"Direct link to Test\")\n\n* Curl\n* OpenAI\n\n```\ncurl -X POST 'http://0.0.0.0:4000/v1/images/generations' \\  \n -H 'Content-Type: application/json' \\  \n -H 'Authorization: Bearer sk-1234' \\  \n -d '{  \n     \"model\": \"gpt-image-1\",  \n     \"prompt\": \"A cute baby sea otter\",  \n     \"n\": 1,  \n     \"size\": \"1024x1024\"  \n }'\n```\n\n```\nfrom  openai  import  OpenAI  \n client  =  openai . OpenAI (  \n     api_key = \"[REDACTED:api-key]\" ,  \n     base_url = \"http://0.0.0.0:4000\"  \n )  \n   \n   \n image  =  client . images . generate (  \n     prompt = \"A cute baby sea otter\" ,  \n     model = \"dall-e-3\" ,  \n )  \n   \n print ( image )\n```\n\n## Input Params for `litellm.image_generation()` [‚Äã]( \"Direct link to input-params-for-litellmimage_generation\")\n\ninfo\n\nAny non-openai params, will be treated as provider-specific params, and sent in the request body as kwargs to the provider.\n\n[**See Reserved Params**](https://github.com/BerriAI/litellm/blob/2f5f85cb52f36448d1f8bbfbd3b8af8167d0c4c8/litellm/main.py)\n\n### Required Fields [‚Äã]( \"Direct link to Required Fields\")\n\n* `prompt` : _string_ \\- A text description of the desired image(s).\n\n### Optional LiteLLM Fields [‚Äã]( \"Direct link to Optional LiteLLM Fields\")\n\nmodel: Optional[str] = None,\nn: Optional[int] = None,\nquality: Optional[str] = None,\nresponse\\_format: Optional[str] = None,\nsize: Optional[str] = None,\nstyle: Optional[str] = None,\nuser: Optional[str] = None,\ntimeout=600, # default to 10 minutes\napi\\_key: Optional[str] = None,\napi\\_base: Optional[str] = None,\napi\\_version: Optional[str] = None,\nlitellm\\_logging\\_obj=None,\ncustom\\_llm\\_provider=None,\n\n* `model` : _string (optional)_ The model to use for image generation. Defaults to openai/gpt-image-1\n* `n` : _int (optional)_ The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.\n* `quality` : _string (optional)_ The quality of the image that will be generated.\n  \n    + `auto` (default value) will automatically select the best quality for the given model.\n    + `high` , `medium` and `low` are supported for `gpt-image-1` .\n    + `hd` and `standard` are supported for `dall-e-3` .\n    + `standard` is the only option for `dall-e-2` .\n* `response_format` : _string (optional)_ The format in which the generated images are returned. Must be one of url or b64\\_json.\n* `size` : _string (optional)_ The size of the generated images. Must be one of `1024x1024` , `1536x1024` (landscape), `1024x1536` (portrait), or `auto` (default value) for `gpt-image-1` , one of `256x256` , `512x512` , or `1024x1024` for `dall-e-2` , and one of `1024x1024` , `1792x1024` , or `1024x1792` for `dall-e-3` .\n* `timeout` : _integer_ \\- The maximum time, in seconds, to wait for the API to respond. Defaults to 600 seconds (10 minutes).\n* `user` : _string (optional)_ A unique identifier representing your end-user,\n* `api_base` : _string (optional)_ \\- The api endpoint you want to call the model with\n* `api_version` : _string (optional)_ \\- (Azure-specific) the api version for the call; required for dall-e-3 on Azure\n* `api_key` : _string (optional)_ \\- The API key to authenticate and authorize requests. If not provided, the default API key is used.\n* `api_type` : _string (optional)_ \\- The type of API to use.\n\n### Output from `litellm.image_generation()` [‚Äã]( \"Direct link to output-from-litellmimage_generation\")\n\n```\n  \n {  \n \"created\" : 1703658209 ,  \n \"data\" : [ {  \n         'b64_json' :  None ,  \n         'revised_prompt' :  'Adorable baby sea otter with a coat of thick brown fur ,  playfully swimming in blue ocean waters. Its curious ,  bright eyes gleam as it is surfaced above water ,  tiny paws held close to its chest ,  as it playfully spins in the gentle waves under the soft rays of a setting sun.' ,  \n         'url' :  'https : //oaidalleapiprodscus.blob.core.windows.net/private/org-ikDc4ex8NB5ZzfTf8m5WYVB7/user-JpwZsbIXubBZvan3Y3GchiiB/img-dpa3g5LmkTrotY6M93dMYrdE.png?st=2023-12-27T05%3A23%3A29Z&se=2023-12-27T07%3A23%3A29Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-26T13%3A22%3A56Z&ske=2023-12-27T13%3A22%3A56Z&sks=b&skv=2021-08-06&sig=hUuQjYLS%2BvtsDdffEAp2gwewjC8b3ilggvkd9hgY6Uw%3D'  \n } ] ,  \n \"usage\" : { 'prompt_tokens' : 0 ,  'completion_tokens' : 0 ,  'total_tokens' : 0 }  \n }\n```\n\n## OpenAI Image Generation Models [‚Äã]( \"Direct link to OpenAI Image Generation Models\")\n\n### Usage [‚Äã]( \"Direct link to Usage\")\n\n```\nfrom  litellm  import  image_generation  \n import  os  \n os . environ [ 'OPENAI_API_KEY' ] = \"\"  \n response  =  image_generation ( model = 'gpt-image-1' ,  prompt = \"cute baby otter\" )\n```\n\n|Model Name |Function Call |Required OS Variables |\n| --- | --- | --- |\n|gpt-image-1 |`image_generation(model='gpt-image-1', prompt=\"cute baby otter\")` |`os.environ['OPENAI_API_KEY']` |\n|dall-e-3 |`image_generation(model='dall-e-3', prompt=\"cute baby otter\")` |`os.environ['OPENAI_API_KEY']` |\n|dall-e-2 |`image_generation(model='dall-e-2', prompt=\"cute baby otter\")` |`os.environ['OPENAI_API_KEY']` |\n\n\n## Azure OpenAI Image Generation Models [‚Äã]( \"Direct link to Azure OpenAI Image Generation Models\")\n\n### API keys [‚Äã]( \"Direct link to API keys\")\n\nThis can be set as env variables or passed as **params to litellm.image\\_generation()**\n\n```\nimport  os  \n os . environ [ 'AZURE_API_KEY' ] =  \n os . environ [ 'AZURE_API_BASE' ] =  \n os . environ [ 'AZURE_API_VERSION' ] =\n```\n\n### Usage [‚Äã]( \"Direct link to Usage\")\n\n```\nfrom  litellm  import  embedding  \n response  =  embedding (  \n     model = \"azure/<your deployment name>\" ,  \n     prompt = \"cute baby otter\" ,  \n     api_key = api_key ,  \n     api_base = api_base ,  \n     api_version = api_version ,  \n )  \n print ( response )\n```\n\n|Model Name |Function Call |\n| --- | --- |\n|gpt-image-1 |`image_generation(model=\"azure/<your deployment name>\", prompt=\"cute baby otter\")` |\n|dall-e-3 |`image_generation(model=\"azure/<your deployment name>\", prompt=\"cute baby otter\")` |\n|dall-e-2 |`image_generation(model=\"azure/<your deployment name>\", prompt=\"cute baby otter\")` |\n\n\n## Xinference Image Generation Models [‚Äã]( \"Direct link to Xinference Image Generation Models\")\n\nUse this for Stable Diffusion models hosted on Xinference\n\n#### Usage [‚Äã]( \"Direct link to Usage\")\n\nSee Xinference usage with LiteLLM [here](/docs/providers/xinference)\n\n## Recraft Image Generation Models [‚Äã]( \"Direct link to Recraft Image Generation Models\")\n\nUse this for AI-powered design and image generation with Recraft\n\n#### Usage [‚Äã]( \"Direct link to Usage\")\n\n```\nfrom  litellm  import  image_generation  \n import  os  \n   \n os . environ [ 'RECRAFT_API_KEY' ] = \"your-api-key\"  \n   \n response  =  image_generation (  \n     model = \"recraft/recraftv3\" ,  \n     prompt = \"A beautiful sunset over a calm ocean\" ,  \n )  \n print ( response )\n```\n\nSee Recraft usage with LiteLLM [here](/docs/providers/recraft)\n\n## OpenAI Compatible Image Generation Models [‚Äã]( \"Direct link to OpenAI Compatible Image Generation Models\")\n\nUse this for calling `/image_generation` endpoints on OpenAI Compatible Servers, example <https://github.com/xorbitsai/inference>\n\n**Note add `openai/` prefix to model so litellm knows to route to OpenAI**\n\n### Usage [‚Äã]( \"Direct link to Usage\")\n\n```\nfrom  litellm  import  image_generation  \n response  =  image_generation (  \n   model  = \"openai/<your-llm-name>\" , # add `openai/` prefix to model so litellm knows to route to OpenAI  \n   api_base = \"http://0.0.0.0:8000/\" # set API Base of your Custom OpenAI Endpoint  \n   prompt = \"cute baby otter\"  \n )\n```\n\n## Bedrock - Stable Diffusion [‚Äã]( \"Direct link to Bedrock - Stable Diffusion\")\n\nUse this for stable diffusion on bedrock\n\n### Usage [‚Äã]( \"Direct link to Usage\")\n\n```\nimport  os  \n from  litellm  import  image_generation  \n   \n os . environ [ \"AWS_ACCESS_KEY_ID\" ] = \"\"  \n os . environ [ \"AWS_SECRET_ACCESS_KEY\" ] = \"\"  \n os . environ [ \"AWS_REGION_NAME\" ] = \"\"  \n   \n response  =  image_generation (  \n             prompt = \"A cute baby sea otter\" ,  \n             model = \"bedrock/stability.stable-diffusion-xl-v0\" ,  \n )  \n print ( f\"response:  { response } \" )\n```\n\n## VertexAI - Image Generation Models [‚Äã]( \"Direct link to VertexAI - Image Generation Models\")\n\n### Usage [‚Äã]( \"Direct link to Usage\")\n\nUse this for image generation models on VertexAI\n\n```\nresponse  =  litellm . image_generation (  \n     prompt = \"An olympic size swimming pool\" ,  \n     model = \"vertex_ai/imagegeneration@006\" ,  \n     vertex_ai_project = \"adroit-crow-413218\" ,  \n     vertex_ai_location = \"us-central1\" ,  \n )  \n print ( f\"response:  { response } \" )\n```\n\n## Supported Providers [‚Äã]( \"Direct link to Supported Providers\")\n\n#### ‚ö°Ô∏èSee all supported models and providers at [models.litellm.ai](https://models.litellm.ai/) [‚Äã](#Ô∏èsee-all-supported-models-and-providers-at-modelslitellmai \"Direct link to Ô∏èsee-all-supported-models-and-providers-at-modelslitellmai\")\n\n|Provider |Documentation Link |\n| --- | --- |\n|OpenAI |[OpenAI Image Generation ‚Üí](/docs/providers/openai) |\n|Azure OpenAI |[Azure OpenAI Image Generation ‚Üí](/docs/providers/azure/azure) |\n|Google AI Studio |[Google AI Studio Image Generation ‚Üí](/docs/providers/google_ai_studio/image_gen) |\n|Vertex AI |[Vertex AI Image Generation ‚Üí](/docs/providers/vertex_image) |\n|AWS Bedrock |[Bedrock Image Generation ‚Üí](/docs/providers/bedrock) |\n|Recraft |[Recraft Image Generation ‚Üí](/docs/providers/recraft) |\n|Xinference |[Xinference Image Generation ‚Üí](/docs/providers/xinference) |\n|Nscale |[Nscale Image Generation ‚Üí](/docs/providers/nscale) |\n\n[Previous text\\_completion()](/docs/text_completion) [Next transcription()](/docs/audio_transcription)\n\nDocs\n\n* [Getting Started](https://docs.litellm.ai/docs/)\n\nCommunity\n\n* [Discord](https://discord.com/invite/wuPM9dRgDw)\n* [Twitter](https://twitter.com/LiteLLM)\n\nMore\n\n* [GitHub](https://github.com/BerriAI/litellm/)\n\nCopyright ¬© 2025 liteLLM\n"
    ]
  },
  {
    "title": "LiteLLM - Getting Started | liteLLM",
    "url": "https://docs.litellm.ai/",
    "excerpts": [
      "[Skip to main content]()\n\n# LiteLLM - Getting Started\n\n<https://github.com/BerriAI/litellm>\n\n## **Call 100+ LLMs using the OpenAI Input/Output Format** [‚Äã]( \"Direct link to call-100-llms-using-the-openai-inputoutput-format\")\n\n* Translate inputs to provider's `completion` , `embedding` , and `image_generation` endpoints\n* [Consistent output](https://docs.litellm.ai/docs/completion/output) , text responses will always be available at `['choices'][0]['message']['content']`\n* Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n* Track spend & set budgets per project [LiteLLM Proxy Server](https://docs.litellm.ai/docs/simple_proxy)\n\n## How to use LiteLLM [‚Äã]( \"Direct link to How to use LiteLLM\")\n\nYou can use litellm through either:\n\n1. [LiteLLM Proxy Server]() \\- Server (LLM Gateway) to call 100+ LLMs, load balance, cost tracking across projects\n2.\n[LiteLLM python SDK]() \\- Python Client to call 100+ LLMs, load balance, cost tracking\n\n### **When to use LiteLLM Proxy Server (LLM Gateway)** [‚Äã]( \"Direct link to when-to-use-litellm-proxy-server-llm-gateway\")\n\ntip\n\nUse LiteLLM Proxy Server if you want a **central service (LLM Gateway) to access multiple LLMs**\n\nTypically used by Gen AI Enablement / ML PLatform Teams\n\n* LiteLLM Proxy gives you a unified interface to access multiple LLMs (100+ LLMs)\n* Track LLM Usage and setup guardrails\n* Customize Logging, Guardrails, Caching per project\n\n### **When to use LiteLLM Python SDK** [‚Äã]( \"Direct link to when-to-use-litellm-python-sdk\")\n\ntip\n\nUse LiteLLM Python SDK if you want to use LiteLLM in your **python code**\n\nTypically used by developers building llm projects\n\n* LiteLLM SDK gives you a unified interface to access multiple LLMs (100+ LLMs)\n* Retry/fallback logic across multiple deployments (e.g.\nAzure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n\n## **LiteLLM Python SDK** [‚Äã]( \"Direct link to litellm-python-sdk\")\n\n### Basic usage [‚Äã]( \"Direct link to Basic usage\")\n\n[](https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb)\n\n```\npip install litellm\n```\n\n* OpenAI\n* Anthropic\n* VertexAI\n* NVIDIA\n* HuggingFace\n* Azure OpenAI\n* Ollama\n* Openrouter\n* Novita AI\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"gpt-3.5-turbo\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"claude-2\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\"\n, \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n # auth: run 'gcloud auth application-default'  \n os . environ [ \"VERTEX_PROJECT\" ] = \"hardy-device-386718\"  \n os . environ [ \"VERTEX_LOCATION\" ] = \"us-central1\"  \n   \n response  =  completion (  \n   model = \"chat-bison\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"NVIDIA_NIM_API_KEY\" ] = \"nvidia_api_key\"  \n os . environ [ \"NVIDIA_NIM_API_BASE\" ] = \"nvidia_nim_endpoint_url\"  \n   \n response  =  completion (  \n   model = \"nvidia_nim/<model_name>\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n os . environ [ \"HUGGINGFACE_API_KEY\" ] = \"huggingface_api_key\"  \n   \n # e.g.\nCall 'WizardLM/WizardCoder-Python-34B-V1.0' hosted on HF Inference endpoints  \n response  =  completion (  \n   model = \"huggingface/WizardLM/WizardCoder-Python-34B-V1.0\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   api_base = \"https://my-endpoint.huggingface.cloud\"  \n )  \n   \n print ( response )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"AZURE_API_KEY\" ] = \"\"  \n os . environ [ \"AZURE_API_BASE\" ] = \"\"  \n os . environ [ \"AZURE_API_VERSION\" ] = \"\"  \n   \n # azure call  \n response  =  completion (  \n \"azure/<your_deployment_name>\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  completion  \n   \n response  =  completion (  \n             model = \"ollama/llama2\" ,  \n             messages  = [ { \"content\" : \"Hello, how are you?\"\n, \"role\" : \"user\" } ] ,  \n             api_base = \"http://localhost:11434\"  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENROUTER_API_KEY\" ] = \"openrouter_api_key\"  \n   \n response  =  completion (  \n   model = \"openrouter/google/palm-2-chat-bison\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables. Visit https://novita.ai/settings/key-management to get your API key  \n os . environ [ \"NOVITA_API_KEY\" ] = \"novita-api-key\"  \n   \n response  =  completion (  \n   model = \"novita/deepseek/deepseek-r1\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ]  \n )\n```\n\n### Responses API [‚Äã]( \"Direct link to Responses API\")\n\nUse `litellm.responses()` for advanced models that support reasoning content like GPT-5, o3, etc.\n* OpenAI\n* Anthropic (Claude)\n* VertexAI\n* Azure OpenAI\n\n```\nfrom  litellm  import  responses  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  responses (  \n   model = \"gpt-5-mini\" ,  \n   messages = [ { \"content\" : \"What is the capital of France?\" , \"role\" : \"user\" } ] ,  \n   reasoning_effort = \"medium\"  \n )  \n   \n print ( response )  \n print ( response . choices [ 0 ] . message . content ) # response  \n print ( response . choices [ 0 ] . message . reasoning_content ) # reasoning\n```\n\n```\nfrom  litellm  import  responses  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  responses (  \n   model = \"claude-3.5-sonnet\" ,  \n   messages = [ { \"content\" : \"What is the capital of France?\" , \"role\" : \"user\" } ]  \n )\n```\n\n```\nfrom  litellm  import  responses  \n import  os  \n   \n # auth: run 'gcloud auth application-default'  \n os .\n ... \nenviron [ \"OPENAI_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"gpt-3.5-turbo\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"your-api-key\"  \n   \n response  =  completion (  \n   model = \"claude-2\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n # auth: run 'gcloud auth application-default'  \n os . environ [ \"VERTEX_PROJECT\" ] = \"hardy-device-386718\"  \n os . environ [ \"VERTEX_LOCATION\" ] = \"us-central1\"  \n   \n response  =  completion (  \n   model = \"chat-bison\" ,  \n   messages = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n```\nfrom  litellm  import  completion  \n import  os  \n   \n ## set ENV variables  \n os .\n ... \nVisit https://novita.ai/settings/key-management to get your API key  \n os . environ [ \"NOVITA_API_KEY\" ] = \"novita_api_key\"  \n   \n response  =  completion (  \n   model = \"novita/deepseek/deepseek-r1\" ,  \n   messages  = [ { \"content\" : \"Hello, how are you?\" , \"role\" : \"user\" } ] ,  \n   stream = True ,  \n )\n```\n\n### Exception handling [‚Äã]( \"Direct link to Exception handling\")\n\nLiteLLM maps exceptions across all supported providers to the OpenAI exceptions. All our exceptions inherit from OpenAI's exception types, so any error-handling you have for that, should work out of the box with LiteLLM. ```\nfrom  openai . error  import  OpenAIError  \n from  litellm  import  completion  \n   \n os . environ [ \"ANTHROPIC_API_KEY\" ] = \"bad-key\"  \n try :  \n # some code  \n     completion ( model = \"claude-instant-1\" ,  messages = [ { \"role\" : \"user\" , \"content\" : \"Hey, how's it going?\"\n ... \nsuccess_callback  = [ \"lunary\" , \"mlflow\" , \"langfuse\" , \"helicone\" ] # log input/output to lunary, mlflow, langfuse, helicone  \n   \n #openai call  \n response  =  completion ( model = \"gpt-3.5-turbo\" ,  messages = [ { \"role\" : \"user\" , \"content\" : \"Hi üëã - i'm openai\" } ] )\n```\n\n### Track Costs, Usage, Latency for streaming [‚Äã]( \"Direct link to Track Costs, Usage, Latency for streaming\")\n\nUse a callback function for this - more info on custom callbacks: [https://docs.litellm.ai/docs/observability/custom\\_callback](https://docs.litellm.ai/docs/observability/custom_callback)\n\n```\nimport  litellm  \n   \n # track_cost_callback  \n def track_cost_callback (  \n     kwargs , # kwargs to completion  \n     completion_response , # response from completion  \n     start_time ,  end_time     # start/end time  \n ) :  \n try :  \n       response_cost  =  kwargs . get ( \"response_cost\" , 0 )  \n print ( \"streaming response_cost\" ,  response_cost )  \n except :  \n pass  \n # set callback  \n litellm .\n ... \nCREATE config.yaml\")\n\nExample `litellm_config.yaml`\n\n```\nmodel_list :  \n - model_name :  gpt - 3.5 - turbo  \n litellm_params :  \n model :  azure/<your - azure - model - deployment >  \n api_base :  os.environ/AZURE_API_BASE  # runs os.getenv(\"AZURE_API_BASE\")  \n api_key :  os.environ/AZURE_API_KEY  # runs os.getenv(\"AZURE_API_KEY\")  \n api_version : \"2023-07-01-preview\"  \n   \n litellm_settings :  \n master_key :  sk - 1234  \n database_url :  postgres : //\n```\n\n### Step 2. RUN Docker Image [‚Äã]( \"Direct link to Step 2.\nRUN Docker Image\")\n\n```\ndocker run \\  \n     -v $(pwd)/litellm_config.yaml:/app/config.yaml \\  \n     -e AZURE_API_KEY=d6*********** \\  \n     -e AZURE_API_BASE=https://openai-***********/ \\  \n     -p 4000:4000 \\  \n     docker.litellm.ai/berriai/litellm:main-latest \\  \n     --config /app/config.yaml --detailed_debug\n```\n\n#### Step 2: Make ChatCompletions Request to Proxy [‚Äã]( \"Direct link to Step 2: Make ChatCompletions Request to Proxy\")\n\n* Chat Completions\n* Responses API\n\n```\nimport  openai  # openai v1.0.0+  \n client  =  openai . OpenAI ( api_key = \"[REDACTED:api-key]\" , base_url = \"http://0.0.0.0:4000\" ) # set proxy to base_url  \n # request sent to model set on litellm proxy, `litellm --model`  \n response  =  client . chat . completions .\ncreate ( model = \"gpt-3.5-turbo\" ,  messages  = [  \n {  \n \"role\" : \"user\" ,  \n \"content\" : \"this is a test request, write a short poem\"  \n }  \n ] )  \n   \n print ( response )\n```\n\n```\nfrom  openai  import  OpenAI  \n   \n client  =  OpenAI (  \n     api_key = \"[REDACTED:api-key]\" ,  \n     base_url = \"http://0.0.0.0:4000\"  \n )  \n   \n response  =  client . responses . create (  \n   model = \"gpt-5\" ,  \n input = \"Tell me a three sentence bedtime story about a unicorn.\" )  \n   \n print ( response )\n```\n\n## More details [‚Äã]( \"Direct link to More details\")\n\n* [exception mapping](/docs/exception_mapping)\n* [E2E Tutorial for LiteLLM Proxy Server](/docs/proxy/docker_quick_start)\n* [proxy virtual keys & spend management](/docs/proxy/virtual_keys)\n\nDocs\n\n* [Getting Started](https://docs.litellm.ai/docs/)\n\nCommunity\n\n* [Discord](https://discord.com/invite/wuPM9dRgDw)\n* [Twitter](https://twitter.com/LiteLLM)\n\nMore\n\n* [GitHub](https://github.com/BerriAI/litellm/)\n\nCopyright ¬© 2025 liteLLM"
    ]
  }
]
```

**Tool Cancelled:** `toolu_vrtx_01A4tGZGELPHg3JWyPZhZc1D`

## User

I doubt that jupyter-ai handles images in cells, but I could be wrong.  

## Assistant

You're likely right. Let me check the current ai-jup context gathering and see what's needed to capture cell outputs (images).

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "read_range": [
    196,
    240
  ]
}
```

**Tool Use:** `Grep`

```json
{
  "pattern": "outputs|OutputArea|execute_result|display_data",
  "path": "/Users/hamel/git/ai-jup/src"
}
```

## User

**Tool Result:** `toolu_vrtx_017Gqiq9GafoJ9V1nyqugXyf`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "content": "[... omitted lines 1 to 195 ...]\n196:    * Gather context for the prompt including preceding code and referenced items.\n197:    */\n198:   private async _gatherContext(\n199:     panel: NotebookPanel,\n200:     connector: IKernelConnector,\n201:     parsed: ReturnType<typeof parsePrompt>\n202:   ): Promise<IPromptContext> {\n203:     const notebook = panel.content;\n204:     const activeIndex = notebook.activeCellIndex;\n205: \n206:     // Get preceding code cells\n207:     const precedingCode: string[] = [];\n208:     for (let i = 0; i < activeIndex; i++) {\n209:       const cell = notebook.widgets[i];\n210:       if (cell.model.type === 'code') {\n211:         precedingCode.push(cell.model.sharedModel.getSource());\n212:       }\n213:     }\n214: \n215:     // Get referenced variables\n216:     const variables: Record<string, IVariableInfo> = {};\n217:     for (const varName of parsed.variables) {\n218:       const info = await connector.getVariable(varName);\n219:       if (info) {\n220:         variables[varName] = info;\n221:       }\n222:     }\n223: \n224:     // Get referenced functions\n225:     const functions: Record<string, IFunctionInfo> = {};\n226:     for (const funcName of parsed.functions) {\n227:       const info = await connector.getFunction(funcName);\n228:       if (info) {\n229:         functions[funcName] = info;\n230:       }\n231:     }\n232: \n233:     return {\n234:       preceding_code: precedingCode.join('\\n\\n'),\n235:       variables,\n236:       functions\n237:     };\n238:   }\n239: \n240:   /**\n[... omitted lines 241 to 487 ...]"
}
```

**Tool Result:** `toolu_vrtx_01HXhQ9sygvL7v5bX4fJWZ2q`

```
[
  "/Users/hamel/git/ai-jup/src/kernelConnector.ts:79:} else if (msgType === 'execute_result') {",
  "/Users/hamel/git/ai-jup/src/promptModel.test.ts:42:const outputs: string[] = [];",
  "/Users/hamel/git/ai-jup/src/promptModel.test.ts:43:model.outputChanged.connect((_, output) => outputs.push(output));",
  "/Users/hamel/git/ai-jup/src/promptModel.test.ts:48:expect(outputs).toEqual(['']);"
]
```

## Assistant

Let me check the JupyterLab cell model API for accessing outputs:

**Tool Use:** `web_search`

```json
{
  "objective": "How to access cell outputs (images, display_data) from JupyterLab notebook cell model programmatically - ICodeCellModel outputs API",
  "search_queries": [
    "jupyterlab cell outputs ICodeCellModel",
    "jupyterlab access cell output image base64"
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01JbHydd6ekgXoWj8MSTFUGV`

```
[
  {
    "title": "python - Programmatically get current IPython notebook cell output? - Stack Overflow",
    "url": "https://stackoverflow.com/questions/27952428/programmatically-get-current-ipython-notebook-cell-output",
    "excerpts": [
      "[Skip to main content]()\n\n[](#) [Stack Overflow](https://stackoverflow.com)\n\n1. [About](https://stackoverflow.co/)\n2. Products\n3. [For Teams](https://stackoverflow.co/internal/)\n\n1. [Stack Internal Implement a knowledge platform layer to power your enterprise and AI tools. ](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-for-teams)\n2. [Stack Data Licensing Get access to top-class technical expertise with trusted & attributed content. ](https://stackoverflow.co/data-licensing/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=overflow-api)\n3. [Stack Ads Connect your brand to the world‚Äôs most trusted technologist communities. ](https://stackoverflow.co/advertising/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-advertising)\n4.\n[Releases Keep up-to-date on features we add to Stack Overflow and Stack Internal. ](https://stackoverflow.blog/releases/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=releases)\n5. [About the company](https://stackoverflow.co/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=about-the-company) [Visit the blog](https://stackoverflow.blog/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=blog)\n\nLoading‚Ä¶\n\n1.\n### [current community](https://stackoverflow.com)\n   \n    + [Stack Overflow](https://stackoverflow.com)\n         \n         [help](https://stackoverflow.com/help) [chat](https://chat.stackoverflow.com/?tab=explore)\n    + [Meta Stack Overflow](https://meta.stackoverflow.com)\n   \n   ### your communities\n   \n   [Sign up](https://stackoverflow.com/users/signup?ssrc=site_switcher&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f27952428%2fprogrammatically-get-current-ipython-notebook-cell-output) or [log in](https://stackoverflow.com/users/login?ssrc=site_switcher&returnurl=https%3a%2f%2fstackoverflow.com%2fquestions%2f27952428%2fprogrammatically-get-current-ipython-notebook-cell-output) to customize your list. ### [more stack exchange communities](https://stackexchange.com/sites)\n   \n   [company blog](https://stackoverflow.blog)\n2. 3.\n ... \n[Learn more](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams-compact)\n\n##### Collectives‚Ñ¢ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. [Learn more about Collectives](/collectives)\n\n**Stack Internal**\n\nKnowledge at work\n\nBring the best of human thought and AI automation together at your work. [Explore Stack Internal](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams-compact-popover)\n\n# [Programmatically get current IPython notebook cell output? ](/questions/27952428/programmatically-get-current-ipython-notebook-cell-output)\n\n[Ask Question](/questions/ask)\n\nAsked\n\nModified [4 years, 8 months ago](?lastactivity \"2021-04-20 09:11:46Z\")\n\nViewed 37k times\n\n51\n\n[](/posts/27952428/timeline \"Show activity on this post.\")\nI have an imported function that runs in an IPython notebook (input cell X) which produces an output (in output cell X). After the function runs, I have some more code (also in input cell X); is there any way for that code to retrieve the current output (in output cell X)? There may be other ways to do what I am trying to achieve; but I am curious if the above is possible.\n* [python](/questions/tagged/python \"show questions tagged 'python'\")\n* [jupyter-notebook](/questions/tagged/jupyter-notebook \"show questions tagged 'jupyter-notebook'\")\n* [ipython](/questions/tagged/ipython \"show questions tagged 'ipython'\")\n\n[Share](/q/27952428 \"Short permalink to this question\")\n\n[Improve this question](/posts/27952428/edit)\n\nFollow\n\n[edited Apr 20, 2021 at 9:11](/posts/27952428/revisions \"show all edits to this post\")\n\n[](/users/10794031/bad-coder)\n\n[bad\\_coder](/users/10794031/bad-coder)\n\n13\\.3k 20 20 gold badges 60 60 silver badges 95 95 bronze badges\n\nasked Jan 14, 2015 at 21:05\n\n[](/users/997338/killajoule)\n\n[killajoule](/users/997338/killajoule)\n\n3,862 7 7 gold badges 34 34 silver badges 37 37 bronze badges\n\n[Add a comment](# \"Use comments to ask for more information or suggest improvements. Avoid answering questions in comments.\")\n| [](# \"Expand to show all comments on this post\")\n\n## 3 Answers 3\n\nSorted by: [Reset to default](/questions/27952428/programmatically-get-current-ipython-notebook-cell-output?answertab=scoredesc)\n\nHighest score (default) Trending (recent votes count more) Date modified (newest first) Date created (oldest first)\n\n80\n\n[](/posts/27952661/timeline \"Show activity on this post.\") IPython's [output caching system](http://ipython.org/ipython-doc/stable/interactive/reference.html) defines several global variables:\n\n* [ `_` ] (a single underscore): stores previous output, like Python‚Äôs default interpreter. * [ `__` ] (two underscores): next previous. * [ `___` ] (three underscores): next-next previous. Additionally, after each output `x` is created, there is a variable `_<x>` created with the output as its value.\n ... \n](https://stackoverflow.blog/2025/12/16/live-from-re-invent-it-s-stack-overflow/)\n* Featured on Meta\n* [AI Assist is now available on Stack Overflow](https://meta.stackexchange.com/questions/415115/ai-assist-is-now-available-on-stack-overflow)\n* [Native Ads coming soon to Stack Overflow and Stack Exchange](https://meta.stackexchange.com/questions/415259/native-ads-coming-soon-to-stack-overflow-and-stack-exchange)\n* [Policy: Generative AI (e.g., ChatGPT) is banned](https://meta.stackoverflow.com/questions/421831/policy-generative-ai-e-g-chatgpt-is-banned)\n\n#### Linked\n\n[1](/questions/67173787/finding-output-history-on-jupyterlab \"Question score (upvotes - downvotes)\") [Finding output history on JupyterLab](/questions/67173787/finding-output-history-on-jupyterlab?noredirect=1)\n\n[43](/questions/20704063/pipe-ipython-magic-output-to-a-variable \"Question score (upvotes - downvotes)\") [Pipe Ipython magic output to a variable?\n ... \nnotebook cell in python](/questions/51931874/execute-the-previous-notebook-cell-in-python?noredirect=1)\n\n[0](/questions/65632077/what-is-the-purpose-of-104-in-round-function-in-python \"Question score (upvotes - downvotes)\") [What is the purpose of \"\\_104\" in round function in python? ](/questions/65632077/what-is-the-purpose-of-104-in-round-function-in-python?noredirect=1)\n\n[0](/questions/78600371/how-can-i-programmatically-capture-the-output-from-the-current-cell-in-a-functio \"Question score (upvotes - downvotes)\") [How can I programmatically capture the output from the current cell in a function called from Jupyter / ipython?\n ... \n](/questions/17457921/ipython-notebook-save-the-currently-running-notebook-as-ipynb-file-using-a-pyth)\n\n[47](/questions/27994137/how-to-save-the-output-of-a-cell-in-ipython-notebook \"Question score (upvotes - downvotes)\") [How to save the output of a cell in iPython notebook](/questions/27994137/how-to-save-the-output-of-a-cell-in-ipython-notebook)\n\n[3](/questions/28652753/ipython-notebook-how-to-write-cell-magic-which-can-access-notebook-variables \"Question score (upvotes - downvotes)\") [IPython notebook: How to write cell magic which can access notebook variables?\n](/questions/28652753/ipython-notebook-how-to-write-cell-magic-which-can-access-notebook-variables)\n\n[4](/questions/29888736/ipython-notebook-custom-cells-and-execute-hook \"Question score (upvotes - downvotes)\") [ipython notebook: custom cells and execute hook](/questions/29888736/ipython-notebook-custom-cells-and-execute-hook)\n\n[35](/questions/33508377/read-cell-content-in-an-ipython-notebook \"Question score (upvotes - downvotes)\") [Read cell content in an ipython notebook](/questions/33508377/read-cell-content-in-an-ipython-notebook)\n\n[30](/questions/37592778/exporting-single-ipython-jupyter-notebook-cell-output \"Question score (upvotes - downvotes)\") [Exporting single iPython Jupyter Notebook cell output](/questions/37592778/exporting-single-ipython-jupyter-notebook-cell-output)\n\n[18](/questions/40518074/jupyter-get-arbitrary-notebook-cell-contents \"Question score (upvotes - downvotes)\") [Jupyter get arbitrary notebook cell\ncontents](/questions/40518074/jupyter-get-arbitrary-notebook-cell-contents)\n\n[2](/questions/41668399/change-output-in-ipython-while-cell-is-running \"Question score (upvotes - downvotes)\") [Change output in ipython while cell is running](/questions/41668399/change-output-in-ipython-while-cell-is-running)\n\n[1](/questions/49176817/save-the-output-of-a-cell-in-ipython-notebook \"Question score (upvotes - downvotes)\") [Save the output of a cell in iPython notebook? ](/questions/49176817/save-the-output-of-a-cell-in-ipython-notebook)\n\n[5](/questions/64572585/how-to-simultaneously-capture-and-display-ipython-jupyter-notebook-cell-output \"Question score (upvotes - downvotes)\") [How to simultaneously capture and display ipython (jupyter notebook) cell output?\n ... \n](https://rpg.stackexchange.com/questions/218292/does-a-vampire-on-a-path-of-humanity-still-benefit-from-the-effects-of-following)\n* [Why did my credit score drop when I added a gold card? ](https://money.stackexchange.com/questions/169064/why-did-my-credit-score-drop-when-i-added-a-gold-card)\n* [Statistical analysis and study design for small sample size](https://stats.stackexchange.com/questions/673918/statistical-analysis-and-study-design-for-small-sample-size)\n* [Why do \\_llseek and ftruncate64 have their argument order reversed from each other in Linux i386 syscall ABI? ](https://retrocomputing.stackexchange.com/questions/32332/why-do-llseek-and-ftruncate64-have-their-argument-order-reversed-from-each-othe)\n* [What syntax are commonly used for labels when pure identifiers are ambiguous? ](https://langdev.stackexchange.com/questions/4681/what-syntax-are-commonly-used-for-labels-when-pure-identifiers-are-ambiguous)\n* [How to batch convert PDF files to an image based PDF?\n](https://superuser.com/questions/1932462/how-to-batch-convert-pdf-files-to-an-image-based-pdf)\n* [Is my understanding of Kant's reasoning for space=form correct? ](https://philosophy.stackexchange.com/questions/134467/is-my-understanding-of-kants-reasoning-for-space-form-correct)\n* [Is is true that Julius Caesar made a pun about ‚ÄúRex‚Äù meaning ‚Äúking‚Äù and how did this pun work?\n ... \nPolicy](https://stackoverflow.com/legal/privacy-policy)\n* [Terms of Service](https://stackoverflow.com/legal/terms-of-service/public)\n* [Contact Us](/contact)\n* Your Privacy Choices\n* [Cookie Policy](https://policies.stackoverflow.co/stack-overflow/cookie-policy)\n\n##### [Stack Exchange Network](https://stackexchange.com)\n\n* [Technology](https://stackexchange.com/sites)\n* [Culture & recreation](https://stackexchange.com/sites)\n* [Life & arts](https://stackexchange.com/sites)\n* [Science](https://stackexchange.com/sites)\n* [Professional](https://stackexchange.com/sites)\n* [Business](https://stackexchange.com/sites)\n* [API](https://api.stackexchange.com/)\n* [Data](https://data.stackexchange.com/)\n\n* [Blog](https://stackoverflow.blog?blb=1)\n* [Facebook](https://www.facebook.com/officialstackoverflow/)\n* [Twitter](https://twitter.com/stackoverflow)\n* [LinkedIn](https://linkedin.com/company/stack-overflow)\n* [Instagram](https://www.instagram.com/thestackoverflow)\n\nSite design / logo ¬© 2025"
    ]
  },
  {
    "title": "How to get output model for a given cell in a JupyterLab ...",
    "url": "https://discourse.jupyter.org/t/how-to-get-output-model-for-a-given-cell-in-a-jupyterlab-extension/11342",
    "excerpts": [
      "Oct 21, 2021 ‚Äî I am able to access a cell model and its corresponding widget. How can I get the corresponding output model? I would like to know the output ...Read more"
    ]
  },
  {
    "title": "python - How to embed image or picture in jupyter notebook, either from a local machine or from a web resource? - Stack Overflow",
    "url": "https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o",
    "excerpts": [
      "[Skip to main content]()\n\n[](#) [Stack Overflow](https://stackoverflow.com)\n\n1. [About](https://stackoverflow.co/)\n2. Products\n3. [For Teams](https://stackoverflow.co/internal/)\n\n1. [Stack Internal Implement a knowledge platform layer to power your enterprise and AI tools. ](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-for-teams)\n2. [Stack Data Licensing Get access to top-class technical expertise with trusted & attributed content. ](https://stackoverflow.co/data-licensing/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=overflow-api)\n3. [Stack Ads Connect your brand to the world‚Äôs most trusted technologist communities. ](https://stackoverflow.co/advertising/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-advertising)\n4.\n ... \n](/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o)\n\n[Ask Question](/questions/ask)\n\nAsked\n\nModified [1 year, 8 months ago](?lastactivity \"2024-04-30 22:39:46Z\")\n\nViewed 921k times\n\n532\n\n[](/posts/32370281/timeline \"Show activity on this post.\") I would like to include image in a jupyter notebook. If I did the following, it works :\n\n```\nfrom IPython.display import Image\nImage(\"img/picture.png\")\n```\n\nBut I would like to include the images in a markdown cell and the following code gives a 404 error :\n\n```\n! [title](\"img/picture.png\")\n```\n\nI also tried\n\n```\n!\n ... \n```\nPATH = \"/Users/reblochonMasque/Documents/Drawings/\"\nImage(filename = PATH + \"My_picture.jpg\", width=100, height=100)\n```\n\n**if the image it wider than the display settings:** [thanks](https://stackoverflow.com/questions/36766082/how-to-display-large-image-from-ipython-notebook-with-scrolling)\n\nuse `unconfined=True` to disable max-width confinement of the image\n\n```\nfrom IPython.core.display import Image, display\ndisplay(Image(url='https://i.ytimg.com/vi/j22DmsZEv30/maxresdefault.jpg', width=1900, unconfined=True))\n```\n\n## or via markdown:\n\n* _**make sure the cell is a markdown cell**_ , and not a code cell, thanks @Ê∏∏ÂáØË∂Ö in the comments)\n* Please note that on some systems, the markdown does not allow white space in the filenames. Thanks to @CoffeeTableEspresso and @zebralamy in the comments)  \n  (On macos, as long as you are on a markdown cell you would do like this: `! [title](../image 1.png)` , and not worry about the white space). **for a web image:**\n\n```\n!\n ... \n0 Reply\n\n* Copy link\n\n[](/users/786559/ciprian-tomoiag%c4%83)\n\nCiprian TomoiagƒÉ\n\n[Ciprian TomoiagƒÉ](/users/786559/ciprian-tomoiag%c4%83) []()\n\ncan you put a local one with absolute path, via markdown ? i tried but it doesn't seem to work. Edit: Firefox (and Chrome) don't allow access to local files for security\n\n4 Reply\n\n* Copy link\n\n[](/users/9191338/youkaichao)\n\nyoukaichao\n\n[youkaichao](/users/9191338/youkaichao) []()\n\n@SwimBikeRun you should select the right cell type. your current cell type must be **code**\n\n2 Reply\n\n* Copy link\n\n[](/users/6565511/monty-s-flying-circus)\n\nMonty \\_s Flying Circus\n\n[Monty \\_s Flying Circus](/users/6565511/monty-s-flying-circus) []()\n\nUnderrated answer.\n ... \nAlternatively, you can use a plain HTML `<img src>` , which allows you to change height and width and is still read by the markdown interpreter:\n\n```\n<img src=\"subdirectory/MyImage.png\" width=60 height=60 />\n```\n\n[Share](/a/36410846 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/36410846/edit)\n\nFollow\n\n[edited Mar 29, 2020 at 17:33](/posts/36410846/revisions \"show all edits to this post\")\n\n[](/users/80869/piotr-czapla)\n\n[Piotr Czapla](/users/80869/piotr-czapla)\n\n26\\.8k 26 26 gold badges 106 106 silver badges 123 123 bronze badges\n\nanswered Apr 4, 2016 at 19:05\n\n[](/users/1060554/alistair)\n\n[Alistair](/users/1060554/alistair)\n\n1,249 1 1 gold badge 9 9 silver badges 4 4 bronze badges\n\n## 4 Comments\n\nAdd a comment\n\n[](/users/3318335/svend)\n\nSvend\n\n[Svend](/users/3318335/svend) []()\n\nGreat! I much prefer this answer since we have the control on the output size ! That's important for example when using the pelican tool to publish jupyter notebook as static html pages.\n5 Reply\n\n* Copy link\n\n[](/users/361609/colllin)\n\ncolllin\n\n[colllin](/users/361609/colllin) []()\n\nHTML doesn't need commas ‚Äî just put a space between your attributes ‚Äî and it's recommended to put quotes around all attribute values, e.g. `width=\"60\"` . 11 Reply\n\n* Copy link\n\n[](/users/2059820/babou)\n\nbabou\n\n[babou](/users/2059820/babou) []()\n\nYes it is awesome :) but ... the image is lost in PDF versions. Too bad. 1 Reply\n\n* Copy link\n\n[](/users/5481576/kai)\n\nKai\n\n[Kai](/users/5481576/kai) []()\n\nthis solution worked in Microsoft Edge, but does NOT work on Google Chrome. Unfortunately, Microsoft Edge keeps crashing, so having to go back to Chrome\n\n1 Reply\n\n* Copy link\n\nAdd a comment\n\n68\n\n[](/posts/57632351/timeline \"Show activity on this post.\") ## Insert the image directly in the Jupyter notebook. **Note: You should have a local copy of the image on your computer**\n\nYou can insert the image in the Jupyter notebook itself. This way you don't need to keep the image separately in the folder.\n**Steps:**\n\n1. Convert the cell to `markdown` by:\n   \n    + pressing M on the selected cell  \n         OR\n    + From menu bar, Cell > Cell Type > Markdown. ( **Note:** It's important to convert the cell to Markdown, otherwise the \"Insert Image\" option in Step 2 will not be active)\n2. Now go to menu bar and select Edit -> Insert Image. 3. Select image from your disk and upload. 4. Press `Ctrl` \\+ `Enter` or `Shift` \\+ `Enter` . This will make the image as part of the notebook and you don't need to upload in the directory or Github. I feel this looks more clean and not prone to broken URL issue.\n[Share](/a/57632351 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/57632351/edit)\n\nFollow\n\n[edited Aug 23, 2019 at 20:43](/posts/57632351/revisions \"show all edits to this post\")\n\n[](/users/8049790/ejderuby)\n\n[ejderuby](/users/8049790/ejderuby)\n\n735 6 6 silver badges 23 23 bronze badges\n\nanswered Aug 23, 2019 at 19:59\n\n[](/users/7929998/courage)\n\n[courage](/users/7929998/courage)\n\n815 1 1 gold badge 8 8 silver badges 9 9 bronze badges\n\n## 5 Comments\n\nAdd a comment\n\n[](/users/1204331/eldad-a)\n\neldad-a\n\n[eldad-a](/users/1204331/eldad-a) []()\n\nYet another helpful functionality added by the devs of `Jupyter` . One disadvantage to note, for cases when this matters: it would include the image in the `IPyNB Jason` code, thus making it much bigger and not as code revision friendly (e.g. when inspecting `diff` s).\n ... \nThis is Jupyter Notebook 7.4.5 in Firefox 145.0.2. 0 Reply\n\n* Copy link\n\nAdd a comment\n\n43\n\n[](/posts/58694656/timeline \"Show activity on this post.\") 1. Set cell mode to Markdown\n2. **Drag and drop your image into the cell. ** The following command will be created:\n\n`! [image.png](attachment:image.png)`\n\n1. Execute/Run the cell and the image shows up. The image is actually embedded in the ipynb Notebook and you don't need to mess around with separate files. This is unfortunately not working with Jupyter-Lab (v 1.1.4) yet.\nEdit: Works in JupyterLab Version 1.2.6\n\n[Share](/a/58694656 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/58694656/edit)\n\nFollow\n\n[edited Apr 30, 2020 at 4:24](/posts/58694656/revisions \"show all edits to this post\")\n\n[](/users/1204331/eldad-a)\n\n[eldad-a](/users/1204331/eldad-a)\n\n3,241 3 3 gold badges 26 26 silver badges 26 26 bronze badges\n\nanswered Nov 4, 2019 at 13:41\n\n[](/users/7108589/peter)\n\n[Peter](/users/7108589/peter)\n\n12\\.7k 2 2 gold badges 37 37 silver badges 52 52 bronze badges\n\n## 4 Comments\n\nAdd a comment\n\n[](/users/7219659/efueyo)\n\nefueyo\n\n[efueyo](/users/7219659/efueyo) []()\n\nYes bat when I want download as HTML embebed , I get the error \"nbconvert failed: missing attachment: imagen.png\". ¬øWhy?\n3 Reply\n\n* Copy link\n\n[](/users/161152/cgodley)\n\nCgodLEY\n\n[CgodLEY](/users/161152/cgodley) []()\n\nPasting an image from the clipboard also works for me (after setting cell mode to `Markdown` )\n\n0 Reply\n\n* Copy link\n\n[](/users/1349673/james-hirschorn)\n\nJames Hirschorn\n\n[James Hirschorn](/users/1349673/james-hirschorn) []()\n\nI wonder if an embedded image can be used with `HTML` ? That way we could specify the width and height. 1 Reply\n\n* Copy link\n\n[](/users/5386903/uniquegino)\n\nuniquegino\n\n[uniquegino](/users/5386903/uniquegino) []()\n\nas of JupyterLab version 3.6.8 \" `attachment:` \" needs to get removed to get the image display work, with markdown\n\n0 Reply\n\n* Copy link\n\nAdd a comment\n\n34\n\n[](/posts/54072803/timeline \"Show activity on this post.\") I know this is not fully relevant, but since this answer is ranked first many a times when you search ' _how to display images in Jupyter_ ', please consider this answer as well. You could use matplotlib to show an image as follows.\n ... \n[Share](/a/55073727 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/55073727/edit)\n\nFollow\n\nanswered Mar 9, 2019 at 3:32\n\n[](/users/1106466/cybervigilante)\n\n[cybervigilante](/users/1106466/cybervigilante)\n\n261 1 1 silver badge 6 6 bronze badges\n\n## Comments\n\nAdd a comment\n\n1\n\n[](/posts/69076493/timeline \"Show activity on this post.\") I'm assuming you mean in a markdown cell, inside a jupyter notebook. If so, here are the easiest ways for both:\n\n**From the local computer:**\n\nWith your file browser, navigate to the directory containing your picture. Then, drag and drop it into the cell you want to embed the picture. It must already be in markdown mode or nothing will happen. The pros of doing it this way are:\n\n* It's simple, just drag and drop\n\nThe cons of doing it this way:\n\n* You **cannot** copy and paste that link, and use it in any other cell.\nIf you want to use the same picture in any other notebook cell, you will need to find the picture, drag and drop into the cell again. * You should consider a file management scheme that will allow you to find a picture again, if you plan on using allot of pictures in your notebooks. This is an effective way of doing things that's worked for me. **From the web:**\n\nThe easiest way is to simply find a picture using your web browser, right click and \"Copy image\", paste this into the cell you want the image displayed. As above this does come with caveats in that you can't copy this and link and paste it into another cell. So if you plan on using the image again, you'll have to either paste into another cell, or save this image locally. The images are embedded in the notebook so should work anywhere the notebook is opened.\n ... \n](/questions/36766082/how-to-display-large-image-from-ipython-notebook-with-scrolling?noredirect=1)\n\n[3](/questions/71591993/how-to-upload-ipynb-with-images-in-its-markdown-cells-to-google-colab \"Question score (upvotes - downvotes)\") [How to upload ipynb with images in it's markdown cells to google colab](/questions/71591993/how-to-upload-ipynb-with-images-in-its-markdown-cells-to-google-colab?noredirect=1)\n\n[2](/questions/45929035/how-to-import-png-images-python \"Question score (upvotes - downvotes)\") [How to import png images Python](/questions/45929035/how-to-import-png-images-python?noredirect=1)\n\n[1](/questions/73706734/merge-image-with-jupyter-notebook-to-be-one-file \"Question score (upvotes - downvotes)\") [Merge Image with Jupyter Notebook To Be One File](/questions/73706734/merge-image-with-jupyter-notebook-to-be-one-file?noredirect=1)\n\n[0](/questions/60376242/embed-images-in-colab \"Question score (upvotes - downvotes)\") [Embed images in"
    ]
  },
  {
    "title": "Interface ICodeCellModel - JupyterLab Documentation",
    "url": "https://jupyterlab.readthedocs.io/en/stable/api/interfaces/cells.ICodeCellModel.html",
    "excerpts": [
      "GitHub Jupyter\n    * Preparing search index...\n    * The search index is not available\n @jupyterlab\n    * @jupyterlab\n    * cells\n    * ICodeCellModel\n\nInterface ICodeCellModel\nThe definition of a code cell.\ninterface ICodeCellModel { ¬†¬†¬†¬† contentChanged : ISignal < ICellModel , void > ; ¬†¬†¬†¬† executionCount : ExecutionCount ; ¬†¬†¬†¬† executionState : IExecutionState ; ¬†¬†¬†¬† id : string ; ¬†¬†¬†¬† isDirty : boolean ; ¬†¬†¬†¬† isDisposed : boolean ; ¬†¬†¬†¬† metadata : Omit < IBaseCellMetadata , \"trusted\" > ; ¬†¬†¬†¬† metadataChanged : ISignal < ICellModel , IMapChange < any > > ; ¬†¬†¬†¬† mimeType : string ; ¬†¬†¬†¬† mimeTypeChanged : ISignal < ¬†¬†¬†¬†¬†¬†¬†¬† CodeEditor . IModel , ¬†¬†¬†¬†¬†¬†¬†¬† IChangedArgs < string , string , string > , ¬†¬†¬†¬† > ; ¬†¬†¬†¬† outputs : IOutputAreaModel ; ¬†¬†¬†¬† selections : IObservableMap < ITextSelection [] > ; ¬†¬†¬†¬† sharedModel : ISharedCodeCell ; ¬†¬†¬†¬† stateChanged : ISignal < ICellModel , IChangedArgs < boolean , boolean , any > > ; ¬†¬†¬†¬† trusted : boolean ; ¬†¬†¬†¬† type : \"code\" ; ¬†¬†¬†¬† clearExecution () : void ; ¬†¬†¬†¬† deleteMetadata ( key : string ) : void ; ¬†¬†¬†¬† dispose () : void ; ¬†¬†¬†¬† getMetadata ( key : string ) : any ; ¬†¬†¬†¬† setMetadata ( key : string , value : any ) : void ; ¬†¬†¬†¬† toJSON () : ICodeCell ; }\nHierarchy ( View Summary )\n    * ICellModel\n       * ICodeCellModel\n\n\nImplemented by\n    * CodeCellModel\n\n    * Defined in packages/cells/src/model.ts:133\n\nGitHub Jupyter\n@jupyterlab\n    * Loading...\n\nGenerated using TypeDoc"
    ]
  },
  {
    "title": "How to get a reference to a CodeCell? - JupyterLab - Jupyter Community Forum",
    "url": "https://discourse.jupyter.org/t/how-to-get-a-reference-to-a-codecell/8517",
    "excerpts": [
      "[Jupyter Community Forum](/)\n\n# [How to get a reference to a CodeCell?](/t/how-to-get-a-reference-to-a-codecell/8517)\n\n[JupyterLab](/c/jupyterlab/17)\n\n[adpatter](https://discourse.jupyter.org/u/adpatter) 1\n\nI am able to iterate through the cells in a Notebook using the method below, however each cell implements ICellModel. In order to get the cell output I have cast the model to ICodeCellModel. However, what is the correct way to access the outputs of these Cells?\n\n```\nconst extension: JupyterFrontEndPlugin<object> = {\n  id: \"etc-jupyterlab-telemetry:plugin\",\n  autoStart: true,\n  requires: [INotebookTracker],\n  activate: (app: JupyterFrontEnd, tracker: INotebookTracker) => {\n\ntracker.widgetAdded.connect((tracker: INotebookTracker, nbPanel: NotebookPanel) => {\n\n  nbPanel.revealed.then((r)=>{\n    nbPanel.content.widgets.forEach(\n      (cell: Cell<ICellModel>,\n        index: number,\n        array: readonly Cell<ICellModel>[]\n      ) => {\n        console.log((cell.model as ICodeCellModel).toJSON());\n      })\n  })\n});\nreturn {};\n  }\n};\n```\n\n[jasongrout](https://discourse.jupyter.org/u/jasongrout) 2\n\nSee isCodeCellModel: [jupyterlab/model.ts at b4db7f03d2ad0b91a9c8c252c56ae5e4a9408fbb ¬∑ jupyterlab/jupyterlab ¬∑ GitHub](https://github.com/jupyterlab/jupyterlab/blob/b4db7f03d2ad0b91a9c8c252c56ae5e4a9408fbb/packages/cells/src/model.ts)\n\n[adpatter](https://discourse.jupyter.org/u/adpatter) 3\n\n[@jasongrout](/u/jasongrout) Thank you. I didn‚Äôt even know TypeScript had an `is` operator until today\n\nI see how that works. Thanks again.\n\n### Related topics\n\n|Topic |Replies |Views |Activity |\n| --- | --- | --- | --- |\n|[What is a `model` in JupyterLab?](https://discourse.jupyter.org/t/what-is-a-model-in-jupyterlab/8537)\n\n[JupyterLab](/c/jupyterlab/17) |3 |965 |April 13, 2021 |\n|[Accessing cell subelements using API](https://discourse.jupyter.org/t/accessing-cell-subelements-using-api/5320)\n\n[JupyterLab](/c/jupyterlab/17) |2 |761 |August 10, 2020 |\n|[How to get output model for a given cell in a JupyterLab extension?](https://discourse.jupyter.org/t/how-to-get-output-model-for-a-given-cell-in-a-jupyterlab-extension/11342)\n\n[extensions](/c/jupyterlab/extensions/43) |6 |3824 |November 7, 2021 |\n|[Retrieving cell metadata from cell](https://discourse.jupyter.org/t/retrieving-cell-metadata-from-cell/10252)\n\n[JupyterLab](/c/jupyterlab/17)\n\n[how-to](https://discourse.jupyter.org/tag/how-to) , [help-wanted](https://discourse.jupyter.org/tag/help-wanted) |2 |1358 |August 4, 2021 |\n|[Get all code and output above a cell in the notebook](https://discourse.jupyter.org/t/get-all-code-and-output-above-a-cell-in-the-notebook/26512)\n\n[JupyterLab](/c/jupyterlab/17) |0 |87 |June 24, 2024 |\n\n* [Home](/)\n* [Categories](/categories)\n* [Guidelines](/guidelines)\n* [Terms of Service](/tos)\n* [Privacy Policy](/privacy)\n\nPowered by [Discourse](https://www.discourse.org) , best viewed with JavaScript enabled\n"
    ]
  }
]
```

## Assistant

Good findings. Now I have the key info:

### Summary of Discovery

#### LiteLLM Multimodal Support ‚úÖ
LiteLLM fully supports multimodal/vision via the OpenAI format:
```python
messages=[{
    "role": "user",
    "content": [
        {"type": "text", "text": "What's in this image?"},
        {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
    ]
}]
```
- Supports base64 encoded images
- `litellm.supports_vision(model)` checks if a model supports vision
- Works with Claude, GPT-4V, Gemini, etc.

#### JupyterLab Cell Outputs API
From `ICodeCellModel`, code cells have an `outputs: IOutputAreaModel` property that contains execution outputs including images:
- Outputs contain MIME types like `image/png`, `image/jpeg`
- Image data is base64 encoded in the output model

#### What ai-jup Currently Does
[`_gatherContext()`](file:///Users/hamel/git/ai-jup/src/promptCell.ts#L198-L238) only extracts **source code** from preceding cells:
```typescript
precedingCode.push(cell.model.sharedModel.getSource());  // Only source, no outputs!
```

#### What's Needed for Multimodal
1. **Frontend** (TypeScript): Extract `image/png` and `image/jpeg` outputs from `ICodeCellModel.outputs`
2. **Add to context**: Pass base64 images in `IPromptContext`
3. **Backend** (Python): Format messages with image content blocks for LiteLLM
4. **Model validation**: Check `litellm.supports_vision(model)` before sending images

This is definitely achievable. Want me to implement the multimodal image extraction and LiteLLM integration?

## User

Yeah it should work seamlessly, meaning if there is a plot or something like that in the notebook, the ai prompt should be able to just natively see it.  I'm not sure all plots are base64, some plots are html I think, like altair or maybe seaborn.  You should think about how this would be robust if possible?  You also do want images that you bring in through attachments or markdown to work as well. Think about all the ways images or image like objects are rendered in notebooks.  

## Assistant

Great point. Let me investigate all the ways visual content appears in notebooks:

**Tool Use:** `web_search`

```json
{
  "objective": "JupyterLab notebook cell output MIME types - what MIME types are used for different visualizations like matplotlib, altair, plotly, seaborn, vega-lite",
  "search_queries": [
    "jupyter notebook output mime types image/png vegalite plotly",
    "jupyterlab IOutputAreaModel mime types"
  ]
}
```

**Tool Use:** `web_search`

```json
{
  "objective": "JupyterLab markdown cell attachments - how to access embedded images in markdown cells via extension API",
  "search_queries": [
    "jupyterlab markdown cell attachments api",
    "jupyter notebook cell attachments base64 image"
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_019cvnKLcjY9Q7AXwV3Fj9JE`

```
[
  {
    "title": "Custom mimetypes (MIME types) ‚Äî Jupyter Documentation 4.1.1 alpha documentation",
    "url": "https://docs.jupyter.org/en/latest/reference/mimetype.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\n[](../index.html)\n\n* [Try Jupyter](../start/index.html)\n* [Usage](../use/using.html)\n* [Projects](../projects/content-projects.html)\n* [Community](../community/content-community.html)\n* [Contributing](../contributing/content-contributor.html)\n* More\n  \n    + [Reference](content-reference.html)\n    + [jupyter.org](https://jupyter.org)\n\nSearch `Ctrl` \\+ `K`\n\n* [GitHub](https://github.com/jupyter/jupyter \"GitHub\")\n* [Discourse](https://discourse.jupyter.org \"Discourse\")\n\nSearch `Ctrl` \\+ `K`\n\n* [](../index.html)\n* [Reference](content-reference.html)\n* Custom mimetypes (MIME types)\n\n# Custom mimetypes (MIME types) [\\#]( \"Link to this heading\")\n\n## What‚Äôs a mimetype? [\\#]( \"Link to this heading\")\n\nWhen an internet request and response occurs, a `Content-Type` header is\npassed. A mimetype, also referred to as MIME type, identifies how the content\nthat is being returned should be handled or used, based on type, by the\napplication and browser. A MIME type is made up of a MIME _group_ (i.e.\napplication, image, audio, etc.) and a MIME _subtype_ . For example, a MIME\ntype is `image/png` where MIME _group_ is `image` and _subtype_ is `png` .\n\nAs types may contain vendor specific items, a [custom vendor specific MIME type](https://en.wikipedia.org/wiki/Media_type) , `vnd` , can be used. A vendor specific MIME type will contain `vnd` such as `application/vnd.jupyter.cells` .\n\n## Custom mimetypes used in Jupyter and IPython projects [\\#]( \"Link to this heading\")\n\n* `application/vnd.jupyter`\n* `application/vnd.jupyter.cells`\n* `application/vnd.jupyter.dragindex` used by nbdime\n* `application/x-ipynb+json` for notebooks\n* `text/html`\n  \n    + metadata:\n        \n        - `isolated: boolean` ‚Äì HTML should be rendered inside an `<iframe>` .\n\n## Listing of custom mimetypes used for display [\\#]( \"Link to this heading\")\n\n* `application/vnd.geo+json` \\- [GeoJSON spec](http://geojson.org/geojson-spec.html) `application/vnd.geo+json` is now deprecated and replaced by `application/geo+json`\n* `application/geo+json` \\- preferred [GeoJSON spec](http://geojson.org/geojson-spec.html)\n* `application/vnd.plotly.v1+json` \\- [Plotly JSON Schema](http://help.plot.ly/json-chart-schema/)\n* `application/vdom.v1+json` \\- [Virtual DOM spec](https://github.com/nteract/vdom/blob/master/docs/spec.md)\n\n[previous Reference](content-reference.html \"previous page\") [next IPython](ipython.html \"next page\")\n\nOn this page\n\n* [What‚Äôs a mimetype?]()\n* [Custom mimetypes used in Jupyter and IPython projects]()\n* [Listing of custom mimetypes used for display]()\n\n[Edit on GitHub](https://github.com/jupyter/jupyter/edit/master/docs/source/reference/mimetype.md)\n\n### This Page\n\n* [Show Source](../_sources/reference/mimetype.md.txt)\n\n¬© Copyright 2015, Jupyter Team, https://jupyter.org.\n\nCreated using [Sphinx](https://www.sphinx-doc.org/) 8\\.2.3.\n\nBuilt with the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html) 0\\.16.1.\n"
    ]
  },
  {
    "title": "File and Output Formats ‚Äî JupyterLab 4.6.0a0 documentation",
    "url": "https://jupyterlab.readthedocs.io/en/latest/user/file_formats.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\nüöÄ JupyterLab 4.5.0 is now available ¬∑ [INSTALL](https://jupyterlab.rtfd.io/en/latest/getting_started/installation.html) ¬∑ [RELEASE NOTES](https://jupyterlab.rtfd.io/en/latest/getting_started/changelog.html)\n\n[](../index.html)\n\nChoose version\n\n* [Get Started](../getting_started/overview.html)\n* [User Guide](index.html)\n* [Develop Extensions](../extension/extension_dev.html)\n* [Contribute](../developer/contributing.html)\n* [API Reference](../developer/api.html)\n\nSearch `Ctrl` \\+ `K`\n\n* [](https://jupyter.org \"jupyter.org\")\n* [GitHub](https://github.com/jupyterlab/jupyterlab \"GitHub\")\n* [Discourse](https://discourse.jupyter.org/c/jupyterlab/17 \"Discourse\")\n* [Zulip](https://jupyter.zulipchat.com//channel/469762-jupyterlab \"Zulip\")\n\nSearch `Ctrl` \\+ `K`\n\n* [](../index.html)\n* [User Guide](index.html)\n* File and Output Formats\n\n# File and Output Formats [\\#]( \"Link to this heading\")\n\nJupyterLab provides a unified architecture for viewing and editing data\nin a wide variety of formats. This model applies whether the data is in\na file or is provided by a kernel as rich cell output in a notebook or\ncode console.\n\nFor files, the data format is detected by the extension of the file (or\nthe whole filename if there is no extension). A single file extension\nmay have multiple editors or viewers registered. For example, a Markdown\nfile ( `.md` ) can be edited in the file editor or rendered and\ndisplayed as HTML. You can open different editors and viewers for a file\nby right-clicking on the filename in the file browser and using the\n‚ÄúOpen With‚Äù submenu:\n\nTo use these different data formats as output in a notebook or code\nconsole, you can use the relevant display API for the kernel you are\nusing. For example, the IPython kernel provides a variety of convenience\nclasses for displaying rich output:\n\n```\nfrom IPython.display import display , HTML \n display ( HTML ( '<h1>Hello World</h1>' ))\n```\n\nRunning this code will display the HTML in the output of a notebook or\ncode console cell:\n\nThe IPython display function can also construct a raw rich output\nmessage from a dictionary of keys (MIME types) and values (MIME data):\n\n```\nfrom IPython.display import display \n display ({ 'text/html' : '<h1>Hello World</h1>' , 'text/plain' : 'Hello World' }, raw = True )\n```\n\nOther Jupyter kernels offer similar APIs.\n\nThe rest of this section highlights some of the common data formats that\nJupyterLab supports by default. JupyterLab extensions can also add support for\nother file formats.\n\n## Markdown [\\#]( \"Link to this heading\")\n\n* File extension: `.md`\n* MIME type: `text/markdown`\n\nMarkdown is a simple and popular markup language used for text cells in\nthe Jupyter Notebook.\n\nMarkdown documents can be edited as text files or rendered inline:\n\nThe Markdown syntax supported in this mode is the same syntax used in\nthe Jupyter Notebook (for example, LaTeX equations work). As seen in the\nanimation, edits to the Markdown source are immediately reflected in the\nrendered version.\n\n## Images [\\#]( \"Link to this heading\")\n\n* File extensions: `.bmp` , `.gif` , `.jpeg` , `.jpg` , `.png` , `.svg`\n* MIME types: `image/bmp` , `image/gif` , `image/jpeg` , `image/png` , `image/svg+xml`\n\nJupyterLab supports image data in cell output and as files in the above\nformats. In the image file viewer, you can use keyboard shortcuts such\nas `+` and `-` to zoom the image, `[` and `]` to rotate the image,\nand `H` and `V` to flip the image horizontally and vertically. Use `I` to invert the colors, and use `0` to reset the image.\n\nTo edit an SVG image as a text file, right-click on the SVG filename in\nthe file browser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù\nsubmenu:\n\n## Delimiter-separated Values [\\#]( \"Link to this heading\")\n\n* File extension: `.csv`\n* MIME type: None\n\nFiles with rows of delimiter-separated values, such as CSV files, are a common format for\ntabular data. The default viewer for these files in JupyterLab is a\nhigh-performance data grid viewer which can display comma-separated, tab-separated, and\nsemicolon-separated values:\n\nWhile tab-separated value files can be read by the grid viewer, it currently does not automatically recognize `.tsv` files.\nTo view, you must change the extension to `.csv` and set the delimiter to tabs.\n\nTo edit a CSV file as a text file, right-click on the file in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\nJupyterLab‚Äôs grid viewer can open large files, up to the maximum string size of the particular browser.\nBelow is a table that shows the sizes of the largest test files we successfully opened in each browser we support:\n\n|Browser |Max Size |\n| --- | --- |\n|Firefox |1\\.04GB |\n|Chrome |730MB |\n|Safari |1\\.8GB |\n\nThe actual maximum size of files that can be successfully loaded will vary depending on the browser version and file content.\n\n## JSON [\\#]( \"Link to this heading\")\n\n* File extension: `.json`\n* MIME type: `application/json`\n\nJavaScript Object Notation (JSON) files are common in data science.\nJupyterLab supports displaying JSON data in cell output or viewing a\nJSON file using a searchable tree view:\n\nTo edit the JSON as a text file, right-click on the filename in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\n## HTML [\\#]( \"Link to this heading\")\n\n* File extension: `.html`\n* MIME type: `text/html`\n\nJupyterLab supports rendering HTML in cell output and editing HTML files\nas text in the file editor.\n\n## LaTeX [\\#]( \"Link to this heading\")\n\n* File extension: `.tex`\n* MIME type: `text/latex`\n\nJupyterLab supports rendering LaTeX equations in cell output and editing\nLaTeX files as text in the file editor.\n\n## PDF [\\#]( \"Link to this heading\")\n\n* File extension: `.pdf`\n* MIME type: `application/pdf`\n\nPDF is a common standard file format for documents. To view a PDF file\nin JupyterLab, double-click on the file in the file browser:\n\n## Vega/Vega-Lite [\\#]( \"Link to this heading\")\n\nVega:\n\n* File extensions: `.vg` , `.vg.json`\n* MIME type: `application/vnd.vega.v5+json`\n\nVega-Lite:\n\n* File extensions: `.vl` , `.vl.json`\n* MIME type: `application/vnd.vegalite.v3+json`\n\nVega and Vega-Lite are declarative visualization grammars that enable\nvisualizations to be encoded as JSON data. For more information, see the\ndocumentation of Vega or Vega-Lite. JupyterLab supports rendering Vega\n5\\.x and Vega-Lite 3.x data in files and cell output.\n\nVega-Lite 1.x files, with a `.vl` or `.vl.json` file extension, can\nbe opened by double-clicking the file in the file browser:\n\nThe files can also be opened in the JSON viewer or file editor through\nthe ‚ÄúOpen With‚Ä¶‚Äù submenu in the file browser content menu:\n\nAs with other files in JupyterLab, multiple views of a single file\nremain synchronized, enabling you to interactively edit and render\nVega/Vega-Lite visualizations:\n\nThe same workflow also works for Vega 2.x files, with a `.vg` or `.vg.json` file extension.\n\nOutput support for Vega/Vega-Lite in a notebook or code console is\nprovided through third-party libraries such as Altair (Python), the\nvegalite R package, or Vegas (Scala/Spark).\n\nA JupyterLab extension that supports Vega 3.x and Vega-Lite 2.x can be\nfound [here](https://github.com/jupyterlab/jupyter-renderers) .\n\n[previous Documents and Kernels](documents_kernels.html \"previous page\") [next Debugger](debugger.html \"next page\")\n\n¬© Copyright 2018-2025, Project Jupyter.  \nThe Jupyter Trademark is registered with the U.S. Patent & Trademark Office.\n\n[Privacy Policies](../privacy_policies.html)\n\nBuilt with the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html) 0\\.16.1.\n"
    ]
  },
  {
    "title": "MIME type warning when plotting data with R in a Jupyter notebook",
    "url": "https://stackoverflow.com/questions/53457000/mime-type-warning-when-plotting-data-with-r-in-a-jupyter-notebook",
    "excerpts": [
      "Every time I plot data with R (IRKernel) into a Jupyter notebook I get a warning message that says \"No renderer could be found for output. It has the following ..."
    ]
  },
  {
    "title": "Displaying Altair Charts ‚Äî Vega-Altair 6.0.0 documentation",
    "url": "https://altair-viz.github.io/user_guide/display_frontends.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\n[](../index.html)\n\n[Vega-Altair](../index.html)\n\n* [Getting Started](../getting_started/overview.html)\n* [User Guide](data.html)\n* [Examples](../gallery/index.html)\n* [API](api.html)\n* More\n  \n    + [Release Notes](https://github.com/vega/altair/releases)\n    + [About](../about/roadmap.html)\n\nSearch `Ctrl` \\+ `K`\n\n* [GitHub](https://github.com/vega/altair \"GitHub\")\n* [StackOverflow](https://stackoverflow.com/tags/altair \"StackOverflow\")\n\nSearch `Ctrl` \\+ `K`\n\n* [](../index.html)\n* [Specifying Data](data.html)\n* Displaying Altair Charts\n\n# Displaying Altair Charts [\\#]( \"Link to this heading\")\n\nAltair produces [Vega-Lite](http://vega.github.io/vega-lite) visualizations, which require a Javascript frontend to\ndisplay the charts. Because notebook environments combine a Python backend with a Javascript frontend,\nmany users find them convenient for using Altair.\nAltair charts work out-of-the-box on [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/) , [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) , [Zeppelin](https://zeppelin.apache.org/) ,\nand related notebook environments, so long as there is a web connection to load the\nrequired javascript libraries. Altair can also be used with various IDEs that are enabled to display Altair charts,\nand can be used offline in most platforms with an appropriate frontend extension enabled;\ndetails are below. ## Altair‚Äôs Renderer Framework [\\#]( \"Link to this heading\")\n\nBecause different display systems have different requirements and constraints, Altair provides\nan API to switch between various _renderers_ to tune Altair‚Äôs chart representation. These can be chosen with the renderer registry in `alt.renderers` . The most used built-in renderers are:\n\n`alt.renderers.enable(\"html\")`\n    _(the default)_ Output an HTML representation of the chart.\nThe HTML renderer works\nin [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) , [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/) , [Zeppelin](https://zeppelin.apache.org/) , [VSCode-Python](https://code.visualstudio.com/docs/python/python-tutorial) and many related notebook frontends,\nas well as Jupyter ecosystem tools like [nbviewer](https://nbviewer.jupyter.org/) and [nbconvert](https://nbconvert.readthedocs.io/) HTML output. It requires a web connection in order to load relevant Javascript libraries. `alt.renderers.enable(\"mimetype\")`\n    _(default prior to Altair 4.0):_ Output a vega-lite specific mimetype that can be\ninterpreted by appropriate frontend extensions to display charts. This also outputs\na PNG representation of the plot, which is useful to view plots offline or on\nplatforms that don‚Äôt support rendering vegaspecs, such as GitHub.\nIt works with\nnewer versions of [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) , [nteract](https://nteract.io) , and [VSCode-Python](https://code.visualstudio.com/docs/python/python-tutorial) , but does not work\nwith the [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/) , or with tools like [nbviewer](https://nbviewer.jupyter.org/) and [nbconvert](https://nbconvert.readthedocs.io/) . `alt.renderers.enable(\"jupyter\")`\n    _(added in version 5.3):_ Output the chart using [JupyterChart](interactions/jupyter_chart.html) . This renderer\nis compatible with environments that support third-party Jupyter Widgets including [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/) , [Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/) , [VSCode-Python](https://code.visualstudio.com/docs/python/python-tutorial) , and [Colab](https://colab.research.google.com) . It requires a web connection in order to load relevant Javascript libraries.\n ... \nIn addition, Altair includes the following renderers:\n\n* `\"default\"` , `\"colab\"` , `\"kaggle\"` , `\"zeppelin\"` : identical to `\"html\"`\n* `\"jupyterlab\"` , `\"nteract\"` : identical to `\"mimetype\"`\n* `\"png\"` : renderer that renders and converts the chart to PNG, outputting it\n  using the `\"image/png\"` MIME type. * `\"svg\"` : renderer that renders and converts the chart to an SVG image,\n  outputting it using the `\"image/svg+xml\"` MIME type. * `\"olli\"` : renderer that uses [Olli](https://mitvis.github.io/olli/) to generate accessible text structures for screen reader users. * `\"json\"` : renderer that outputs the raw JSON chart specification, using the `\"application/json\"` MIME type. You can use `alt.renderers.names()` to return all registered renderers as a Python list. Other renderers can be installed by third-party packages via Python‚Äôs entrypoints system or you can create your own,\nsee [Customizing Renderers](custom_renderers.html) .\n ... \nEven so, Altair can be used effectively in non-browser\nbased environments using the approaches described below. ### Static Image Renderers [\\#]( \"Link to this heading\")\n\nThe `\"png\"` and `\"svg\"` renderers rely on the JavaScript engine embedded in\nthe vl-convert optional dependency to generate static images from Vega-Lite chart\nspecifications. These static images are then displayed in IPython-based environments\nusing the Mime Renderer Extensions system. This approach may be used to display static\nversions of Altair charts inline in the [IPython QtConsole](https://qtconsole.readthedocs.io/en/stable/) and [Spyder](https://www.spyder-ide.org/) , as well as\nin browser-based environments like JupyterLab. The `\"svg\"` renderer is enabled like this:\n\n```\nalt . renderers . enable ( \"svg\" )\n```\n\nThe `\"png\"` renderer is enabled like this:\n\n```\nalt . renderers .\n ... \nCheck your\n   browser‚Äôs [Javascript Console](https://webmasters.stackexchange.com/questions/8525/how-do-i-open-the-javascript-console-in-different-browsers) to see if there are errors. 3. You may be failing to trigger the notebook‚Äôs display mechanism (see below). If you are working in a notebook environment, the chart is only displayed if the **last line of the cell evaluates to a chart object**\n\nBy analogy, consider the output of simple Python operations:\n\n```\n>>> x = 4  # no output here \n >>> x      # output here, because x is evaluated \n 4 \n >>> x * 2  # output here, because the expression is evaluated \n 8\n```\n\nIf the last thing you type consists of an assignment operation, there will be no\noutput displayed. This turns out to be true of Altair charts as well:\n\n```\nimport altair as alt \n from altair.datasets import data \n cars = data . cars . url \n\n chart = alt . Chart ( cars ) . mark_point () .\n ... \nencode ( \n    x = 'x:Q' , \n    y = 'y:Q' , \n    color = 'color:Q'  # <-- this field does not exist in the data! \n  ) ```\n\nAltair does not check whether fields are valid, because there are many avenues\nby which a field can be specified within the full schema, and it is too difficult\nto account for all corner cases. Improving the user experience in this is a\npriority; see [vega/vega-lite](https://github.com/vega/vega-lite/issues/3576) . #### Encodings with special characters [\\#]( \"Link to this heading\")\n\nThe Vega-Lite grammar on which Altair is based allows for encoding names to use\nspecial characters to access nested properties (See Vega-Lite‚Äôs [Field](https://vega.github.io/vega-lite/docs/field.html) documentation). This can lead to errors in Altair when trying to use such columns in your chart. For example, the following chart is invalid:\n\n```\nimport pandas as pd \n data = pd . DataFrame ({ 'x.value' : [ 1 , 2 , 3 ]}) \n\n alt . Chart ( data ) . mark_point () .\nencode ( \n    x = 'x.value:Q' , \n )\n```\n\nTo plot this data directly, you must escape the period in the field name:\n\n```\nimport pandas as pd \n data = pd . DataFrame ({ 'x.value' : [ 1 , 2 , 3 ]}) \n\n alt . Chart ( data ) . mark_point () . encode ( \n    x = r 'x\\.value:Q' , \n )\n```\n\nIn general, it is better to avoid special characters like `\". \"` , `\"[\"` , and `\"]\"` in your data sources where possible. ### Troubleshooting in JupyterLab [\\#]( \"Link to this heading\")\n\n#### VegaLite 4/5 Object [\\#]( \"Link to this heading\")\n\n_If you are using the Jupyter notebook rather than JupyterLab, then refer to_ [Notebook: VegaLite 4/5 object]()\n\nIf you are using JupyterLab (not Jupyter notebook) and see an error message\nmentioning either `VegaLite 4 object` or `VegaLite 5 object` , then this means\nthat you have enabled the `mimetype` renderer, but that your JupyterLab\nfrontend does not support the VegaLite 4 or 5 mimetype. The easiest solution is to use the default renderer:\n\n```\nalt . renderers .\nenable ( 'default' )\n```\n\nand rerun the cell with the chart. If you would like to use the mimetype rendering with JupyterLab,\nupdate JupyterLab to the newest version with `pip install -U jupyterlab` or `conda update jupyterlab` . #### VegaLite 3 Object [\\#]( \"Link to this heading\")\n\n_If you are using the Jupyter notebook rather than JupyterLab, then refer to_ [Notebook: VegaLite 3 object]()\n\nIf you are using JupyterLab (not Jupyter notebook) and see the following output:\n\n```\n< VegaLite 3 object >\n```\n\nThis most likely means that you are using too old a version of JupyterLab. Altair 3.0 or later works best with JupyterLab version 1.0 or later;\ncheck the version with:\n\n```\n$ jupyter lab --version\n1.2.0\n```\n\nIf you have an older jupyterlab version, then use `pip install -U jupyterlab` or `conda update jupyterlab` to update JupyterLab, depending on how you\nfirst installed it.\n#### JavaScript output is disabled in JupyterLab [\\#]( \"Link to this heading\")\n\nIf you are using JupyterLab and see the following output:\n\n```\nJavaScript output is disabled in JupyterLab\n```\n\nit can mean one of two things is wrong\n\n1. You are using an old version of Altair. JupyterLab only works with Altair\n   version 2.0 or newer; you can check the altair version by executing the\n   following in a notebook code cell:\n   \n   ```\n   import altair as alt \n    alt . __version__\n   ```\n   \n   If the version is older than 2.0, then exit JupyterLab and follow the\n   installation instructions at [Displaying in JupyterLab]() . 2. You have enabled the wrong renderer. JupyterLab works with the default\n   renderer, but if you have used `alt.renderers.enable()` to enable\n   another renderer, charts will no longer render correctly in JupyterLab. You can check which renderer is active by running:\n   \n   ```\n   import altair as alt \n    print ( alt . renderers .\n ... \nenable ( 'default' )\n```\n\n### Troubleshooting in Notebooks [\\#]( \"Link to this heading\")\n\n#### Notebook: VegaLite 4/5 object [\\#]( \"Link to this heading\")\n\n_If you are using JupyterLab rather than the Jupyter notebook, then refer to_ [VegaLite 4/5 Object]()\n\nIf you are using Jupyter Notebook (not JupyterLab) and see an error message\nmentioning either `VegaLite 4 object` or `VegaLite 5 object` , then this means\nthat you have enabled the `mimetype` renderer. The easiest solution is to use the default renderer:\n\n```\nalt . renderers . enable ( 'default' )\n```\n\nand rerun the cell with the chart. #### Notebook: VegaLite 3 object [\\#]( \"Link to this heading\")\n\n_If you are using JupyterLab rather than the Jupyter notebook, then refer to_ [VegaLite 3 Object]()\n\nIf you are using the notebook (not JupyterLab) and see the the following output:\n\n```\n< Vegalite 3 object >\n```\n\nit means that either:\n\n1. You have forgotten to enable the notebook renderer."
    ]
  },
  {
    "title": "outputarea - v4.4.10 - JupyterLab Documentation - Read the Docs",
    "url": "https://jupyterlab.readthedocs.io/en/4.4.x/api/modules/outputarea.html",
    "excerpts": [
      "Output areas are able to render results of several different mime types, which are implemented in the rendermime package. This list of mime types may be ..."
    ]
  }
]
```

**Tool Result:** `toolu_vrtx_01B9FD1J7EVQ3y1by3XGEvLQ`

```
[
  {
    "title": "File and Output Formats ‚Äî JupyterLab 4.6.0a0 documentation",
    "url": "https://jupyterlab.readthedocs.io/en/latest/user/file_formats.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\nüöÄ JupyterLab 4.5.0 is now available ¬∑ [INSTALL](https://jupyterlab.rtfd.io/en/latest/getting_started/installation.html) ¬∑ [RELEASE NOTES](https://jupyterlab.rtfd.io/en/latest/getting_started/changelog.html)\n\n[](../index.html)\n\nChoose version\n\n* [Get Started](../getting_started/overview.html)\n* [User Guide](index.html)\n* [Develop Extensions](../extension/extension_dev.html)\n* [Contribute](../developer/contributing.html)\n* [API Reference](../developer/api.html)\n\nSearch `Ctrl` \\+ `K`\n\n* [](https://jupyter.org \"jupyter.org\")\n* [GitHub](https://github.com/jupyterlab/jupyterlab \"GitHub\")\n* [Discourse](https://discourse.jupyter.org/c/jupyterlab/17 \"Discourse\")\n* [Zulip](https://jupyter.zulipchat.com//channel/469762-jupyterlab \"Zulip\")\n\nSearch `Ctrl` \\+ `K`\n\n* [](../index.html)\n* [User Guide](index.html)\n* File and Output Formats\n\n# File and Output Formats [\\#]( \"Link to this heading\")\n\nJupyterLab provides a unified architecture for viewing and editing data\nin a wide variety of formats. This model applies whether the data is in\na file or is provided by a kernel as rich cell output in a notebook or\ncode console.\n\nFor files, the data format is detected by the extension of the file (or\nthe whole filename if there is no extension). A single file extension\nmay have multiple editors or viewers registered. For example, a Markdown\nfile ( `.md` ) can be edited in the file editor or rendered and\ndisplayed as HTML. You can open different editors and viewers for a file\nby right-clicking on the filename in the file browser and using the\n‚ÄúOpen With‚Äù submenu:\n\nTo use these different data formats as output in a notebook or code\nconsole, you can use the relevant display API for the kernel you are\nusing. For example, the IPython kernel provides a variety of convenience\nclasses for displaying rich output:\n\n```\nfrom IPython.display import display , HTML \n display ( HTML ( '<h1>Hello World</h1>' ))\n```\n\nRunning this code will display the HTML in the output of a notebook or\ncode console cell:\n\nThe IPython display function can also construct a raw rich output\nmessage from a dictionary of keys (MIME types) and values (MIME data):\n\n```\nfrom IPython.display import display \n display ({ 'text/html' : '<h1>Hello World</h1>' , 'text/plain' : 'Hello World' }, raw = True )\n```\n\nOther Jupyter kernels offer similar APIs.\n\nThe rest of this section highlights some of the common data formats that\nJupyterLab supports by default. JupyterLab extensions can also add support for\nother file formats.\n\n## Markdown [\\#]( \"Link to this heading\")\n\n* File extension: `.md`\n* MIME type: `text/markdown`\n\nMarkdown is a simple and popular markup language used for text cells in\nthe Jupyter Notebook.\n\nMarkdown documents can be edited as text files or rendered inline:\n\nThe Markdown syntax supported in this mode is the same syntax used in\nthe Jupyter Notebook (for example, LaTeX equations work). As seen in the\nanimation, edits to the Markdown source are immediately reflected in the\nrendered version.\n\n## Images [\\#]( \"Link to this heading\")\n\n* File extensions: `.bmp` , `.gif` , `.jpeg` , `.jpg` , `.png` , `.svg`\n* MIME types: `image/bmp` , `image/gif` , `image/jpeg` , `image/png` , `image/svg+xml`\n\nJupyterLab supports image data in cell output and as files in the above\nformats. In the image file viewer, you can use keyboard shortcuts such\nas `+` and `-` to zoom the image, `[` and `]` to rotate the image,\nand `H` and `V` to flip the image horizontally and vertically. Use `I` to invert the colors, and use `0` to reset the image.\n\nTo edit an SVG image as a text file, right-click on the SVG filename in\nthe file browser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù\nsubmenu:\n\n## Delimiter-separated Values [\\#]( \"Link to this heading\")\n\n* File extension: `.csv`\n* MIME type: None\n\nFiles with rows of delimiter-separated values, such as CSV files, are a common format for\ntabular data. The default viewer for these files in JupyterLab is a\nhigh-performance data grid viewer which can display comma-separated, tab-separated, and\nsemicolon-separated values:\n\nWhile tab-separated value files can be read by the grid viewer, it currently does not automatically recognize `.tsv` files.\nTo view, you must change the extension to `.csv` and set the delimiter to tabs.\n\nTo edit a CSV file as a text file, right-click on the file in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\nJupyterLab‚Äôs grid viewer can open large files, up to the maximum string size of the particular browser.\nBelow is a table that shows the sizes of the largest test files we successfully opened in each browser we support:\n\n|Browser |Max Size |\n| --- | --- |\n|Firefox |1\\.04GB |\n|Chrome |730MB |\n|Safari |1\\.8GB |\n\nThe actual maximum size of files that can be successfully loaded will vary depending on the browser version and file content.\n\n## JSON [\\#]( \"Link to this heading\")\n\n* File extension: `.json`\n* MIME type: `application/json`\n\nJavaScript Object Notation (JSON) files are common in data science.\nJupyterLab supports displaying JSON data in cell output or viewing a\nJSON file using a searchable tree view:\n\nTo edit the JSON as a text file, right-click on the filename in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\n## HTML [\\#]( \"Link to this heading\")\n\n* File extension: `.html`\n* MIME type: `text/html`\n\nJupyterLab supports rendering HTML in cell output and editing HTML files\nas text in the file editor.\n\n## LaTeX [\\#]( \"Link to this heading\")\n\n* File extension: `.tex`\n* MIME type: `text/latex`\n\nJupyterLab supports rendering LaTeX equations in cell output and editing\nLaTeX files as text in the file editor.\n\n## PDF [\\#]( \"Link to this heading\")\n\n* File extension: `.pdf`\n* MIME type: `application/pdf`\n\nPDF is a common standard file format for documents. To view a PDF file\nin JupyterLab, double-click on the file in the file browser:\n\n## Vega/Vega-Lite [\\#]( \"Link to this heading\")\n\nVega:\n\n* File extensions: `.vg` , `.vg.json`\n* MIME type: `application/vnd.vega.v5+json`\n\nVega-Lite:\n\n* File extensions: `.vl` , `.vl.json`\n* MIME type: `application/vnd.vegalite.v3+json`\n\nVega and Vega-Lite are declarative visualization grammars that enable\nvisualizations to be encoded as JSON data. For more information, see the\ndocumentation of Vega or Vega-Lite. JupyterLab supports rendering Vega\n5\\.x and Vega-Lite 3.x data in files and cell output.\n\nVega-Lite 1.x files, with a `.vl` or `.vl.json` file extension, can\nbe opened by double-clicking the file in the file browser:\n\nThe files can also be opened in the JSON viewer or file editor through\nthe ‚ÄúOpen With‚Ä¶‚Äù submenu in the file browser content menu:\n\nAs with other files in JupyterLab, multiple views of a single file\nremain synchronized, enabling you to interactively edit and render\nVega/Vega-Lite visualizations:\n\nThe same workflow also works for Vega 2.x files, with a `.vg` or `.vg.json` file extension.\n\nOutput support for Vega/Vega-Lite in a notebook or code console is\nprovided through third-party libraries such as Altair (Python), the\nvegalite R package, or Vegas (Scala/Spark).\n\nA JupyterLab extension that supports Vega 3.x and Vega-Lite 2.x can be\nfound [here](https://github.com/jupyterlab/jupyter-renderers) .\n\n[previous Documents and Kernels](documents_kernels.html \"previous page\") [next Debugger](debugger.html \"next page\")\n\n¬© Copyright 2018-2025, Project Jupyter.  \nThe Jupyter Trademark is registered with the U.S. Patent & Trademark Office.\n\n[Privacy Policies](../privacy_policies.html)\n\nBuilt with the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html) 0\\.16.1.\n"
    ]
  },
  {
    "title": "python - Inserting image into IPython notebook markdown - Stack Overflow",
    "url": "https://stackoverflow.com/questions/10628262/inserting-image-into-ipython-notebook-markdown",
    "excerpts": [
      "[Skip to main content]()\n\n[](#) [Stack Overflow](https://stackoverflow.com)\n\n1. [About](https://stackoverflow.co/)\n2. Products\n3. [For Teams](https://stackoverflow.co/internal/)\n\n1. [Stack Internal Implement a knowledge platform layer to power your enterprise and AI tools. ](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-for-teams)\n2. [Stack Data Licensing Get access to top-class technical expertise with trusted & attributed content. ](https://stackoverflow.co/data-licensing/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=overflow-api)\n3. [Stack Ads Connect your brand to the world‚Äôs most trusted technologist communities. ](https://stackoverflow.co/advertising/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-advertising)\n4.\n ... \nCommunities for your favorite technologies. [Explore all Collectives](/collectives-all)\n2. Stack Internal\n   \n   Stack Overflow for Teams is now called **Stack Internal** . Bring the best of human thought and AI automation together at your work. [Try for free](https://stackoverflowteams.com/teams/create/free/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams) [Learn more](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams)\n3. [Stack Internal](javascript:void\\(0\\))\n4. Bring the best of human thought and AI automation together at your work. [Learn more](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams-compact)\n\n##### Collectives‚Ñ¢ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most.\n[Learn more about Collectives](/collectives)\n\n**Stack Internal**\n\nKnowledge at work\n\nBring the best of human thought and AI automation together at your work. [Explore Stack Internal](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=side-bar&utm_content=explore-teams-compact-popover)\n\n# [Inserting image into IPython notebook markdown](/questions/10628262/inserting-image-into-ipython-notebook-markdown)\n\n[Ask Question](/questions/ask)\n\nAsked\n\nModified [10 months ago](?lastactivity \"2025-02-15 21:24:34Z\")\n\nViewed 555k times\n\n332\n\n[](/posts/10628262/timeline \"Show activity on this post.\") I am starting to depend heavily on the IPython notebook app to develop and document algorithms. It is awesome; but there is something that seems like it should be possible, but I can't figure out how to do it:\n\nI would like to insert a local image into my (local) IPython notebook markdown to aid in documenting an algorithm.\nI know enough to add something like `<img src=\"image.png\">` to the markdown, but that is about as far as my knowledge goes. I assume I could put the image in the directory represented by 127.0.0.1:8888 (or some subdirectory) to be able to access it, but I can't figure out where that directory is. (I'm working on a mac.) So, is it possible to do what I'm trying to do without too much trouble?\n ... \nGetting an image into Jupyter NB is a much simpler operation than most people have alluded to here. 1. Simply create an empty Markdown cell. 2. Then drag-and-drop the image file into the empty Markdown cell. The Markdown code that will insert the image then appears. For example, a string shown highlighted in gray below will appear in the Jupyter cell:\n\n`! [Venus_flytrap_taxonomy.jpg](attachment:Venus_flytrap_taxonomy.jpg)`\n\n1. Then execute the Markdown cell by hitting Shift-Enter. The Jupyter server will then insert the image, and the image will then appear. I am running Jupyter notebook server is: 5.7.4 with Python 3.7.0 on Windows 7. This is so simple ! ! UPDATE AS OF March 18, 2021:\nThis simple \"Drag-and-Drop-from-Windows-File-System\" method still works fine in JupyterLab. JupyterLab inserts the proper HTML code to embed the image directly and permanently into the notebook so the image is stored in the .ipynb file.\nI am running Jupyter Lab v2.2.7 on Windows 10 Python 3.7.9 still works in JupyterLab. I am running Jupyter Lab v2.2.7 using Python 3.7.9 on Windows 10. This stopped working in Jupyter Classic Notebook v6.1.5 sometime last year. I reported an bug notice to the Jupyter Classic Notebook developers. It works again in the latest version of Jupyter Classic Notebook. I just tried it in v6.4 on 7/15/2021. Thank you Jupyter NB Classic Developers ! !\n ... \nThank you\n\n2 Reply\n\n* Copy link\n\n[](/users/32584/steve-l)\n\nSteve L\n\n[Steve L](/users/32584/steve-l) []()\n\nWhile I wasn't (like BND) able to drag a figure from my local machine into a remotely hosted notebook, I was able to do that when I was hosting locally, and moreover I confirmed that the actual image is embedded within the notebook file so you can then send the .ipynb file to others and the graphic will be preserved. The markdown cell with the graphic looks something like this in a plain text editor (image data shortened ...): { \"attachments\": { \"download.png\": { \"image/png\": \"iVBORw0KGgoAAAAN ... U5ErkJggg==\" } },\n\n0 Reply\n\n* Copy link\n\n[](/users/3412660/rich-lysakowski-phd)\n\nRich Lysakowski PhD\n\n[Rich Lysakowski PhD](/users/3412660/rich-lysakowski-phd) []()\n\nThis simple \"Drag-and-Drop-from-Windows-File-System\" method still works fine in JupyterLab. A bug was reported for Jupyter Classic Notebook v6.x.\n ... \ncommas are not needed in HTML commands. see my update\n\n0 Reply\n\n* Copy link\n\n[](/users/591223/vwvan)\n\nvwvan\n\n[vwvan](/users/591223/vwvan) []()\n\nThis is a better solution if you want to control the size of the displayed image in your Jupyter notebook. Otherwise it can work but be a horror story. 0 Reply\n\n* Copy link\n\nAdd a comment\n\n27\n\n[](/posts/33294167/timeline \"Show activity on this post.\") [Obsolete]\n\nIPython/Jupyter now has support for an extension modules that can insert images via copy and paste or drag & drop. <https://github.com/ipython-contrib/IPython-notebook-extensions>\n\nThe drag & drop extension seems to work in most browsers\n\n<https://github.com/ipython-contrib/IPython-notebook-extensions/tree/master/nbextensions/usability/dragdrop>\n\nBut copy and paste only works in Chrome.\n[Share](/a/33294167 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/33294167/edit)\n\nFollow\n\n[edited Oct 24, 2018 at 6:32](/posts/33294167/revisions \"show all edits to this post\")\n\nanswered Oct 23, 2015 at 2:59\n\n[](/users/548039/mike)\n\n[Mike](/users/548039/mike)\n\n2,757 5 5 gold badges 30 30 silver badges 41 41 bronze badges\n\n## 5 Comments\n\nAdd a comment\n\n[](/users/3274693/thoran)\n\nThoran\n\n[Thoran](/users/3274693/thoran) []()\n\nThis answer needs some upvotes. Very few know about Ipython extensions. 2 Reply\n\n* Copy link\n\n[](/users/680232/k-michael-aye)\n\nK.-Michael Aye\n\n[K.-Michael Aye](/users/680232/k-michael-aye) []()\n\njust tried drag'n'drop in Safari and Chrome, not working for me (OSX, Py3, all updates)\n\n0 Reply\n\n* Copy link\n\n[](/users/3899919/mysterious-otter)\n\nMysterious Otter\n\n[Mysterious Otter](/users/3899919/mysterious-otter) []()\n\nThe second link you posted is broken -- extension no longer supported?\n ... \nIn Markdown:\n\n```\n<img src=\"phuong huong xac dinh.PNG\">\n```\n\nCode:\n\n```\nfrom IPython.display import Image\nImage(filename='phuong huong xac dinh.PNG')\n```\n\n[Share](/a/33215876 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/33215876/edit)\n\nFollow\n\n[edited Oct 24, 2015 at 5:11](/posts/33215876/revisions \"show all edits to this post\")\n\n[](/users/880772/approxiblue)\n\n[approxiblue](/users/880772/approxiblue)\n\n7,162 16 16 gold badges 53 53 silver badges 59 59 bronze badges\n\nanswered Oct 19, 2015 at 13:35\n\n[](/users/5463056/hydroman)\n\n[hydroman](/users/5463056/hydroman)\n\n168 1 1 silver badge 5 5 bronze badges\n\n## 1 Comment\n\nAdd a comment\n\n[](/users/127971/michael)\n\nmichael\n\n[michael](/users/127971/michael) []()\n\ndocumentation reference [ipython.readthedocs.io/en/stable/api/generated/‚Ä¶](http://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html.display.Image) , although using markdown ( `!\n[alt text](foo.png \"the title\")` is imho preferable, unless using additional args from api is required. 1 Reply\n\n* Copy link\n\n15\n\n[](/posts/38621771/timeline \"Show activity on this post.\") **_First make sure you are in markdown edit model in the ipython notebook cell_**\n\nThis is an alternative way to the method proposed by others `<img src=\"myimage.png\">` :\n\n```\n! [title](img/picture.png)\n```\n\nIt also seems to work if the title is missing:\n\n```\n! [](img/picture.png)\n```\n\nNote no quotations should be in the path. Not sure if this works for paths with white spaces though!\n ... \n](/questions/32662961/how-to-upload-image-in-ipython-notebook-in-markdown-environment?noredirect=1)\n\n#### Related\n\n[19](/questions/26068316/embedding-image-in-ipython-notebook-for-distribution \"Question score (upvotes - downvotes)\") [Embedding image in ipython notebook for distribution](/questions/26068316/embedding-image-in-ipython-notebook-for-distribution)\n\n[4](/questions/26278480/insert-figure-into-ipython-markdown-cell \"Question score (upvotes - downvotes)\") [Insert figure into iPython markdown cell](/questions/26278480/insert-figure-into-ipython-markdown-cell)\n\n[46](/questions/33647774/how-to-include-two-pictures-side-by-side-in-for-ipython-notebook-jupyter \"Question score (upvotes - downvotes)\") [How to include two pictures side by side in for IPython Notebook (Jupyter)](/questions/33647774/how-to-include-two-pictures-side-by-side-in-for-ipython-notebook-jupyter)\n\n[0](/questions/36959465/ipython-not-displaying-image \"Question score (upvotes - downvotes)\") [Ipython not\ndisplaying image](/questions/36959465/ipython-not-displaying-image)\n\n[3](/questions/57930004/jupyter-notebook-position-embedded-image-in-markdown \"Question score (upvotes - downvotes)\") [Jupyter Notebook position embedded image in markdown](/questions/57930004/jupyter-notebook-position-embedded-image-in-markdown)\n\n[1](/questions/58446361/image-in-markdown-cell-jupyter-notebook \"Question score (upvotes - downvotes)\") [Image in Markdown cell Jupyter Notebook](/questions/58446361/image-in-markdown-cell-jupyter-notebook)\n\n[7](/questions/62617348/how-to-insert-image-from-url-in-jupyter-notebook-markdown \"Question score (upvotes - downvotes)\") [How to insert image from URL in Jupyter Notebook (Markdown)](/questions/62617348/how-to-insert-image-from-url-in-jupyter-notebook-markdown)\n\n[5](/questions/63077452/how-to-insert-an-image-in-jupyter-markdown-in-windows \"Question score (upvotes - downvotes)\") [how to insert an image in jupyter markdown in windows?\n ... \nPolicy](https://stackoverflow.com/legal/privacy-policy)\n* [Terms of Service](https://stackoverflow.com/legal/terms-of-service/public)\n* [Contact Us](/contact)\n* Your Privacy Choices\n* [Cookie Policy](https://policies.stackoverflow.co/stack-overflow/cookie-policy)\n\n##### [Stack Exchange Network](https://stackexchange.com)\n\n* [Technology](https://stackexchange.com/sites)\n* [Culture & recreation](https://stackexchange.com/sites)\n* [Life & arts](https://stackexchange.com/sites)\n* [Science](https://stackexchange.com/sites)\n* [Professional](https://stackexchange.com/sites)\n* [Business](https://stackexchange.com/sites)\n* [API](https://api.stackexchange.com/)\n* [Data](https://data.stackexchange.com/)\n\n* [Blog](https://stackoverflow.blog?blb=1)\n* [Facebook](https://www.facebook.com/officialstackoverflow/)\n* [Twitter](https://twitter.com/stackoverflow)\n* [LinkedIn](https://linkedin.com/company/stack-overflow)\n* [Instagram](https://www.instagram.com/thestackoverflow)\n\nSite design / logo ¬© 2025"
    ]
  },
  {
    "title": "attachments - v4.4.10 - JupyterLab Documentation - Read the Docs",
    "url": "https://jupyterlab.readthedocs.io/en/4.4.x/api/modules/attachments.html",
    "excerpts": [
      "These attachments can be connected to both markdown and raw cells. Attachments are able to render several different mime types, which are implemented in the¬†..."
    ]
  },
  {
    "title": "python - How to embed image or picture in jupyter notebook, either from a local machine or from a web resource? - Stack Overflow",
    "url": "https://stackoverflow.com/questions/32370281/how-to-embed-image-or-picture-in-jupyter-notebook-either-from-a-local-machine-o",
    "excerpts": [
      "[Skip to main content]()\n\n[](#) [Stack Overflow](https://stackoverflow.com)\n\n1. [About](https://stackoverflow.co/)\n2. Products\n3. [For Teams](https://stackoverflow.co/internal/)\n\n1. [Stack Internal Implement a knowledge platform layer to power your enterprise and AI tools. ](https://stackoverflow.co/internal/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-for-teams)\n2. [Stack Data Licensing Get access to top-class technical expertise with trusted & attributed content. ](https://stackoverflow.co/data-licensing/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=overflow-api)\n3. [Stack Ads Connect your brand to the world‚Äôs most trusted technologist communities. ](https://stackoverflow.co/advertising/?utm_medium=referral&utm_source=stackoverflow-community&utm_campaign=top-nav&utm_content=stack-overflow-advertising)\n4.\n ... \n```\nPATH = \"/Users/reblochonMasque/Documents/Drawings/\"\nImage(filename = PATH + \"My_picture.jpg\", width=100, height=100)\n```\n\n**if the image it wider than the display settings:** [thanks](https://stackoverflow.com/questions/36766082/how-to-display-large-image-from-ipython-notebook-with-scrolling)\n\nuse `unconfined=True` to disable max-width confinement of the image\n\n```\nfrom IPython.core.display import Image, display\ndisplay(Image(url='https://i.ytimg.com/vi/j22DmsZEv30/maxresdefault.jpg', width=1900, unconfined=True))\n```\n\n## or via markdown:\n\n* _**make sure the cell is a markdown cell**_ , and not a code cell, thanks @Ê∏∏ÂáØË∂Ö in the comments)\n* Please note that on some systems, the markdown does not allow white space in the filenames. Thanks to @CoffeeTableEspresso and @zebralamy in the comments)  \n  (On macos, as long as you are on a markdown cell you would do like this: `! [title](../image 1.png)` , and not worry about the white space). **for a web image:**\n\n```\n!\n ... \n0 Reply\n\n* Copy link\n\n[](/users/786559/ciprian-tomoiag%c4%83)\n\nCiprian TomoiagƒÉ\n\n[Ciprian TomoiagƒÉ](/users/786559/ciprian-tomoiag%c4%83) []()\n\ncan you put a local one with absolute path, via markdown ? i tried but it doesn't seem to work. Edit: Firefox (and Chrome) don't allow access to local files for security\n\n4 Reply\n\n* Copy link\n\n[](/users/9191338/youkaichao)\n\nyoukaichao\n\n[youkaichao](/users/9191338/youkaichao) []()\n\n@SwimBikeRun you should select the right cell type. your current cell type must be **code**\n\n2 Reply\n\n* Copy link\n\n[](/users/6565511/monty-s-flying-circus)\n\nMonty \\_s Flying Circus\n\n[Monty \\_s Flying Circus](/users/6565511/monty-s-flying-circus) []()\n\nUnderrated answer.\n ... \n**Steps:**\n\n1. Convert the cell to `markdown` by:\n   \n    + pressing M on the selected cell  \n         OR\n    + From menu bar, Cell > Cell Type > Markdown. ( **Note:** It's important to convert the cell to Markdown, otherwise the \"Insert Image\" option in Step 2 will not be active)\n2. Now go to menu bar and select Edit -> Insert Image. 3. Select image from your disk and upload. 4. Press `Ctrl` \\+ `Enter` or `Shift` \\+ `Enter` . This will make the image as part of the notebook and you don't need to upload in the directory or Github. I feel this looks more clean and not prone to broken URL issue.\n ... \nThis is Jupyter Notebook 7.4.5 in Firefox 145.0.2. 0 Reply\n\n* Copy link\n\nAdd a comment\n\n43\n\n[](/posts/58694656/timeline \"Show activity on this post.\") 1. Set cell mode to Markdown\n2. **Drag and drop your image into the cell. ** The following command will be created:\n\n`! [image.png](attachment:image.png)`\n\n1. Execute/Run the cell and the image shows up. The image is actually embedded in the ipynb Notebook and you don't need to mess around with separate files. This is unfortunately not working with Jupyter-Lab (v 1.1.4) yet.\nEdit: Works in JupyterLab Version 1.2.6\n\n[Share](/a/58694656 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/58694656/edit)\n\nFollow\n\n[edited Apr 30, 2020 at 4:24](/posts/58694656/revisions \"show all edits to this post\")\n\n[](/users/1204331/eldad-a)\n\n[eldad-a](/users/1204331/eldad-a)\n\n3,241 3 3 gold badges 26 26 silver badges 26 26 bronze badges\n\nanswered Nov 4, 2019 at 13:41\n\n[](/users/7108589/peter)\n\n[Peter](/users/7108589/peter)\n\n12\\.7k 2 2 gold badges 37 37 silver badges 52 52 bronze badges\n\n## 4 Comments\n\nAdd a comment\n\n[](/users/7219659/efueyo)\n\nefueyo\n\n[efueyo](/users/7219659/efueyo) []()\n\nYes bat when I want download as HTML embebed , I get the error \"nbconvert failed: missing attachment: imagen.png\". ¬øWhy?\n3 Reply\n\n* Copy link\n\n[](/users/161152/cgodley)\n\nCgodLEY\n\n[CgodLEY](/users/161152/cgodley) []()\n\nPasting an image from the clipboard also works for me (after setting cell mode to `Markdown` )\n\n0 Reply\n\n* Copy link\n\n[](/users/1349673/james-hirschorn)\n\nJames Hirschorn\n\n[James Hirschorn](/users/1349673/james-hirschorn) []()\n\nI wonder if an embedded image can be used with `HTML` ? That way we could specify the width and height. 1 Reply\n\n* Copy link\n\n[](/users/5386903/uniquegino)\n\nuniquegino\n\n[uniquegino](/users/5386903/uniquegino) []()\n\nas of JupyterLab version 3.6.8 \" `attachment:` \" needs to get removed to get the image display work, with markdown\n\n0 Reply\n\n* Copy link\n\nAdd a comment\n\n34\n\n[](/posts/54072803/timeline \"Show activity on this post.\") I know this is not fully relevant, but since this answer is ranked first many a times when you search ' _how to display images in Jupyter_ ', please consider this answer as well. You could use matplotlib to show an image as follows.\n ... \nMine says: \"The version of the notebook server is 4.0.4 and is running on: Python 2.7.8 (default, Jun 30 2014, 16:08:48) [MSC v.1500 64 bit (AMD64)]\" Here's a screenshot with the results: [i.imgur.com/4ISJFDE.png](http://i.imgur.com/4ISJFDE.png)\n\n1 Reply\n\n* Copy link\n\n[](/users/419399/intelligent-infrastructure)\n\nIntelligent-Infrastructure\n\n[Intelligent-Infrastructure](/users/419399/intelligent-infrastructure) []()\n\nbeware with the paths, what works locally does not work online, .e. imgs\\\\pic.png works locally, while online it is 1: case sensitve (Pic.png) 2: backslach and slash are not the same ... :/ ‚Äì\n\n0 Reply\n\n* Copy link\n\n[](/users/7339171/coffeetableespresso)\n\nCoffeeTableEspresso\n\n[CoffeeTableEspresso](/users/7339171/coffeetableespresso) []()\n\nNo spaces are allowed in the filename for markdown, in case anyone else was running into this problem\n\n1 Reply\n\n* Copy link\n\nAdd a comment\n\n13\n\n[](/posts/35025976/timeline \"Show activity on this post.\")\nIf you want to use the Jupyter Notebook API (and not the IPython one anymore), I find the [ipywidgets](https://github.com/ipython/ipywidgets) Jupyter's sub-project. You have an `Image` widget. Docstring specifies that you have a `value` parameter which is a bytes. So you can do:\n\n```\nimport requests\nfrom ipywidgets import Image\n\nImage(value=requests.get('https://octodex.github.com/images/yaktocat.png').content)\n```\n\nI agree, it's simpler to use the Markdown style. But it shows you the Image display Notebook API. You can also resize the image with the `width` and `height` parameters. [Share](/a/35025976 \"Short permalink to this answer\")\n\n[Improve this answer](/posts/35025976/edit)\n\nFollow\n\nanswered Jan 26, 2016 at 23:23\n\n[](/users/956765/jazzydag)\n\n[jazzydag](/users/956765/jazzydag)\n\n476 4 4 silver badges 6 6 bronze badges\n\n## 2 Comments\n\nAdd a comment\n\n[](/users/6925185/jottbe)\n\njottbe\n\n[jottbe](/users/6925185/jottbe) []()\n\nIt seems option `value` was renamed to `data` in the meantime.\nFor a locally stored image the code `Image(data=open(filename_png, 'rb').read())` worked for me. 0 Reply\n\n* Copy link\n\n[](/users/8896457/karol-zlot)\n\nKarol Zlot\n\n[Karol Zlot](/users/8896457/karol-zlot) []()\n\n@jottbe I needed to still use `value` , as `data` didn't work with `jupyter==1.0.0 notebook==6.4.0` . So the solution is: `Image(value=open(filename_png, 'rb').read())`\n\n0 Reply\n\n* Copy link\n\n12\n\n[](/posts/61219296/timeline \"Show activity on this post.\") While a lot of the above answers give ways to embed an image using a file or with Python code, there is a way to embed an image in the jupyter notebook itself using only markdown and base64! To view an image in the browser, you can visit the link `data:image/png;base64,**image data here**` for a base64-encoded PNG image, or `data:image/jpg;base64,**image data here**` for a base64-encoded JPG image. An example link can be found at the end of this answer.\n ... \nChange cell to markdown\n2. Copy past:\n\n```\n<div>\n<img src=attachment = \"name.png\" width=\"600\"/>\n</div>\n```\n\n1. Make an empty line after `</div>` in the same cell and drop your image using mouse into this line. 2. You will see something like that:\n\n```\n<div>\n<img src=\"attachment:name.png\" width=\"600\"/>\n</div>\n! [yourname.jpg](attachment:yourname.jpg)\n```\n\n1. Copy `yourname.jpg` and past it instead of `name.png`\n2. Remove `! [yourname.jpg](attachment:yourname.jpg)` . 3. The final result should look like:\n\n```\n<div>\n<img src=\"yourname.jpg\" width=\"600\"/>\n</div>\n```\n\n1.\n ... \n](/questions/36766082/how-to-display-large-image-from-ipython-notebook-with-scrolling?noredirect=1)\n\n[3](/questions/71591993/how-to-upload-ipynb-with-images-in-its-markdown-cells-to-google-colab \"Question score (upvotes - downvotes)\") [How to upload ipynb with images in it's markdown cells to google colab](/questions/71591993/how-to-upload-ipynb-with-images-in-its-markdown-cells-to-google-colab?noredirect=1)\n\n[2](/questions/45929035/how-to-import-png-images-python \"Question score (upvotes - downvotes)\") [How to import png images Python](/questions/45929035/how-to-import-png-images-python?noredirect=1)\n\n[1](/questions/73706734/merge-image-with-jupyter-notebook-to-be-one-file \"Question score (upvotes - downvotes)\") [Merge Image with Jupyter Notebook To Be One File](/questions/73706734/merge-image-with-jupyter-notebook-to-be-one-file?noredirect=1)\n\n[0](/questions/60376242/embed-images-in-colab \"Question score (upvotes - downvotes)\") [Embed images in\n ... \n](/questions/40705032/is-it-possible-to-paste-image-into-jupyter-notebook)\n\n[14](/questions/51975699/embedding-image-into-jupyter-notebook-and-exporting-to-html \"Question score (upvotes - downvotes)\") [embedding image into jupyter notebook and exporting to HTML](/questions/51975699/embedding-image-into-jupyter-notebook-and-exporting-to-html)\n\n[3](/questions/57930004/jupyter-notebook-position-embedded-image-in-markdown \"Question score (upvotes - downvotes)\") [Jupyter Notebook position embedded image in markdown](/questions/57930004/jupyter-notebook-position-embedded-image-in-markdown)\n\n[1](/questions/61305845/how-to-include-an-image-in-a-jupyter-lab-notebook \"Question score (upvotes - downvotes)\") [How to include an image in a Jupyter Lab notebook](/questions/61305845/how-to-include-an-image-in-a-jupyter-lab-notebook)\n\n[7](/questions/62617348/how-to-insert-image-from-url-in-jupyter-notebook-markdown \"Question score (upvotes - downvotes)\") [How to insert image from URL in Jupyter\n ... \nPolicy](https://stackoverflow.com/legal/privacy-policy)\n* [Terms of Service](https://stackoverflow.com/legal/terms-of-service/public)\n* [Contact Us](/contact)\n* Your Privacy Choices\n* [Cookie Policy](https://policies.stackoverflow.co/stack-overflow/cookie-policy)\n\n##### [Stack Exchange Network](https://stackexchange.com)\n\n* [Technology](https://stackexchange.com/sites)\n* [Culture & recreation](https://stackexchange.com/sites)\n* [Life & arts](https://stackexchange.com/sites)\n* [Science](https://stackexchange.com/sites)\n* [Professional](https://stackexchange.com/sites)\n* [Business](https://stackexchange.com/sites)\n* [API](https://api.stackexchange.com/)\n* [Data](https://data.stackexchange.com/)\n\n* [Blog](https://stackoverflow.blog?blb=1)\n* [Facebook](https://www.facebook.com/officialstackoverflow/)\n* [Twitter](https://twitter.com/stackoverflow)\n* [LinkedIn](https://linkedin.com/company/stack-overflow)\n* [Instagram](https://www.instagram.com/thestackoverflow)\n\nSite design / logo ¬© 2025"
    ]
  },
  {
    "title": "JupyterLab Extension - Quarto",
    "url": "https://quarto.org/docs/tools/jupyter-lab-extension.html",
    "excerpts": [
      "## On this page\n\n* [Overview]()\n* [Installing the Extension]()\n* [Using the Extension]()\n* [Disabling the Extension]()\n* [Uninstalling the Extension]()\n* [Reporting Issues]()\n\n* [Edit this page](https://github.com/quarto-dev/quarto-web/edit/main/docs/tools/jupyter-lab-extension.qmd)\n* [Report an issue](https://github.com/quarto-dev/quarto-cli/issues/new/choose)\n\n1. [Guide](../../docs/guide/index.html)\n2. Tools\n3. [JupyterLab](../../docs/tools/jupyter-lab.html)\n4. [JupyterLab Extension](../../docs/tools/jupyter-lab-extension.html)\n\n# JupyterLab Extension\n\n## Overview\n\nThe Quarto JuptyerLab extension enables JupyterLab Notebooks which use Quarto markdown to properly display the contents of the markdown cells. For example, when the Quarto JupyterLab extension is installed, your Notebook will show rendered previews of elements like Callouts, Divs, Mermaid charts, as well as other Quarto elements (including the document front matter as a title block).\n\n## Installing the Extension\n\nInstallation depends on which version of JupyterLab you have:\n\n* JupyterLab 4\n* JupyterLab 3\n\nYou can install the Quarto JupyterLab extension one of two ways:\n\n1. **In the JupyterLab UI** : Search for ‚ÄòQuarto‚Äô in the Extension Manager and install the `jupyterlab-quarto` extension. You‚Äôll be prompted to refresh the page when complete.\n2. **Using `pip`** :\n   \n   |Platform |Commands |\n   | --- | --- |\n   |Mac/Linux |```\n   **Terminal**\n   ```\n   \n   ```\n   []() python3 -m  pip install jupyterlab-quarto\n   ``` |\n   |Windows |```\n   **Terminal**\n   ```\n   \n   ```\n   []() py  - m pip install jupyterlab-quarto\n   ``` |\n\nYou can install the Quarto JupyterLab extension one of two ways:\n\n1. Using `pip` , you can install the `jupyterlab-quarto` by executing:\n   \n   |Platform |Commands |\n   | --- | --- |\n   |Mac/Linux |```\n   **Terminal**\n   ```\n   \n   ```\n   []() python3 -m  pip install jupyterlab-quarto==0.1.45\n   ``` |\n   |Windows |```\n   **Terminal**\n   ```\n   \n   ```\n   []() py  - m pip install jupyterlab-quarto == 0.1 . 45\n   ``` |\n   \n   \n   This is the preferred way to install the JupyterLab Quarto extension as this takes advantage of traditional python packaging and doesn‚Äôt require a rebuild of JupyterLab to complete the installation.\n2. In the JupyterLab UI, you can install the Quarto extension directly using the Extension Manager by searching for ‚ÄòQuarto‚Äô and installing the `@quarto/jupyterlab-quarto` extension. To complete the installation you need to rebuild JupyterLab (you should see a prompt to complete this once you‚Äôve installed the Quarto extension).\n\n## Using the Extension\n\nThe Quarto extension, once installed, will automatically render the contents of markdown cells within your notebook. Cells without Quarto specific markdown will render normally, while cells containing Quarto specific markdown will show a preview of the content in a more usable form.\n\nNote\n\nThe Quarto contents shown in your Notebooks will not match the rendered output precisely. For example, callouts shown in the Notebook don‚Äôt change their display based upon callout options you specify in your markdown.\n\n## Disabling the Extension\n\n1. If you installed the Quarto JupyterLab extension using `pip` , you can use the following commands to disable and enable the extension.\n   \n   To disable extension, use the following command:\n   \n   |Platform |Commands |\n   | --- | --- |\n   |Mac/Linux |```\n   **Terminal**\n   ```\n   \n   ```\n   []() python3 -m  jupyter labextension disable jupyterlab-quarto\n   ``` |\n   |Windows |```\n   **Terminal**\n   ```\n   \n   ```\n   []() py  - m jupyter labextension disable jupyterlab-quarto\n   ``` |\n   \n   \n   To enable the extension, use the following command:\n   \n   |Platform |Commands |\n   | --- | --- |\n   |Mac/Linux |```\n   **Terminal**\n   ```\n   \n   ```\n   []() python3 -m  jupyter labextension enable jupyterlab-quarto\n   ``` |\n   |Windows |```\n   **Terminal**\n   ```\n   \n   ```\n   []() py  - m jupyter labextension enable jupyterlab-quarto\n   ``` |\n2. If you installed the Quarto JupyterLab extension using the JupyterLab Notebook Extension Manager, you can use the UI directly to disable and enable the extension.\n\n## Uninstalling the Extension\n\n1. If you installed the extension using `pip` , you can uninstall the Quarto extension using `pip` , like so:\n   \n   |Platform |Commands |\n   | --- | --- |\n   |Mac/Linux |```\n   **Terminal**\n   ```\n   \n   ```\n   []() python3 -m  pip uninstall jupyterlab-quarto\n   ``` |\n   |Windows |```\n   **Terminal**\n   ```\n   \n   ```\n   []() py  - m pip uninstall jupyterlab-quarto\n   ``` |\n2. If you installed the extension using the JupyterLab Notebook Extension Manager, use the Extension Manager to uninstall the extension. To complete the uninstallation you need to rebuild JupyterLab (you should see a prompt to complete this once you‚Äôve uninstalled the Quarto extension).\n\n## Reporting Issues\n\nPlease report issues with the Quarto JuptyerLab extension [here](https://github.com/quarto-dev/quarto/issues/new) .\n\n[JupyterLab Basics](../../docs/tools/jupyter-lab.html)\n\n[RStudio Basics](../../docs/tools/rstudio.html)\n\nProudly supported by [](https://posit.co) [](https://posit.co)\n\n* [About](../../about.html)\n* [FAQ](../../docs/faq/index.html)\n* [License](../../license.html)\n* [Trademark](../../trademark.html)\n\n* [Edit this page](https://github.com/quarto-dev/quarto-web/edit/main/docs/tools/jupyter-lab-extension.qmd)\n* [Report an issue](https://github.com/quarto-dev/quarto-cli/issues/new/choose)\n\n* [](https://bsky.app/profile/quarto.org)\n* [](https://github.com/quarto-dev/quarto-cli)\n* [](https://quarto.org/docs/blog/index.xml)\n"
    ]
  }
]
```

## Assistant

Now I have a comprehensive picture. Let me write a summary:

### Comprehensive Discovery: Visual Content in Jupyter Notebooks

#### 1. Code Cell Outputs (from `ICodeCellModel.outputs`)

| Source | MIME Types | LLM-Ready? |
|--------|-----------|------------|
| **Matplotlib/Seaborn/PIL** | `image/png`, `image/jpeg` | ‚úÖ Direct base64 |
| **Altair/Vega-Lite** | `application/vnd.vegalite.v*+json` | ‚ö†Ô∏è JSON spec (LLMs can parse) |
| **Plotly** | `application/vnd.plotly.v1+json` | ‚ö†Ô∏è JSON spec |
| **Bokeh/HTML visualizations** | `text/html` | ‚ùå Needs screenshot |
| **IPython.display.Image** | `image/png`, `image/jpeg` | ‚úÖ Direct base64 |
| **SVG graphics** | `image/svg+xml` | ‚ö†Ô∏è Some LLMs support |

#### 2. Markdown Cell Attachments
- Drag-and-drop images are stored in cell metadata as `attachments`
- Format: `{"image.png": {"image/png": "base64data..."}}`
- Referenced via `![](attachment:image.png)`

#### 3. Strategy for Robust Multimodal Support

**Tier 1 - Direct image support (primary):**
- Extract `image/png`, `image/jpeg`, `image/gif` from code cell outputs
- Extract attachments from markdown cells
- Send as base64 to LLM

**Tier 2 - Declarative viz specs (bonus):**
- Extract Vega-Lite/Altair JSON specs ‚Üí LLMs can understand chart structure
- Include as text context: "This cell produced a Vega-Lite chart: {spec}"

**Tier 3 - HTML visualizations (fallback):**
- Note presence of HTML output: "This cell produced an interactive HTML visualization"
- Consider optional screenshot capability (complex, maybe future)

#### Implementation Plan

1. **Update `IPromptContext`** to include images array
2. **Extend `_gatherContext()`** to extract:
   - Image outputs from code cells
   - Attachments from markdown cells
3. **Backend**: Format messages with image content blocks for LiteLLM
4. **Check `litellm.supports_vision(model)`** before including images

Want me to implement this?

## User

ok I think you should start with Tier 1 please, please create tests for this as well. 

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "todo"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "todo"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "todo"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "todo"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "todo"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01S7QhwwXcxWMGPoF8MHEnWb`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"todo"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"todo"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"todo"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"todo"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"todo"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "in-progress"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "todo"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "todo"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "todo"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "todo"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/tokens.ts",
  "read_range": [
    110,
    130
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01QFyzdRKh4e1i3zwnfSc2VE`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"in-progress"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"todo"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"todo"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"todo"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"todo"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

**Tool Result:** `toolu_vrtx_0151JMQExNfNJXdR1QCucC5B`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/src/tokens.ts",
  "content": "[... omitted lines 1 to 109 ...]\n110:   /** Error message for 'error' events */\n111:   error?: string;\n112: }\n113: \n114: /**\n115:  * Prompt execution context containing gathered information.\n116:  */\n117: export interface IPromptContext {\n118:   preceding_code: string;\n119:   variables: Record<string, IVariableInfo>;\n120:   functions: Record<string, IFunctionInfo>;\n121: }\n122: \n123: /**\n124:  * Interface for the prompt model.\n125:  * Manages prompt state and emits streaming events.\n126:  */\n127: export interface IPromptModel {\n128:   /**\n129:    * Signal emitted when streaming events occur.\n130:    */\n[... omitted lines 131 to 273 ...]"
}
```

## Assistant

**Tool Use:** `edit_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/tokens.ts",
  "old_str": "[... old_str omitted in markdown version ...]",
  "new_str": "[... new_str omitted in markdown version ...]"
}
```

## User

**Tool Result:** `toolu_vrtx_01Q3FJsk7sWkT7HowiSqjDdZ`

```
{
  "diff": "```diff\nIndex: /Users/hamel/git/ai-jup/src/tokens.ts\n===================================================================\n--- /Users/hamel/git/ai-jup/src/tokens.ts\toriginal\n+++ /Users/hamel/git/ai-jup/src/tokens.ts\tmodified\n@@ -111,14 +111,30 @@\n   error?: string;\n }\n \n /**\n+ * Image extracted from notebook cells for multimodal context.\n+ */\n+export interface IImageContext {\n+  /** Base64 encoded image data */\n+  data: string;\n+  /** MIME type (image/png, image/jpeg, image/gif) */\n+  mimeType: 'image/png' | 'image/jpeg' | 'image/gif';\n+  /** Source of the image */\n+  source: 'output' | 'attachment';\n+  /** Cell index where the image was found */\n+  cellIndex: number;\n+}\n+\n+/**\n  * Prompt execution context containing gathered information.\n  */\n export interface IPromptContext {\n   preceding_code: string;\n   variables: Record<string, IVariableInfo>;\n   functions: Record<string, IFunctionInfo>;\n+  /** Images from preceding cells (outputs and markdown attachments) */\n+  images?: IImageContext[];\n }\n \n /**\n  * Interface for the prompt model.\n```",
  "lineRange": [
    114,
    137
  ]
}
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "in-progress"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "todo"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "todo"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "todo"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "read_range": [
    1,
    50
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01PUGt4j7zTEbacefmyd56ts`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"in-progress"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"todo"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"todo"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"todo"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

**Tool Result:** `toolu_vrtx_017oMYF38hX9hCfLzZZUvYuS`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "content": "1: /**\n2:  * Prompt cell management and execution.\n3:  */\n4: \n5: import { NotebookPanel, NotebookActions } from '@jupyterlab/notebook';\n6: import { Cell, ICellModel, MarkdownCell } from '@jupyterlab/cells';\n7: import type {\n8:   IPromptCellManager,\n9:   IKernelConnector,\n10:   IPromptModel,\n11:   IPromptContext,\n12:   IVariableInfo,\n13:   IFunctionInfo,\n14:   IExtensionSettings\n15: } from './tokens';\n16: import { parsePrompt, processPrompt } from './promptParser';\n17: import { PromptModel } from './promptModel';\n18: \n19: const PROMPT_CELL_CLASS = 'ai-jup-prompt-cell';\n20: const PROMPT_OUTPUT_CLASS = 'ai-jup-prompt-output';\n21: const PROMPT_METADATA_KEY = 'ai_jup';\n22: \n23: interface PromptMetadata {\n24:   isPromptCell: boolean;\n25:   model?: string;\n26: }\n27: \n28: /**\n29:  * Manages prompt cells within notebooks.\n30:  * Implements IPromptCellManager for dependency injection.\n31:  */\n32: export class PromptCellManager implements IPromptCellManager {\n33:   private _connectors: Map<string, IKernelConnector> = new Map();\n34:   private _settings: IExtensionSettings | null = null;\n35: \n36:   /**\n37:    * Set the settings instance.\n38:    */\n39:   setSettings(settings: IExtensionSettings): void {\n40:     this._settings = settings;\n41:   }\n42: \n43:   /**\n44:    * Set up a notebook for prompt cell handling.\n45:    */\n46:   setupNotebook(panel: NotebookPanel, connector: IKernelConnector): void {\n47:     const notebookId = panel.id;\n48:     this._connectors.set(notebookId, connector);\n49: \n50:     const notebook = panel.content;\n[... omitted lines 51 to 487 ...]"
}
```

## Assistant

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "read_range": [
    196,
    250
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01HE5zs6AZ1arJnZLiZZZmcb`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "content": "[... omitted lines 1 to 195 ...]\n196:    * Gather context for the prompt including preceding code and referenced items.\n197:    */\n198:   private async _gatherContext(\n199:     panel: NotebookPanel,\n200:     connector: IKernelConnector,\n201:     parsed: ReturnType<typeof parsePrompt>\n202:   ): Promise<IPromptContext> {\n203:     const notebook = panel.content;\n204:     const activeIndex = notebook.activeCellIndex;\n205: \n206:     // Get preceding code cells\n207:     const precedingCode: string[] = [];\n208:     for (let i = 0; i < activeIndex; i++) {\n209:       const cell = notebook.widgets[i];\n210:       if (cell.model.type === 'code') {\n211:         precedingCode.push(cell.model.sharedModel.getSource());\n212:       }\n213:     }\n214: \n215:     // Get referenced variables\n216:     const variables: Record<string, IVariableInfo> = {};\n217:     for (const varName of parsed.variables) {\n218:       const info = await connector.getVariable(varName);\n219:       if (info) {\n220:         variables[varName] = info;\n221:       }\n222:     }\n223: \n224:     // Get referenced functions\n225:     const functions: Record<string, IFunctionInfo> = {};\n226:     for (const funcName of parsed.functions) {\n227:       const info = await connector.getFunction(funcName);\n228:       if (info) {\n229:         functions[funcName] = info;\n230:       }\n231:     }\n232: \n233:     return {\n234:       preceding_code: precedingCode.join('\\n\\n'),\n235:       variables,\n236:       functions\n237:     };\n238:   }\n239: \n240:   /**\n241:    * Insert a markdown cell for the AI output.\n242:    * Always creates a new cell for each execution.\n243:    */\n244:   private _insertOutputCell(panel: NotebookPanel, promptCell: Cell): Cell {\n245:     const notebook = panel.content;\n246:     const promptIndex = notebook.widgets.indexOf(promptCell);\n247: \n248:     // Find where to insert - after the prompt cell and any existing output cells\n249:     let insertAfterIndex = promptIndex;\n250:     for (let i = promptIndex + 1; i < notebook.widgets.length; i++) {\n[... omitted lines 251 to 487 ...]"
}
```

## Assistant

Now I need to update the imports and the `_gatherContext` function to extract images. Let me also check how to access cell outputs properly:

**Tool Use:** `web_search`

```json
{
  "objective": "JupyterLab IOutputAreaModel how to iterate outputs and get MIME data - TypeScript extension example",
  "search_queries": [
    "jupyterlab IOutputAreaModel get output data",
    "jupyterlab extension cell outputs mime data"
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01R3vkU4oCfcPpQkT3p7i6AQ`

```
[
  {
    "title": "How to get output model for a given cell in a JupyterLab extension? - #6 by krassowski - extensions - Jupyter Community Forum",
    "url": "https://discourse.jupyter.org/t/how-to-get-output-model-for-a-given-cell-in-a-jupyterlab-extension/11342/6",
    "excerpts": [
      "[Jupyter Community Forum](/)\n\n# [How to get output model for a given cell in a JupyterLab extension?](/t/how-to-get-output-model-for-a-given-cell-in-a-jupyterlab-extension/11342)\n\n[JupyterLab](/c/jupyterlab/extensions/43) [extensions](/c/jupyterlab/extensions/43)\n\n[krassowski](https://discourse.jupyter.org/u/krassowski) 6\n\nTo start with, I am not convinced that trying to obtain this information from JupyterLab frontend is the prime way to approach the problem, but let‚Äôs entertain it for educational reasons.\n\n**First** , you can get the output of the cell from its model and your attempts to use `model.list` were not too far off. You may have seen empty values because the operations are asynchronous, i.e. the first time when Output Area becomes visible it is not yet populated. This can be resolved by listening on changes to its content:\n\nCode:\n\n```\n{\n  id: 'CellOutputExample',\n  autoStart: true,\n  requires: [\"@jupyterlab/notebook:INotebookTracker\"],\n  activate: function(app, notebookTracker) {\n      notebookTracker.widgetAdded.connect((tracker, panel) => {\n          let notebook = panel.content;\n          const notebookModel = notebook.model;\n          notebookModel.cells.changed.connect((_, change) => {\n              if (change.type != 'add') {\n                  return;\n              }\n              for (const cellModel of change.newValues) {\n                  // ensure we have CodeCellModel\n                  if (cellModel.type != 'code') {\n                      return;\n                  }\n                  // IOutputAreaModel\n                  let outputs = cellModel.outputs;\n                  if (!outputs) {\n                      continue;\n                  }\n                  outputs.changed.connect(() => {\n                      console.log('Outputs of the cell', cellModel.id, 'in', notebook.title.label, 'changed:');\n                      console.log(\n                          '\\tThere are now', outputs.length, 'outputs:'\n                      );\n                      for (let i = 0; i < outputs.length; i++) {\n                          // IOutputModel\n                          const outputModel = outputs.get(i);\n                          console.log('\\t\\t', outputModel.data);\n                          // also has `outputModel.executionCount` and `outputModel.metadata`\n                      }\n                  });\n              }\n          });\n      })\n  }\n}\n```\n\n(Note this is JavaScript for [GitHub - jupyterlab/jupyterlab-plugin-playground: A dynamic extension loader for JupyterLab](https://github.com/jupyterlab/jupyterlab-plugin-playground) , not a proper TypeScript code, it‚Äôs Sunday, I‚Äôm not opening my IDE today).\n\n**Second** , I would really avoid observing the results of execution by engaging the DOM or widgets at all; instead I would listen to the [`NotebookActions.executed`](https://jupyterlab.readthedocs.io/en/stable/api/classes/notebook.notebookactions-1.html) which also enables you to handle errors in a clean way. I also highly recommend sticking to the public members (do not use underscored properties like `._output` ). Peeking with the debugger/console is a good approach, but it should be secondary to reading the [API reference](https://jupyterlab.readthedocs.io/en/stable/api/) which let me quickly create the example above. I usually go back and forth between logging to console, API reference and - if needed - the actual source code of JupyterLab (and extensions). Also, a well-configured autocompletion can help a lot!\n\n**Third** , there is a lot of prior work mentioned in [the other DAG thread](https://discourse.jupyter.org/t/dag-based-notebooks/11173/2) but one other extension not mentioned there, but probably very relevant to what you are trying to achieve (though this might not be immediately obvious) is [GitHub - nbsafety-project/nbsafety: Fearless interactivity for Jupyter notebooks.](https://github.com/nbsafety-project/nbsafety) as it tracks the execution of the cells, their results **and** modifies the display of the notebook on the frontend to guide users on out-of-order execution. I think that building upon their approach is one of the most promising ways to implement DAG notebooks (with equally [GitHub - davidbrochart/akernel: Asynchronous, reactive Python Jupyter kernel.](https://github.com/davidbrochart/akernel) being another promising projects).\n\n**Finally** , in another thread you hypothesise that getting the result of cell execution is difficult in JupyterLab and _Maybe that is one of the reasons why other implementations of observable notebooks are based on extra kernels?_ . While I hope I demonstrated above that the premise is not entirely correct, I want to also point you to one of the previous experiments, [GitHub - cphyc/ipyspaghetti](https://github.com/cphyc/ipyspaghetti) which implements what I believe is the most functional DAG notebook for JupyterLab 3.x and it does not require an extra kernel. Maybe, just maybe, contributing to that project would be a better way forward rather than trying to re-implement it from scratch?\n\n3 Likes\n\n[show post in topic](/t/how-to-get-output-model-for-a-given-cell-in-a-jupyterlab-extension/11342)\n\n### Related topics\n\n|Topic |Replies |Views |Activity |\n| --- | --- | --- | --- |\n|[How to Send Jupyter Notebook Cell Output via Email or MS Teams?](https://discourse.jupyter.org/t/how-to-send-jupyter-notebook-cell-output-via-email-or-ms-teams/30012)\n\n[Zero to JupyterHub on Kubernetes](/c/jupyterhub/z2jh-k8s/5)\n\n[community](https://discourse.jupyter.org/tag/community) , [jupyterlab](https://discourse.jupyter.org/tag/jupyterlab) , [jupyterhub](https://discourse.jupyter.org/tag/jupyterhub) , [how-to](https://discourse.jupyter.org/tag/how-to) , [help-wanted](https://discourse.jupyter.org/tag/help-wanted) |1 |229 |November 12, 2024 |\n|[Output Callback in JupyterLab Extension](https://discourse.jupyter.org/t/output-callback-in-jupyterlab-extension/22240)\n\n[Widgets](/c/widgets/46)\n\n[jupyterlab](https://discourse.jupyter.org/tag/jupyterlab) , [help-wanted](https://discourse.jupyter.org/tag/help-wanted) |0 |393 |November 4, 2023 |\n|[What is a `model` in JupyterLab?](https://discourse.jupyter.org/t/what-is-a-model-in-jupyterlab/8537)\n\n[JupyterLab](/c/jupyterlab/17) |3 |963 |April 13, 2021 |\n|[How to get a reference to a CodeCell?](https://discourse.jupyter.org/t/how-to-get-a-reference-to-a-codecell/8517)\n\n[JupyterLab](/c/jupyterlab/17) |2 |894 |March 27, 2021 |\n|[DAG Based notebooks?](https://discourse.jupyter.org/t/dag-based-notebooks/11173)\n\n[JupyterLab](/c/jupyterlab/17) |7 |5234 |May 11, 2022 |\n\n* [Home](/)\n* [Categories](/categories)\n* [Guidelines](/guidelines)\n* [Terms of Service](/tos)\n* [Privacy Policy](/privacy)\n\nPowered by [Discourse](https://www.discourse.org) , best viewed with JavaScript enabled\n"
    ]
  },
  {
    "title": "File and Output Formats ‚Äî JupyterLab 4.4.10 documentation",
    "url": "https://jupyterlab.readthedocs.io/en/4.4.x/user/file_formats.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\nüöÄ You can now test JupyterLab 4.5.0 Release Candidate ¬∑ [INSTALL](https://jupyterlab.rtfd.io/en/latest/getting_started/installation.html) ¬∑ [RELEASE NOTES](https://jupyterlab.rtfd.io/en/latest/getting_started/changelog.html)\n\n[](../index.html)\n\nChoose version\n\n* [Get Started](../getting_started/overview.html)\n* [User Guide](index.html)\n* [Develop Extensions](../extension/extension_dev.html)\n* [Contribute](../developer/contributing.html)\n* [API Reference](../developer/api.html)\n\nSearch `Ctrl` \\+ `K`\n\n* [](https://jupyter.org \"jupyter.org\")\n* [GitHub](https://github.com/jupyterlab/jupyterlab \"GitHub\")\n* [Discourse](https://discourse.jupyter.org/c/jupyterlab/17 \"Discourse\")\n* [Zulip](https://jupyter.zulipchat.com//channel/469762-jupyterlab \"Zulip\")\n\nSearch `Ctrl` \\+ `K`\n\n* [Get Started](../getting_started/overview.html)\n* [User Guide](index.html)\n* [Develop Extensions](../extension/extension_dev.html)\n* [Contribute](../developer/contributing.html)\n* [API Reference](../developer/api.html)\n\n* [](https://jupyter.org \"jupyter.org\")\n* [GitHub](https://github.com/jupyterlab/jupyterlab \"GitHub\")\n* [Discourse](https://discourse.jupyter.org/c/jupyterlab/17 \"Discourse\")\n* [Zulip](https://jupyter.zulipchat.com//channel/469762-jupyterlab \"Zulip\")\n\nSection Navigation\n\n* [Announcements](announcements.html)\n* [The JupyterLab Interface](interface.html)\n* [JupyterLab URLs](urls.html)\n* [Working with Files](files.html)\n* [Text Editor](file_editor.html)\n* [Notebooks](notebook.html)\n* [Code Consoles](code_console.html)\n* [Completer](completer.html)\n* [Terminals](terminal.html)\n* [Managing Kernels and Terminals](running.html)\n* [Commands](commands.html)\n* [Documents and Kernels](documents_kernels.html)\n* [File and Output Formats](#)\n* [Debugger](debugger.html)\n* [Table Of Contents](toc.html)\n* [Extensions](extensions.html)\n* [Workspaces](workspaces.html)\n* [JupyterLab on JupyterHub](jupyterhub.html)\n* [Exporting Notebooks](export.html)\n* [Localization and language](language.html)\n* [Real Time Collaboration](rtc.html)\n* [Language Server Protocol support](lsp.html)\n* [Interface Customization](interface_customization.html)\n* [Advanced Usage](directories.html)\n* [JupyterLab on Binder](binder.html)\n\n* [](../index.html)\n* [User Guide](index.html)\n* File and Output Formats\n\n# File and Output Formats [\\#]( \"Link to this heading\")\n\nJupyterLab provides a unified architecture for viewing and editing data\nin a wide variety of formats. This model applies whether the data is in\na file or is provided by a kernel as rich cell output in a notebook or\ncode console.\n\nFor files, the data format is detected by the extension of the file (or\nthe whole filename if there is no extension). A single file extension\nmay have multiple editors or viewers registered. For example, a Markdown\nfile ( `.md` ) can be edited in the file editor or rendered and\ndisplayed as HTML. You can open different editors and viewers for a file\nby right-clicking on the filename in the file browser and using the\n‚ÄúOpen With‚Äù submenu:\n\nTo use these different data formats as output in a notebook or code\nconsole, you can use the relevant display API for the kernel you are\nusing. For example, the IPython kernel provides a variety of convenience\nclasses for displaying rich output:\n\n```\nfrom IPython.display import display , HTML \n display ( HTML ( '<h1>Hello World</h1>' ))\n```\n\nRunning this code will display the HTML in the output of a notebook or\ncode console cell:\n\nThe IPython display function can also construct a raw rich output\nmessage from a dictionary of keys (MIME types) and values (MIME data):\n\n```\nfrom IPython.display import display \n display ({ 'text/html' : '<h1>Hello World</h1>' , 'text/plain' : 'Hello World' }, raw = True )\n```\n\nOther Jupyter kernels offer similar APIs.\n\nThe rest of this section highlights some of the common data formats that\nJupyterLab supports by default. JupyterLab extensions can also add support for\nother file formats.\n\n## Markdown [\\#]( \"Link to this heading\")\n\n* File extension: `.md`\n* MIME type: `text/markdown`\n\nMarkdown is a simple and popular markup language used for text cells in\nthe Jupyter Notebook.\n\nMarkdown documents can be edited as text files or rendered inline:\n\nThe Markdown syntax supported in this mode is the same syntax used in\nthe Jupyter Notebook (for example, LaTeX equations work). As seen in the\nanimation, edits to the Markdown source are immediately reflected in the\nrendered version.\n\n## Images [\\#]( \"Link to this heading\")\n\n* File extensions: `.bmp` , `.gif` , `.jpeg` , `.jpg` , `.png` , `.svg`\n* MIME types: `image/bmp` , `image/gif` , `image/jpeg` , `image/png` , `image/svg+xml`\n\nJupyterLab supports image data in cell output and as files in the above\nformats. In the image file viewer, you can use keyboard shortcuts such\nas `+` and `-` to zoom the image, `[` and `]` to rotate the image,\nand `H` and `V` to flip the image horizontally and vertically. Use `I` to invert the colors, and use `0` to reset the image.\n\nTo edit an SVG image as a text file, right-click on the SVG filename in\nthe file browser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù\nsubmenu:\n\n## Delimiter-separated Values [\\#]( \"Link to this heading\")\n\n* File extension: `.csv`\n* MIME type: None\n\nFiles with rows of delimiter-separated values, such as CSV files, are a common format for\ntabular data. The default viewer for these files in JupyterLab is a\nhigh-performance data grid viewer which can display comma-separated, tab-separated, and\nsemicolon-separated values:\n\nWhile tab-separated value files can be read by the grid viewer, it currently does not automatically recognize `.tsv` files.\nTo view, you must change the extension to `.csv` and set the delimiter to tabs.\n\nTo edit a CSV file as a text file, right-click on the file in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\nJupyterLab‚Äôs grid viewer can open large files, up to the maximum string size of the particular browser.\nBelow is a table that shows the sizes of the largest test files we successfully opened in each browser we support:\n\n|Browser |Max Size |\n| --- | --- |\n|Firefox |1\\.04GB |\n|Chrome |730MB |\n|Safari |1\\.8GB |\n\nThe actual maximum size of files that can be successfully loaded will vary depending on the browser version and file content.\n\n## JSON [\\#]( \"Link to this heading\")\n\n* File extension: `.json`\n* MIME type: `application/json`\n\nJavaScript Object Notation (JSON) files are common in data science.\nJupyterLab supports displaying JSON data in cell output or viewing a\nJSON file using a searchable tree view:\n\nTo edit the JSON as a text file, right-click on the filename in the file\nbrowser and select the ‚ÄúEditor‚Äù item in the ‚ÄúOpen With‚Äù submenu:\n\n## HTML [\\#]( \"Link to this heading\")\n\n* File extension: `.html`\n* MIME type: `text/html`\n\nJupyterLab supports rendering HTML in cell output and editing HTML files\nas text in the file editor.\n\n## LaTeX [\\#]( \"Link to this heading\")\n\n* File extension: `.tex`\n* MIME type: `text/latex`\n\nJupyterLab supports rendering LaTeX equations in cell output and editing\nLaTeX files as text in the file editor.\n\n## PDF [\\#]( \"Link to this heading\")\n\n* File extension: `.pdf`\n* MIME type: `application/pdf`\n\nPDF is a common standard file format for documents. To view a PDF file\nin JupyterLab, double-click on the file in the file browser:\n\n## Vega/Vega-Lite [\\#]( \"Link to this heading\")\n\nVega:\n\n* File extensions: `.vg` , `.vg.json`\n* MIME type: `application/vnd.vega.v5+json`\n\nVega-Lite:\n\n* File extensions: `.vl` , `.vl.json`\n* MIME type: `application/vnd.vegalite.v3+json`\n\nVega and Vega-Lite are declarative visualization grammars that enable\nvisualizations to be encoded as JSON data. For more information, see the\ndocumentation of Vega or Vega-Lite. JupyterLab supports rendering Vega\n5\\.x and Vega-Lite 3.x data in files and cell output.\n\nVega-Lite 1.x files, with a `.vl` or `.vl.json` file extension, can\nbe opened by double-clicking the file in the file browser:\n\nThe files can also be opened in the JSON viewer or file editor through\nthe ‚ÄúOpen With‚Ä¶‚Äù submenu in the file browser content menu:\n\nAs with other files in JupyterLab, multiple views of a single file\nremain synchronized, enabling you to interactively edit and render\nVega/Vega-Lite visualizations:\n\nThe same workflow also works for Vega 2.x files, with a `.vg` or `.vg.json` file extension.\n\nOutput support for Vega/Vega-Lite in a notebook or code console is\nprovided through third-party libraries such as Altair (Python), the\nvegalite R package, or Vegas (Scala/Spark).\n\nA JupyterLab extension that supports Vega 3.x and Vega-Lite 2.x can be\nfound [here](https://github.com/jupyterlab/jupyter-renderers) .\n\n[previous Documents and Kernels](documents_kernels.html \"previous page\") [next Debugger](debugger.html \"next page\")\n\nOn this page\n\n* [Markdown]()\n* [Images]()\n* [Delimiter-separated Values]()\n* [JSON]()\n* [HTML]()\n* [LaTeX]()\n* [PDF]()\n* [Vega/Vega-Lite]()\n\n[Edit on GitHub](https://github.com/jupyterlab/jupyterlab/edit/4.4.x/docs/source/user/file_formats.rst)\n\n### This Page\n\n* [Show Source](../_sources/user/file_formats.rst.txt)\n\n¬© Copyright 2018-2025, Project Jupyter.  \nThe Jupyter Trademark is registered with the U.S. Patent & Trademark Office.\n\n[Privacy Policies](../privacy_policies.html)\n\nBuilt with the [PyData Sphinx Theme](https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html) 0\\.16.1.\n"
    ]
  },
  {
    "title": "Develop Extensions ‚Äî JupyterLab 4.6.0a0 documentation",
    "url": "https://jupyterlab.readthedocs.io/en/latest/extension/extension_dev.html",
    "excerpts": [
      "[Skip to main content]()\n\nBack to top\n\nüöÄ JupyterLab 4.5.0 is now available ¬∑ [INSTALL](https://jupyterlab.rtfd.io/en/latest/getting_started/installation.html) ¬∑ [RELEASE NOTES](https://jupyterlab.rtfd.io/en/latest/getting_started/changelog.html)\n\n[](../index.html)\n\nChoose version\n\n* [Get Started](../getting_started/overview.html)\n* [User Guide](../user/index.html)\n* [Develop Extensions](#)\n* [Contribute](../developer/contributing.html)\n* [API Reference](../developer/api.html)\n\nSearch `Ctrl` \\+ `K`\n\n* [](https://jupyter.org \"jupyter.org\")\n* [GitHub](https://github.com/jupyterlab/jupyterlab \"GitHub\")\n* [Discourse](https://discourse.jupyter.org/c/jupyterlab/17 \"Discourse\")\n* [Zulip](https://jupyter.zulipchat.com//channel/469762-jupyterlab \"Zulip\")\n\nSearch `Ctrl` \\+ `K`\n\n* [](../index.html)\n* Develop Extensions\n\n# Develop Extensions [\\#]( \"Link to this heading\")\n\nThe JupyterLab application is comprised of a core application object and a set of extensions.\nJupyterLab extensions provide nearly every function in JupyterLab, including notebooks, document editors and viewers, code consoles, terminals, themes, the file browser, contextual help system, debugger, and settings editor. Extensions even provide more fundamental parts of the application, such as the menu system, status bar, and the underlying communication mechanism with the server. A JupyterLab extension is a package that contains a number of JupyterLab plugins. We will discuss how to write a plugin, then how to package together a set of plugins into a JupyterLab extension. See the sections below for more detailed information, or browse the rest of this page for an overview. ## Other resources [\\#]( \"Link to this heading\")\n\nBefore we get started, here are some resources for hands-on practice or more in-depth reference documentation.\n### Tutorials [\\#]( \"Link to this heading\")\n\nLearn how to write JupyterLab extensions with these guides:\n\n* [Extension Tutorial](extension_tutorial.html) : A tutorial to learn how to make a simple JupyterLab extension. * [Making Extensions Compatible with Multiple Applications Tutorial](extension_multiple_ui.html) A tutorial for making extensions that work in both JupyterLab, Jupyter Notebook 7+ and more\n* The [JupyterLab Extension Examples Repository](https://github.com/jupyterlab/extension-examples) : A short tutorial series to learn how to develop extensions for JupyterLab by example. * [Common Extension Points](extension_points.html) : A list of the most common JupyterLab extension points. * Another common pattern for extending JupyterLab document widgets with application plugins is covered in [Documents](documents.html) .\n ... \n## Plugins [\\#]( \"Link to this heading\")\n\nA JupyterLab plugin is the basic unit of extensibility in JupyterLab. JupyterLab supports several types of plugins:\n\n* **Application plugins:** Application plugins are the fundamental building block of JupyterLab functionality. Application plugins interact with JupyterLab and other plugins by requiring services provided by other plugins, and optionally providing their own service to the system. Application plugins in core JupyterLab include the main menu system, the file browser, and the notebook, console, and file editor components. * **Mime renderer plugins:** Mime renderer plugins are simplified, restricted ways to extend JupyterLab to render custom mime data in notebooks and files. These plugins are automatically converted to equivalent application plugins by JupyterLab when they are loaded. Examples of mime renderer plugins that come in core JupyterLab are the pdf viewer, the JSON viewer, and the Vega viewer.\n ... \nA token defined in TypeScript can also define a TypeScript interface for the service associated with the token. If a package using the token uses TypeScript, the service will be type-checked against this interface when the package is compiled to JavaScript. Note\n\nJupyterLab uses tokens to identify services (instead of strings, for example) to prevent conflicts between identifiers and to enable type checking when using TypeScript. #### Publishing Tokens [\\#]( \"Link to this heading\")\n\nSince consumers will need to import a token used by a provider, the token should be exported in a published JavaScript package. Tokens will need to be deduplicated in JupyterLab‚Äîsee [Deduplication of Dependencies]() for more details. A pattern in core JupyterLab is to create and export a token from a third package that both the provider and consumer extensions import, rather than defining the token in the provider‚Äôs package.\n ... \nThis pattern enables users to seamlessly change the file browser in JupyterLab by writing their own extension that imports the same token from the `filebrowser` package and provides it to the system with their own alternative file browser service. ### MIME Renderer Plugins [\\#]( \"Link to this heading\")\n\nMIME Renderer plugins are a convenience for creating a plugin\nthat can render mime data in a notebook and files of the given mime type. MIME renderer plugins are more declarative and more restricted than standard plugins. A mime renderer plugin is an object with the fields listed in the [`IExtension`](../api/interfaces/rendermime_interfaces.IRenderMime.IExtension.html) object. JupyterLab has a [pdf mime renderer extension](https://github.com/jupyterlab/jupyterlab/tree/main/packages/pdf-extension) , for example. In core JupyterLab, this is used to view pdf files and view pdf data mime data in a notebook.\nWe have a [MIME renderer example](https://github.com/jupyterlab/extension-examples/tree/master/mimerenderer) walking through creating a mime renderer extension which adds mp4 video rendering to JupyterLab. The [extension template](https://github.com/jupyterlab/extension-template) supports MIME renderer extensions. The mime renderer can update its data by calling `.setData()` on the\nmodel it is given to render. This can be used for example to add a `png` representation of a dynamic figure, which will be picked up by a\nnotebook model and added to the notebook document. When using [`IDocumentWidgetFactoryOptions`](../api/interfaces/rendermime.IRenderMime.IDocumentWidgetFactoryOptions.html) ,\nyou can update the document model by\ncalling `.setData()` with updated data for the rendered MIME type. The\ndocument can then be saved by the user in the usual manner.\n ... \nSet the value to `true` if the default export of the main package module (i.e., the file listed in the `main` key of `package.json` ) is an application plugin or a list of application plugins. If your plugins are exported as default exports from a different module, set this to the relative path to the module (e.g., `\"lib/foo\"` ). Example:\n\n```\n\"jupyterlab\" : { \n  \"extension\" : true \n }\n```\n\n### MIME Renderer Plugins [\\#]( \"Link to this heading\")\n\nThe `jupyterlab.mimeExtension` field signifies that the package exports mime renderer plugins. Like the `jupyterlab.extension` field, the value can be a boolean (indicating a mime renderer plugin or list of mime renderer plugins is the default export from the `main` field), or a string, which is the relative path to the module exporting (as the default export) one or more mime renderer plugins.\n ... \n### Extension CSS [\\#]( \"Link to this heading\")\n\nIf your extension has a top-level `style` key in `package.json` , the CSS file it points to will be included on the page automatically. A convention in JupyterLab for deduplicating CSS on the page is that if your extension has a top-level `styleModule` key in `package.json` giving a JavaScript module that can be imported, it will be imported (as a JavaScript module) instead of importing the `style` key CSS file as a CSS file. ## Prebuilt Extensions [\\#]( \"Link to this heading\")\n\n### package.json metadata [\\#]( \"Link to this heading\")\n\nIn addition to the package metadata for [source extensions]() , prebuilt extensions have extra `jupyterlab` metadata. * `outputDir` : [Output Directory]()\n* `webpackConfig` : [Custom Rspack config]()\n\n#### Output Directory [\\#]( \"Link to this heading\")\n\nWhen JupyterLab builds the prebuilt extension, it creates a directory of files which can then be copied into the [appropriate install location]() .\n ... \nIt would be best if you had in mind that the JupyterLab packages that your extension depends on may differ from those published; this means that your extension doesn‚Äôt build with JupyterLab dependencies from your node\\_modules folder but those in JupyterLab source code. If you are using TypeScript, the TypeScript compiler would complain because the dependencies of your extension may differ from those in JupyterLab. For that reason, you need to add to your `tsconfig.json` the path where to search for these dependencies by adding the option [paths](https://www.typescriptlang.org/tsconfig) :\n\n```\n{ \n \"compilerOptions\" : { \n \"baseUrl\" : \".\" , \n \"paths\" : { \n \"@jupyterlab/*\" : [ \"../jupyterlab/packages/*\" ], \n \"*\" : [ \"node_modules/*\" ] \n } \n } \n }\n```\n\nWhen adding the path to find JupyterLab dependencies, it may cause troubles with other dependencies (like lumino or react) in your project because JupyterLab packages will take its dependencies from JupyterLab `node_modules` folder.\n ... \nA system package manager, or even just an administrative script that copies directories, could be used as well. To distribute a prebuilt extension, copy its [output directory]() to a location where JupyterLab will find it, typically `<sys-prefix>/share/jupyter/labextensions/<package-name>` , where `<package-name>` is the JavaScript package name in the `package.json` . For example, if your JavaScript package name is `@my-org/my-package` , then the appropriate directory would be `<sys-prefix>/share/jupyter/labextensions/@my-org/my-package` . The JupyterLab server makes the `static/` files available via a `/labextensions/` server handler. The settings and themes handlers in the server also load settings and themes from the prebuilt extension directories. If a prebuilt extension has the same name as a source extension, the prebuilt extension is preferred.\n ... \nWhen using local extensions and linked packages, you can run the command\n\n```\njupyterlab--watch\n```\n\nThis will cause the application to incrementally rebuild when one of the\nlinked packages changes. Note that only compiled JavaScript files (and\nthe CSS files) are watched by the Rspack process. This means that if\nyour extension is in TypeScript you‚Äôll have to run a `jlpm run build` before the changes will be reflected in JupyterLab. To avoid this step\nyou can also watch the TypeScript sources in your extension which is\nusually assigned to the `tsc -w` shortcut. If Rspack doesn‚Äôt seem to\ndetect the changes, this can be related to [the number of available watches](https://github.com/webpack/docs/wiki/troubleshooting) . Note that the application is built against **released** versions of the\ncore JupyterLab extensions.\n ... \nIf your package uses any other file type it must be converted to one of\nthe above types or [include a loader in the import statement](https://rspack.rs/api/loader-api/inline) . If you include a loader, the loader must be importable at build time, so if\nit is not already installed by JupyterLab, you must add it as a dependency\nof your extension. If your JavaScript is written in any other dialect than\nEMCAScript 6 (2015) it should be converted using an appropriate tool. You can use Rspack or Webpack to pre-build your extension to use any of it‚Äôs features\nnot enabled in our build configuration. To build a compatible package, set `output.libraryTarget` to `\"commonjs2\"` in your Webpack or Rspack configuration. (see [this](https://github.com/saulshanabrook/jupyterlab-webpack) example repo). If you publish your extension on `npm.org` , users will be able to install\nit as simply `jupyter labextension install <foo>` , where `<foo>` is\nthe name of the published `npm` package."
    ]
  },
  {
    "title": "OutputAreaModel | @jupyterlab",
    "url": "https://jupyterlab.readthedocs.io/en/3.3.x/api/modules/outputarea.outputareamodel.html",
    "excerpts": [
      "The namespace for OutputAreaModel class statics. Index. Classes. ContentFactory. Variables. defaultContentFactory. Variables. Read more"
    ]
  },
  {
    "title": "OutputAreaModel - JupyterLab Documentation",
    "url": "https://jupyterlab.readthedocs.io/en/4.0.x/api/classes/outputarea.OutputAreaModel-1.html",
    "excerpts": [
      "Construct a new observable outputs instance. Parameters. Optional options: IOutputAreaModel.IOptions. Returns OutputAreaModel.Read more"
    ]
  }
]
```

## Assistant

Good, now I understand the API. The key is `cellModel.outputs` which is an `IOutputAreaModel`, and each output has a `data` property containing MIME type keys. Let me now implement the image extraction:

**Tool Use:** `edit_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "old_str": "[... old_str omitted in markdown version ...]",
  "new_str": "[... new_str omitted in markdown version ...]"
}
```

## User

**Tool Result:** `toolu_vrtx_01Pk8TsoaC2KoUEWokmZtajP`

```
{
  "diff": "```diff\nIndex: /Users/hamel/git/ai-jup/src/promptCell.ts\n===================================================================\n--- /Users/hamel/git/ai-jup/src/promptCell.ts\toriginal\n+++ /Users/hamel/git/ai-jup/src/promptCell.ts\tmodified\n@@ -3,20 +3,26 @@\n  */\n \n import { NotebookPanel, NotebookActions } from '@jupyterlab/notebook';\n import { Cell, ICellModel, MarkdownCell } from '@jupyterlab/cells';\n+import { ICodeCellModel, isCodeCellModel } from '@jupyterlab/cells';\n import type {\n   IPromptCellManager,\n   IKernelConnector,\n   IPromptModel,\n   IPromptContext,\n   IVariableInfo,\n   IFunctionInfo,\n-  IExtensionSettings\n+  IExtensionSettings,\n+  IImageContext\n } from './tokens';\n import { parsePrompt, processPrompt } from './promptParser';\n import { PromptModel } from './promptModel';\n \n+/** Supported image MIME types for multimodal context */\n+const IMAGE_MIME_TYPES = ['image/png', 'image/jpeg', 'image/gif'] as const;\n+type ImageMimeType = (typeof IMAGE_MIME_TYPES)[number];\n+\n const PROMPT_CELL_CLASS = 'ai-jup-prompt-cell';\n const PROMPT_OUTPUT_CLASS = 'ai-jup-prompt-output';\n const PROMPT_METADATA_KEY = 'ai_jup';\n \n```",
  "lineRange": [
    5,
    23
  ]
}
```

## Assistant

Now let me update the `_gatherContext` function to extract images:

**Tool Use:** `edit_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptCell.ts",
  "old_str": "[... old_str omitted in markdown version ...]",
  "new_str": "[... new_str omitted in markdown version ...]"
}
```

## User

**Tool Result:** `toolu_vrtx_01QQDNFFCmV6xiVqL8QYymHw`

```
{
  "diff": "```diff\nIndex: /Users/hamel/git/ai-jup/src/promptCell.ts\n===================================================================\n--- /Users/hamel/git/ai-jup/src/promptCell.ts\toriginal\n+++ /Users/hamel/git/ai-jup/src/promptCell.ts\tmodified\n@@ -208,16 +208,27 @@\n   ): Promise<IPromptContext> {\n     const notebook = panel.content;\n     const activeIndex = notebook.activeCellIndex;\n \n-    // Get preceding code cells\n+    // Get preceding code cells and extract images from outputs\n     const precedingCode: string[] = [];\n+    const images: IImageContext[] = [];\n+\n     for (let i = 0; i < activeIndex; i++) {\n       const cell = notebook.widgets[i];\n-      if (cell.model.type === 'code') {\n-        precedingCode.push(cell.model.sharedModel.getSource());\n+      const cellModel = cell.model;\n+\n+      if (cellModel.type === 'code') {\n+        precedingCode.push(cellModel.sharedModel.getSource());\n+        // Extract images from code cell outputs\n+        if (isCodeCellModel(cellModel)) {\n+          this._extractImagesFromCodeCell(cellModel, i, images);\n         }\n+      } else if (cellModel.type === 'markdown') {\n+        // Extract images from markdown cell attachments\n+        this._extractImagesFromMarkdownCell(cellModel, i, images);\n       }\n+    }\n \n     // Get referenced variables\n     const variables: Record<string, IVariableInfo> = {};\n     for (const varName of parsed.variables) {\n@@ -238,13 +249,86 @@\n \n     return {\n       preceding_code: precedingCode.join('\\n\\n'),\n       variables,\n-      functions\n+      functions,\n+      images: images.length > 0 ? images : undefined\n     };\n   }\n \n   /**\n+   * Extract images from code cell outputs.\n+   */\n+  private _extractImagesFromCodeCell(\n+    cellModel: ICodeCellModel,\n+    cellIndex: number,\n+    images: IImageContext[]\n+  ): void {\n+    const outputs = cellModel.outputs;\n+    if (!outputs) {\n+      return;\n+    }\n+\n+    for (let j = 0; j < outputs.length; j++) {\n+      const outputModel = outputs.get(j);\n+      const data = outputModel.data;\n+\n+      // Check each supported image MIME type\n+      for (const mimeType of IMAGE_MIME_TYPES) {\n+        const imageData = data[mimeType];\n+        if (imageData && typeof imageData === 'string') {\n+          images.push({\n+            data: imageData,\n+            mimeType: mimeType as ImageMimeType,\n+            source: 'output',\n+            cellIndex\n+          });\n+          break; // Only take the first matching image type per output\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Extract images from markdown cell attachments.\n+   */\n+  private _extractImagesFromMarkdownCell(\n+    cellModel: ICellModel,\n+    cellIndex: number,\n+    images: IImageContext[]\n+  ): void {\n+    // Attachments are stored in cell metadata under 'attachments'\n+    const attachments = cellModel.getMetadata('attachments') as\n+      | Record<string, Record<string, string>>\n+      | undefined;\n+\n+    if (!attachments) {\n+      return;\n+    }\n+\n+    // Iterate through each attachment\n+    for (const [_filename, mimeData] of Object.entries(attachments)) {\n+      if (!mimeData || typeof mimeData !== 'object') {\n+        continue;\n+      }\n+\n+      // Check each supported image MIME type\n+      for (const mimeType of IMAGE_MIME_TYPES) {\n+        const imageData = mimeData[mimeType];\n+        if (imageData && typeof imageData === 'string') {\n+          images.push({\n+            data: imageData,\n+            mimeType: mimeType as ImageMimeType,\n+            source: 'attachment',\n+            cellIndex\n+          });\n+          break; // Only take the first matching image type per attachment\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n    * Insert a markdown cell for the AI output.\n    * Always creates a new cell for each execution.\n    */\n   private _insertOutputCell(panel: NotebookPanel, promptCell: Cell): Cell {\n```",
  "lineRange": [
    201,
    328
  ]
}
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "completed"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "completed"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "in-progress"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "todo"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01MVdFKqnuPbxFefXNqRdKDe`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"completed"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"completed"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"in-progress"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"todo"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

## Assistant

Now let me update the backend to handle images. First, let me read the current handler code more fully:

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "read_range": [
    1,
    100
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01Xnavw32pqbdewpZ7aBRicz`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "content": "1: \"\"\"Server handlers for AI prompt processing.\"\"\"\n2: import json\n3: import os\n4: import re\n5: \n6: from jupyter_server.base.handlers import APIHandler\n7: from jupyter_server.utils import url_path_join\n8: from tornado.web import authenticated\n9: from tornado.iostream import StreamClosedError\n10: \n11: # Regex for validating tool names (Python identifiers)\n12: TOOL_NAME_RE = re.compile(r\"^[A-Za-z_][A-Za-z0-9_]*$\")\n13: \n14: try:\n15:     import anthropic\n16:     HAS_ANTHROPIC = True\n17: except ImportError:\n18:     HAS_ANTHROPIC = False\n19: \n20: \n21: class PromptHandler(APIHandler):\n22:     \"\"\"Handler for AI prompt requests with streaming support.\"\"\"\n23: \n24:     @authenticated\n25:     async def post(self):\n26:         \"\"\"Process a prompt and stream the response with optional tool loop.\"\"\"\n27:         try:\n28:             data = self.get_json_body() or {}\n29:             if not isinstance(data, dict):\n30:                 self.set_status(400)\n31:                 self.finish({\"error\": \"Invalid JSON body\"})\n32:                 return\n33:             prompt = data.get(\"prompt\", \"\")\n34:             context = data.get(\"context\", {})\n35:             variables = context.get(\"variables\", {})\n36:             functions = context.get(\"functions\", {})\n37:             preceding_code = context.get(\"preceding_code\", \"\")\n38:             model = data.get(\"model\", \"claude-sonnet-4-20250514\")\n39:             kernel_id = data.get(\"kernel_id\")  # For server-side tool execution\n40:             max_steps = int(data.get(\"max_steps\", 1))  # Max tool loop iterations\n41: \n42:             system_prompt = self._build_system_prompt(preceding_code, variables, functions)\n43:             \n44:             self.set_header(\"Content-Type\", \"text/event-stream\")\n45:             self.set_header(\"Cache-Control\", \"no-cache\")\n46:             self.set_header(\"Connection\", \"keep-alive\")\n47: \n48:             if not HAS_ANTHROPIC:\n49:                 self.set_status(500)\n50:                 self.finish({\"error\": \"anthropic package not installed\"})\n51:                 return\n52: \n53:             api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n54:             if not api_key:\n55:                 self.set_status(500)\n56:                 self.finish({\"error\": \"ANTHROPIC_API_KEY environment variable not set\"})\n57:                 return\n58: \n59:             client = anthropic.Anthropic(api_key=api_key)\n60:             \n61:             tools = self._build_tools(functions)\n62:             \n63:             # Get kernel for tool execution if max_steps >= 1\n64:             kernel = None\n65:             if max_steps >= 1 and kernel_id:\n66:                 kernel_manager = self.settings.get(\"kernel_manager\")\n67:                 if kernel_manager:\n68:                     kernel = kernel_manager.get_kernel(kernel_id)\n69:             \n70:             messages = [{\"role\": \"user\", \"content\": prompt}]\n71:             steps = 0\n72:             \n73:             while True:\n74:                 current_tool_call = None\n75:                 current_text_block = None\n76:                 tool_input_buffer = \"\"\n77:                 text_buffer = \"\"\n78:                 assistant_content = []\n79:                 \n80:                 with client.messages.stream(\n81:                     model=model,\n82:                     max_tokens=4096,\n83:                     system=system_prompt,\n84:                     messages=messages,\n85:                     tools=tools if tools else anthropic.NOT_GIVEN,\n86:                 ) as stream:\n87:                     for event in stream:\n88:                         if hasattr(event, 'type'):\n89:                             if event.type == 'content_block_delta':\n90:                                 # Handle both text and tool input deltas\n91:                                 if hasattr(event.delta, 'text'):\n92:                                     text_buffer += event.delta.text\n93:                                     await self._write_sse({\"text\": event.delta.text})\n94:                                 if hasattr(event.delta, 'partial_json') and current_tool_call:\n95:                                     tool_input_buffer += event.delta.partial_json\n96:                                     await self._write_sse({\"tool_input\": event.delta.partial_json})\n97:                             elif event.type == 'content_block_start':\n98:                                 if hasattr(event.content_block, 'type'):\n99:                                     if event.content_block.type == 'tool_use':\n100:                                         current_tool_call = {\n[... omitted lines 101 to 780 ...]"
}
```

## Assistant

**Tool Use:** `Grep`

```json
{
  "pattern": "def _build_system_prompt",
  "path": "/Users/hamel/git/ai-jup/ai_jup/handlers.py"
}
```

## User

**Tool Result:** `toolu_vrtx_01BDnLesRPCMXySB89mmzpWq`

```
[
  "/Users/hamel/git/ai-jup/ai_jup/handlers.py:411:def _build_system_prompt(self, preceding_code: str, variables: dict, functions: dict) -> list:"
]
```

## Assistant

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "read_range": [
    411,
    500
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01RU8ajGHkXei3EP1CvfuRW5`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "content": "[... omitted lines 1 to 410 ...]\n411:     def _build_system_prompt(self, preceding_code: str, variables: dict, functions: dict) -> list:\n412:         \"\"\"Build the system prompt with context as content blocks for caching.\n413:         \n414:         Returns a list of content blocks suitable for Anthropic's system parameter.\n415:         The base instructions are cached (stable across calls), while dynamic\n416:         context (code, variables) is not cached.\n417:         \"\"\"\n418:         # Base instructions - stable, cacheable (requires ~1024 tokens minimum)\n419:         base_instructions = (\n420:             \"You are an AI assistant embedded in a Jupyter notebook. \"\n421:             \"Help the user with their data science and programming tasks. \"\n422:             \"When generating code, write Python that can be executed in the notebook. \"\n423:             \"Be concise and practical. \"\n424:             \"When you need to perform computations or get data, use the available tools. \"\n425:             \"Tools return rich results including DataFrames as HTML tables and matplotlib figures as images.\"\n426:         )\n427:         \n428:         blocks = [\n429:             {\n430:                 \"type\": \"text\",\n431:                 \"text\": base_instructions,\n432:                 \"cache_control\": {\"type\": \"ephemeral\"}\n433:             }\n434:         ]\n435:         \n436:         # Dynamic context - not cached since it changes per-cell\n437:         dynamic_parts = []\n438:         \n439:         if preceding_code:\n440:             dynamic_parts.append(f\"## Preceding Code Context\\n```python\\n{preceding_code}\\n```\")\n441:         \n442:         if variables:\n443:             var_desc = \"## Available Variables\\n\"\n444:             for name, info in variables.items():\n445:                 var_desc += f\"- `{name}`: {info.get('type', 'unknown')} = {info.get('repr', 'N/A')}\\n\"\n446:             dynamic_parts.append(var_desc)\n447:         \n448:         if functions:\n449:             func_desc = \"## Available Functions (you can call these as tools)\\n\"\n450:             for name, info in functions.items():\n451:                 sig = info.get('signature', '()')\n452:                 doc = info.get('docstring', 'No documentation')\n453:                 func_desc += f\"- `{name}{sig}`: {doc}\\n\"\n454:             dynamic_parts.append(func_desc)\n455:         \n456:         if dynamic_parts:\n457:             blocks.append({\n458:                 \"type\": \"text\", \n459:                 \"text\": \"\\n\\n\".join(dynamic_parts)\n460:             })\n461:         \n462:         return blocks\n463: \n464:     def _build_tools(self, functions: dict) -> list:\n465:         \"\"\"Build Anthropic tool definitions from function info.\"\"\"\n466:         tools = []\n467:         for name, info in functions.items():\n468:             params = info.get('parameters', {})\n469:             # Convert parameters to valid JSON Schema format\n470:             properties = {}\n471:             required = []\n472:             for param_name, param_info in params.items():\n473:                 # Convert Python type names to JSON Schema types\n474:                 python_type = param_info.get('type', 'string')\n475:                 json_type = self._python_type_to_json_schema(python_type)\n476:                 properties[param_name] = {\n477:                     \"type\": json_type,\n478:                     \"description\": param_info.get('description', param_name)\n479:                 }\n480:                 # Only add to required if no default value\n481:                 if 'default' not in param_info:\n482:                     required.append(param_name)\n483:             \n484:             tool = {\n485:                 \"name\": name,\n486:                 \"description\": info.get('docstring', f\"Call the {name} function\"),\n487:                 \"input_schema\": {\n488:                     \"type\": \"object\",\n489:                     \"properties\": properties,\n490:                     \"required\": required\n491:                 }\n492:             }\n493:             tools.append(tool)\n494:         return tools\n495:     \n496:     def _python_type_to_json_schema(self, python_type: str) -> str:\n497:         \"\"\"Convert Python type name to JSON Schema type.\"\"\"\n498:         type_map = {\n499:             'int': 'integer',\n500:             'float': 'number',\n[... omitted lines 501 to 780 ...]"
}
```

## Assistant

Now I need to update the handler to:
1. Extract images from the context
2. Build multimodal user messages with image content blocks

Let me update the `post` method to extract images and modify the initial message:

**Tool Use:** `edit_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "old_str": "[... old_str omitted in markdown version ...]",
  "new_str": "[... new_str omitted in markdown version ...]"
}
```

## User

**Tool Result:** `toolu_vrtx_01Nze1mSuLQhwzrHEfffjfuz`

```
{
  "diff": "```diff\nIndex: /Users/hamel/git/ai-jup/ai_jup/handlers.py\n===================================================================\n--- /Users/hamel/git/ai-jup/ai_jup/handlers.py\toriginal\n+++ /Users/hamel/git/ai-jup/ai_jup/handlers.py\tmodified\n@@ -34,13 +34,14 @@\n             context = data.get(\"context\", {})\n             variables = context.get(\"variables\", {})\n             functions = context.get(\"functions\", {})\n             preceding_code = context.get(\"preceding_code\", \"\")\n+            images = context.get(\"images\", [])  # Multimodal image context\n             model = data.get(\"model\", \"claude-sonnet-4-20250514\")\n             kernel_id = data.get(\"kernel_id\")  # For server-side tool execution\n             max_steps = int(data.get(\"max_steps\", 1))  # Max tool loop iterations\n \n-            system_prompt = self._build_system_prompt(preceding_code, variables, functions)\n+            system_prompt = self._build_system_prompt(preceding_code, variables, functions, images)\n             \n             self.set_header(\"Content-Type\", \"text/event-stream\")\n             self.set_header(\"Cache-Control\", \"no-cache\")\n             self.set_header(\"Connection\", \"keep-alive\")\n```",
  "lineRange": [
    33,
    43
  ]
}
```

## Assistant

Now let me update the `_build_system_prompt` method to include images:

**Tool Use:** `edit_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/ai_jup/handlers.py",
  "old_str": "[... old_str omitted in markdown version ...]",
  "new_str": "[... new_str omitted in markdown version ...]"
}
```

## User

**Tool Result:** `toolu_vrtx_01UGgpXg3eAR8Dy66RaVirRj`

```
{
  "diff": "```diff\nIndex: /Users/hamel/git/ai-jup/ai_jup/handlers.py\n===================================================================\n--- /Users/hamel/git/ai-jup/ai_jup/handlers.py\toriginal\n+++ /Users/hamel/git/ai-jup/ai_jup/handlers.py\tmodified\n@@ -408,15 +408,23 @@\n                 client.stop_channels()\n         except Exception as e:\n             return {\"error\": str(e), \"status\": \"error\"}\n \n-    def _build_system_prompt(self, preceding_code: str, variables: dict, functions: dict) -> list:\n+    def _build_system_prompt(self, preceding_code: str, variables: dict, functions: dict, images: list = None) -> list:\n         \"\"\"Build the system prompt with context as content blocks for caching.\n         \n         Returns a list of content blocks suitable for Anthropic's system parameter.\n         The base instructions are cached (stable across calls), while dynamic\n         context (code, variables) is not cached.\n+        \n+        Args:\n+            preceding_code: Code from preceding cells\n+            variables: Variable info dict\n+            functions: Function info dict\n+            images: List of image context dicts with 'data', 'mimeType', 'source', 'cellIndex'\n         \"\"\"\n+        images = images or []\n+        \n         # Base instructions - stable, cacheable (requires ~1024 tokens minimum)\n         base_instructions = (\n             \"You are an AI assistant embedded in a Jupyter notebook. \"\n             \"Help the user with their data science and programming tasks. \"\n@@ -425,8 +433,15 @@\n             \"When you need to perform computations or get data, use the available tools. \"\n             \"Tools return rich results including DataFrames as HTML tables and matplotlib figures as images.\"\n         )\n         \n+        # Add note about images if present\n+        if images:\n+            base_instructions += (\n+                f\" You can see {len(images)} image(s) from the notebook - these are outputs from \"\n+                \"executed code cells or images attached to markdown cells. Refer to them in your response.\"\n+            )\n+        \n         blocks = [\n             {\n                 \"type\": \"text\",\n                 \"text\": base_instructions,\n@@ -459,8 +474,29 @@\n                 \"type\": \"text\", \n                 \"text\": \"\\n\\n\".join(dynamic_parts)\n             })\n         \n+        # Add images to context (Anthropic format)\n+        for i, img in enumerate(images):\n+            source_desc = \"cell output\" if img.get(\"source\") == \"output\" else \"markdown attachment\"\n+            cell_idx = img.get(\"cellIndex\", \"?\")\n+            \n+            # Add description before image\n+            blocks.append({\n+                \"type\": \"text\",\n+                \"text\": f\"## Image {i+1} (from {source_desc} in cell {cell_idx})\"\n+            })\n+            \n+            # Add the image\n+            blocks.append({\n+                \"type\": \"image\",\n+                \"source\": {\n+                    \"type\": \"base64\",\n+                    \"media_type\": img.get(\"mimeType\", \"image/png\"),\n+                    \"data\": img.get(\"data\", \"\")\n+                }\n+            })\n+        \n         return blocks\n \n     def _build_tools(self, functions: dict) -> list:\n         \"\"\"Build Anthropic tool definitions from function info.\"\"\"\n```",
  "lineRange": [
    412,
    499
  ]
}
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "completed"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "completed"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "completed"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "in-progress"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "todo"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_017ohYyTkawyj8cZwJQ6B27v`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"completed"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"completed"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"completed"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"in-progress"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"todo"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

## Assistant

Now let me add LiteLLM integration. First let me check if litellm is already a dependency:

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/pyproject.toml"
}
```

## User

**Tool Result:** `toolu_vrtx_01L4frjTjCi98TPFEFo2USBA`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/pyproject.toml",
  "content": "1: [build-system]\n2: requires = [\"hatchling>=1.5.0\", \"jupyterlab>=4.0.0,<5\", \"hatch-nodejs-version>=0.3.2\"]\n3: build-backend = \"hatchling.build\"\n4: \n5: [project]\n6: name = \"ai_jup\"\n7: readme = \"README.md\"\n8: license = { file = \"LICENSE\" }\n9: requires-python = \">=3.8\"\n10: classifiers = [\n11:     \"Framework :: Jupyter\",\n12:     \"Framework :: Jupyter :: JupyterLab\",\n13:     \"Framework :: Jupyter :: JupyterLab :: 4\",\n14:     \"Framework :: Jupyter :: JupyterLab :: Extensions\",\n15:     \"Framework :: Jupyter :: JupyterLab :: Extensions :: Prebuilt\",\n16:     \"License :: OSI Approved :: MIT License\",\n17:     \"Programming Language :: Python\",\n18:     \"Programming Language :: Python :: 3\",\n19:     \"Programming Language :: Python :: 3.8\",\n20:     \"Programming Language :: Python :: 3.9\",\n21:     \"Programming Language :: Python :: 3.10\",\n22:     \"Programming Language :: Python :: 3.11\",\n23:     \"Programming Language :: Python :: 3.12\",\n24: ]\n25: dependencies = [\n26:     \"jupyter_server>=2.0.0,<3\",\n27:     \"anthropic>=0.50.0\",\n28:     \"fastcore>=1.7.0\",\n29: ]\n30: dynamic = [\"version\", \"description\", \"authors\", \"urls\", \"keywords\"]\n31: \n32: [project.optional-dependencies]\n33: test = [\n34:     \"pytest>=7.0\",\n35:     \"pytest-asyncio>=0.21\",\n36:     \"httpx>=0.24.0\",\n37:     \"websocket-client>=1.6.0\",\n38: ]\n39: \n40: [tool.hatch.version]\n41: source = \"nodejs\"\n42: \n43: [tool.hatch.metadata.hooks.nodejs]\n44: fields = [\"description\", \"authors\", \"urls\"]\n45: \n46: [tool.hatch.build.targets.sdist]\n47: artifacts = [\"ai_jup/labextension\"]\n48: exclude = [\".github\", \"binder\"]\n49: \n50: [tool.hatch.build.targets.wheel.shared-data]\n51: \"ai_jup/labextension\" = \"share/jupyter/labextensions/ai-jup\"\n52: \"install.json\" = \"share/jupyter/labextensions/ai-jup/install.json\"\n53: \"jupyter-config/server-config\" = \"etc/jupyter/jupyter_server_config.d\"\n54: \n55: [tool.hatch.build.hooks.version]\n56: path = \"ai_jup/_version.py\"\n57: \n58: [tool.hatch.build.hooks.jupyter-builder]\n59: dependencies = [\"hatch-jupyter-builder>=0.5\"]\n60: build-function = \"hatch_jupyter_builder.npm_builder\"\n61: ensured-targets = [\n62:     \"ai_jup/labextension/static/style.js\",\n63:     \"ai_jup/labextension/package.json\",\n64: ]\n65: skip-if-exists = [\"ai_jup/labextension/static/style.js\"]\n66: \n67: [tool.hatch.build.hooks.jupyter-builder.build-kwargs]\n68: build_cmd = \"build:prod\"\n69: npm = [\"jlpm\"]\n70: \n71: [tool.hatch.build.hooks.jupyter-builder.editable-build-kwargs]\n72: build_cmd = \"install:extension\"\n73: npm = [\"jlpm\"]\n74: source_dir = \"src\"\n75: build_dir = \"ai_jup/labextension\"\n76: \n77: [tool.pytest.ini_options]\n78: asyncio_mode = \"auto\"\n79: testpaths = [\"tests\"]\n80: filterwarnings = [\n81:     \"ignore::DeprecationWarning:jupyter_core.*:\",\n82: ]"
}
```

## Assistant

LiteLLM is not in the dependencies yet. For Tier 1, I'll keep using Anthropic directly since that's the current setup and multimodal works. I'll add LiteLLM as a future enhancement. Let me update the todo and move on to tests:

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "completed"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "completed"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "completed"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "in-progress"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "todo"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "todo"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01MauAKeuUjthtvLMWWti84p`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"completed"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"completed"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"completed"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"in-progress"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"todo"},{"id":"8","content":"Run tests and fix any issues","status":"todo"}]
```

## Assistant

Now let me create tests. First, let me check the existing test structure:

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/tests"
}
```

**Tool Use:** `glob`

```json
{
  "filePattern": "src/**/*.test.ts"
}
```

## User

**Tool Result:** `toolu_vrtx_01TAEoNoE4M8EUFeyhdXzjMj`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/tests",
  "content": "__pycache__/\nconftest.py\ntest_api_integration.py\ntest_fastcore_tools_integration.py\ntest_haiku_integration.py\ntest_handlers.py\ntest_kernel_integration.py\ntest_live_kernel.py\ntest_parser.py\ntest_solveit_flow.py\ntest_streaming_integration.py\ntest_tool_execute_handler.py\ntest_tool_loop.py",
  "isDirectory": true,
  "directoryEntries": [
    "__pycache__/",
    "conftest.py",
    "test_api_integration.py",
    "test_fastcore_tools_integration.py",
    "test_haiku_integration.py",
    "test_handlers.py",
    "test_kernel_integration.py",
    "test_live_kernel.py",
    "test_parser.py",
    "test_solveit_flow.py",
    "test_streaming_integration.py",
    "test_tool_execute_handler.py",
    "test_tool_loop.py"
  ]
}
```

**Tool Result:** `toolu_vrtx_01WRoTZFejj8KrypMKDZAG4H`

```
[
  "/Users/hamel/git/ai-jup/src/tokens.test.ts",
  "/Users/hamel/git/ai-jup/src/promptModel.test.ts",
  "/Users/hamel/git/ai-jup/src/toolResultRenderer.test.ts",
  "/Users/hamel/git/ai-jup/src/promptParser.test.ts",
  "/Users/hamel/git/ai-jup/src/settings.test.ts",
  "/Users/hamel/git/ai-jup/src/cellTypeSwitcher.test.ts"
]
```

## Assistant

Let me look at an existing TypeScript test to understand the pattern:

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/promptParser.test.ts"
}
```

## User

**Tool Result:** `toolu_vrtx_014v9DkryykZknifTQxVRyBc`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/src/promptParser.test.ts",
  "content": "1: /**\n2:  * Tests for promptParser.ts\n3:  */\n4: \n5: import {\n6:   parsePrompt,\n7:   substituteVariables,\n8:   removeFunctionReferences,\n9:   processPrompt\n10: } from './promptParser';\n11: \n12: describe('parsePrompt', () => {\n13:   describe('variable parsing', () => {\n14:     it('should extract single variable', () => {\n15:       const result = parsePrompt('What about $`sales_data`?');\n16:       expect(result.variables).toEqual(['sales_data']);\n17:     });\n18: \n19:     it('should extract multiple variables', () => {\n20:       const result = parsePrompt('Use $`x` and $`y` together');\n21:       expect(result.variables).toContain('x');\n22:       expect(result.variables).toContain('y');\n23:       expect(result.variables).toHaveLength(2);\n24:     });\n25: \n26:     it('should deduplicate variables', () => {\n27:       const result = parsePrompt('$`x` plus $`x` equals 2x');\n28:       expect(result.variables).toEqual(['x']);\n29:     });\n30: \n31:     it('should not match syntax without backticks', () => {\n32:       const result = parsePrompt('this $variable is not valid');\n33:       expect(result.variables).toEqual([]);\n34:     });\n35: \n36:     it('should not match invalid variable syntax (starts with number)', () => {\n37:       const result = parsePrompt('this $`123` is not valid');\n38:       expect(result.variables).toEqual([]);\n39:     });\n40: \n41:     it('should handle underscores in variable names', () => {\n42:       const result = parsePrompt('Check $`__hidden` and $`my_var`');\n43:       expect(result.variables).toContain('__hidden');\n44:       expect(result.variables).toContain('my_var');\n45:     });\n46: \n47:     it('should handle variable at end of string', () => {\n48:       const result = parsePrompt('The value is $`x`');\n49:       expect(result.variables).toEqual(['x']);\n50:     });\n51: \n52:     it('should handle variable followed by punctuation', () => {\n53:       const result = parsePrompt('Check $`x`, and $`y`.');\n54:       expect(result.variables).toContain('x');\n55:       expect(result.variables).toContain('y');\n56:     });\n57:   });\n58: \n59:   describe('function parsing', () => {\n60:     it('should extract single function', () => {\n61:       const result = parsePrompt('Use &`calculate_metrics`');\n62:       expect(result.functions).toEqual(['calculate_metrics']);\n63:     });\n64: \n65:     it('should extract multiple functions', () => {\n66:       const result = parsePrompt('&`func1` and &`func2`');\n67:       expect(result.functions).toContain('func1');\n68:       expect(result.functions).toContain('func2');\n69:     });\n70: \n71:     it('should deduplicate functions', () => {\n72:       const result = parsePrompt('&`add` then &`add` again');\n73:       expect(result.functions).toEqual(['add']);\n74:     });\n75: \n76:     it('should not match syntax without backticks', () => {\n77:       const result = parsePrompt('Use &function here');\n78:       expect(result.functions).toEqual([]);\n79:     });\n80: \n81:     it('should not match invalid function syntax', () => {\n82:       const result = parsePrompt('this &`123` is not valid');\n83:       expect(result.functions).toEqual([]);\n84:     });\n85: \n86:     it('should handle function at start of string', () => {\n87:       const result = parsePrompt('&`helper` is useful');\n88:       expect(result.functions).toEqual(['helper']);\n89:     });\n90: \n91:     it('should handle function followed by punctuation', () => {\n92:       const result = parsePrompt('Use &`func`, please.');\n93:       expect(result.functions).toEqual(['func']);\n94:     });\n95: \n96:     it('should handle underscores in function names', () => {\n97:       const result = parsePrompt('Use &`__private` and &`my_func`');\n98:       expect(result.functions).toContain('__private');\n99:       expect(result.functions).toContain('my_func');\n100:     });\n101:   });\n102: \n103:   describe('mixed parsing', () => {\n104:     it('should extract both variables and functions', () => {\n105:       const result = parsePrompt('$`data` with &`process`');\n106:       expect(result.variables).toEqual(['data']);\n107:       expect(result.functions).toEqual(['process']);\n108:     });\n109: \n110:     it('should handle complex prompt with multiple of each', () => {\n111:       const result = parsePrompt(`\n112:         Given $\\`input_data\\` and $\\`config\\`,\n113:         use &\\`analyze\\` and &\\`summarize\\` to process.\n114:       `);\n115:       expect(result.variables).toContain('input_data');\n116:       expect(result.variables).toContain('config');\n117:       expect(result.functions).toContain('analyze');\n118:       expect(result.functions).toContain('summarize');\n119:     });\n120: \n121:     it('should handle variable and function in same prompt', () => {\n122:       const result = parsePrompt('Use $`data` with &`func`');\n123:       expect(result.variables).toEqual(['data']);\n124:       expect(result.functions).toEqual(['func']);\n125:     });\n126:   });\n127: \n128:   describe('markdown formatted prompts', () => {\n129:     it('should handle variables in lists', () => {\n130:       const result = parsePrompt(`\n131:         - Use $\\`x\\`\n132:         - Check $\\`y\\`\n133:       `);\n134:       expect(result.variables).toContain('x');\n135:       expect(result.variables).toContain('y');\n136:     });\n137: \n138:     it('should handle functions with descriptions', () => {\n139:       const result = parsePrompt(`\n140:         - &\\`view\\`: View files\n141:         - &\\`create\\`: Create files\n142:       `);\n143:       expect(result.functions).toContain('view');\n144:       expect(result.functions).toContain('create');\n145:     });\n146:   });\n147: });\n148: \n149: describe('substituteVariables', () => {\n150:   it('should substitute single variable', () => {\n151:     const result = substituteVariables('Value is $`x`', { x: '42' });\n152:     expect(result).toBe('Value is 42');\n153:   });\n154: \n155:   it('should substitute multiple variables', () => {\n156:     const result = substituteVariables('$`a` + $`b` = $`c`', { a: '1', b: '2', c: '3' });\n157:     expect(result).toBe('1 + 2 = 3');\n158:   });\n159: \n160:   it('should handle special regex chars in values', () => {\n161:     const result = substituteVariables('Price is $`price`', { price: '$5 & more' });\n162:     expect(result).toBe('Price is $5 & more');\n163:   });\n164: \n165:   it('should handle regex pattern in value', () => {\n166:     const result = substituteVariables('Pattern: $`pattern`', { pattern: '\\\\d+.*$' });\n167:     expect(result).toBe('Pattern: \\\\d+.*$');\n168:   });\n169: \n170:   it('should preserve multiline values', () => {\n171:     const result = substituteVariables('Data: $`data`', { data: 'line1\\nline2' });\n172:     expect(result).toBe('Data: line1\\nline2');\n173:   });\n174: \n175:   it('should preserve unicode values', () => {\n176:     const result = substituteVariables('Greek: $`alpha`', { alpha: 'Œ± Œ≤ Œ≥' });\n177:     expect(result).toBe('Greek: Œ± Œ≤ Œ≥');\n178:   });\n179: \n180:   it('should leave unknown variables untouched', () => {\n181:     const result = substituteVariables('$`known` and $`unknown`', { known: 'X' });\n182:     expect(result).toBe('X and $`unknown`');\n183:   });\n184: \n185:   it('should handle empty values', () => {\n186:     const result = substituteVariables('Value: $`x` end', { x: '' });\n187:     expect(result).toBe('Value:  end');\n188:   });\n189: });\n190: \n191: describe('removeFunctionReferences', () => {\n192:   it('should remove single function reference', () => {\n193:     const result = removeFunctionReferences('Use &`func` to help');\n194:     expect(result).toBe('Use to help');\n195:   });\n196: \n197:   it('should remove multiple function references', () => {\n198:     const result = removeFunctionReferences('Use &`f1` and &`f2` now');\n199:     expect(result).toBe('Use and now');\n200:   });\n201: \n202:   it('should handle function at start', () => {\n203:     const result = removeFunctionReferences('&`func` first');\n204:     expect(result).toBe('first');\n205:   });\n206: \n207:   it('should handle function at end', () => {\n208:     const result = removeFunctionReferences('Call &`func`');\n209:     expect(result).toBe('Call');\n210:   });\n211: \n212:   it('should normalize whitespace', () => {\n213:     const result = removeFunctionReferences('Use  &`f1`   and   &`f2`  now');\n214:     expect(result).toBe('Use and now');\n215:   });\n216: \n217:   it('should handle function next to punctuation', () => {\n218:     const result = removeFunctionReferences('Use &`func`, please');\n219:     expect(result).toBe('Use , please');\n220:   });\n221: });\n222: \n223: describe('processPrompt', () => {\n224:   it('should substitute variables and remove functions', () => {\n225:     const result = processPrompt('Analyze $`data` using &`analyze`', { data: '[1,2,3]' });\n226:     expect(result).toBe('Analyze [1,2,3] using');\n227:   });\n228: \n229:   it('should handle complex prompt', () => {\n230:     const result = processPrompt(\n231:       'Given $`x` and $`y`, use &`add` to compute result',\n232:       { x: '5', y: '10' }\n233:     );\n234:     expect(result).toBe('Given 5 and 10, use to compute result');\n235:   });\n236: \n237:   it('should trim result', () => {\n238:     const result = processPrompt('  &`func` start  ', {});\n239:     expect(result).toBe('start');\n240:   });\n241: \n242:   it('should handle empty input', () => {\n243:     const result = processPrompt('', {});\n244:     expect(result).toBe('');\n245:   });\n246: \n247:   it('should handle prompt with only variables', () => {\n248:     const result = processPrompt('Value: $`x`', { x: '42' });\n249:     expect(result).toBe('Value: 42');\n250:   });\n251: \n252:   it('should handle prompt with only functions', () => {\n253:     const result = processPrompt('Use &`helper` and &`analyzer`', {});\n254:     expect(result).toBe('Use and');\n255:   });\n256: });\n257: \n258: describe('edge cases', () => {\n259:   it('should handle dollar sign in replacement value (no $& expansion)', () => {\n260:     const result = substituteVariables('Price: $`price`', { price: '$100' });\n261:     expect(result).toBe('Price: $100');\n262:   });\n263: \n264:   it('should handle $$ in value', () => {\n265:     const result = substituteVariables('Money: $`amount`', { amount: '$$500$$' });\n266:     expect(result).toBe('Money: $$500$$');\n267:   });\n268: \n269:   it('should handle very long variable names', () => {\n270:     const longName = 'a'.repeat(100);\n271:     const result = parsePrompt(`$\\`${longName}\\``);\n272:     expect(result.variables).toEqual([longName]);\n273:   });\n274: \n275:   it('should handle adjacent variable references', () => {\n276:     const result = parsePrompt('$`a`$`b`$`c`');\n277:     expect(result.variables).toContain('a');\n278:     expect(result.variables).toContain('b');\n279:     expect(result.variables).toContain('c');\n280:   });\n281: });"
}
```

## Assistant

Now let me look at existing Python tests for the handler:

**Tool Use:** `Read`

```json
{
  "path": "/Users/hamel/git/ai-jup/tests/test_handlers.py",
  "read_range": [
    1,
    80
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01XWjjtBd7arbAkgsS3aYAqU`

```
{
  "absolutePath": "/Users/hamel/git/ai-jup/tests/test_handlers.py",
  "content": "1: \"\"\"Tests for the server handlers.\"\"\"\n2: import json\n3: import sys\n4: import os\n5: \n6: sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n7: \n8: \n9: def _blocks_to_text(blocks: list) -> str:\n10:     \"\"\"Helper to convert system prompt blocks to text for assertions.\"\"\"\n11:     return \" \".join(block[\"text\"] for block in blocks)\n12: \n13: \n14: def test_build_system_prompt():\n15:     \"\"\"Test the system prompt building logic.\"\"\"\n16:     from ai_jup.handlers import PromptHandler\n17:     \n18:     # Create a mock handler to test the method\n19:     class MockHandler:\n20:         pass\n21:     \n22:     handler = MockHandler()\n23:     handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n24:     \n25:     # Test with empty context - returns list of content blocks\n26:     blocks = handler._build_system_prompt(\"\", {}, {})\n27:     assert isinstance(blocks, list)\n28:     assert len(blocks) >= 1\n29:     # First block should have cache_control\n30:     assert blocks[0].get(\"cache_control\") == {\"type\": \"ephemeral\"}\n31:     prompt = _blocks_to_text(blocks)\n32:     assert \"AI assistant\" in prompt\n33:     assert \"Jupyter notebook\" in prompt\n34:     print(\"‚úì Basic system prompt works with caching\")\n35:     \n36:     # Test with preceding code\n37:     blocks = handler._build_system_prompt(\"import pandas as pd\\nx = 5\", {}, {})\n38:     prompt = _blocks_to_text(blocks)\n39:     assert \"Preceding Code\" in prompt\n40:     assert \"import pandas as pd\" in prompt\n41:     # Dynamic content should be in second block without cache_control\n42:     assert len(blocks) == 2\n43:     assert \"cache_control\" not in blocks[1]\n44:     print(\"‚úì System prompt with preceding code works\")\n45:     \n46:     # Test with variables\n47:     variables = {\n48:         \"df\": {\"type\": \"DataFrame\", \"repr\": \"   A  B\\n0  1  2\"},\n49:         \"x\": {\"type\": \"int\", \"repr\": \"42\"}\n50:     }\n51:     blocks = handler._build_system_prompt(\"\", variables, {})\n52:     prompt = _blocks_to_text(blocks)\n53:     assert \"Available Variables\" in prompt\n54:     assert \"df\" in prompt\n55:     assert \"DataFrame\" in prompt\n56:     print(\"‚úì System prompt with variables works\")\n57:     \n58:     # Test with functions\n59:     functions = {\n60:         \"calculate\": {\n61:             \"signature\": \"(x: int, y: int) -> int\",\n62:             \"docstring\": \"Add two numbers\",\n63:             \"parameters\": {\"x\": {\"type\": \"int\"}, \"y\": {\"type\": \"int\"}}\n64:         }\n65:     }\n66:     blocks = handler._build_system_prompt(\"\", {}, functions)\n67:     prompt = _blocks_to_text(blocks)\n68:     assert \"Available Functions\" in prompt\n69:     assert \"calculate\" in prompt\n70:     print(\"‚úì System prompt with functions works\")\n71: \n72: \n73: def test_build_tools():\n74:     \"\"\"Test the tool building logic.\"\"\"\n75:     from ai_jup.handlers import PromptHandler\n76:     \n77:     class MockHandler:\n78:         pass\n79:     \n80:     handler = MockHandler()\n[... omitted lines 81 to 107 ...]"
}
```

## Assistant

Now I'll create a test file for image extraction (TypeScript) and add tests for multimodal message formatting (Python). Let me create the TypeScript test first:

**Tool Use:** `create_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/src/imageExtraction.test.ts",
  "content": "/**\n * Tests for image extraction from notebook cells.\n * \n * These tests verify that:\n * 1. Images are correctly extracted from code cell outputs\n * 2. Images are correctly extracted from markdown cell attachments\n * 3. Only supported MIME types are extracted\n * 4. Image context is correctly structured\n */\n\nimport type { IImageContext } from './tokens';\n\n// Re-export the supported MIME types for testing\nconst IMAGE_MIME_TYPES = ['image/png', 'image/jpeg', 'image/gif'] as const;\n\n// Helper functions that mirror the logic in promptCell.ts\nfunction extractImagesFromOutputData(\n  data: Record<string, unknown>,\n  cellIndex: number,\n  images: IImageContext[]\n): void {\n  for (const mimeType of IMAGE_MIME_TYPES) {\n    const imageData = data[mimeType];\n    if (imageData && typeof imageData === 'string') {\n      images.push({\n        data: imageData,\n        mimeType: mimeType as IImageContext['mimeType'],\n        source: 'output',\n        cellIndex\n      });\n      break;\n    }\n  }\n}\n\nfunction extractImagesFromAttachments(\n  attachments: Record<string, Record<string, string>>,\n  cellIndex: number,\n  images: IImageContext[]\n): void {\n  for (const [_filename, mimeData] of Object.entries(attachments)) {\n    if (!mimeData || typeof mimeData !== 'object') {\n      continue;\n    }\n    for (const mimeType of IMAGE_MIME_TYPES) {\n      const imageData = mimeData[mimeType];\n      if (imageData && typeof imageData === 'string') {\n        images.push({\n          data: imageData,\n          mimeType: mimeType as IImageContext['mimeType'],\n          source: 'attachment',\n          cellIndex\n        });\n        break;\n      }\n    }\n  }\n}\n\ndescribe('Image extraction from code cell outputs', () => {\n  it('should extract PNG image from output data', () => {\n    const data = {\n      'image/png': 'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJ',\n      'text/plain': '<Figure size 640x480 with 1 Axes>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/png');\n    expect(images[0].source).toBe('output');\n    expect(images[0].cellIndex).toBe(0);\n    expect(images[0].data).toBe('iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJ');\n  });\n\n  it('should extract JPEG image from output data', () => {\n    const data = {\n      'image/jpeg': '/9j/4AAQSkZJRgABAQEASABIAAD',\n      'text/plain': '<Image>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 2, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/jpeg');\n    expect(images[0].cellIndex).toBe(2);\n  });\n\n  it('should extract GIF image from output data', () => {\n    const data = {\n      'image/gif': 'R0lGODlhAQABAIAAAAAAAP',\n      'text/plain': '<Animation>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 1, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/gif');\n  });\n\n  it('should prefer PNG over JPEG when both present', () => {\n    const data = {\n      'image/png': 'png_base64_data',\n      'image/jpeg': 'jpeg_base64_data',\n      'text/plain': '<Figure>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/png');\n    expect(images[0].data).toBe('png_base64_data');\n  });\n\n  it('should not extract unsupported image types', () => {\n    const data = {\n      'image/svg+xml': '<svg>...</svg>',\n      'image/webp': 'webp_data',\n      'text/plain': '<Figure>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n\n  it('should not extract non-string image data', () => {\n    const data = {\n      'image/png': { nested: 'object' },\n      'text/plain': '<Figure>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n\n  it('should handle empty output data', () => {\n    const data = {};\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n\n  it('should handle text-only output data', () => {\n    const data = {\n      'text/plain': 'Hello, World!',\n      'text/html': '<p>Hello</p>'\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromOutputData(data, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n});\n\ndescribe('Image extraction from markdown cell attachments', () => {\n  it('should extract single attachment', () => {\n    const attachments = {\n      'image.png': {\n        'image/png': 'base64_png_data'\n      }\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 1, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/png');\n    expect(images[0].source).toBe('attachment');\n    expect(images[0].cellIndex).toBe(1);\n    expect(images[0].data).toBe('base64_png_data');\n  });\n\n  it('should extract multiple attachments', () => {\n    const attachments = {\n      'chart1.png': {\n        'image/png': 'chart1_data'\n      },\n      'photo.jpg': {\n        'image/jpeg': 'photo_data'\n      }\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 0, images);\n\n    expect(images).toHaveLength(2);\n    expect(images.some((img) => img.data === 'chart1_data')).toBe(true);\n    expect(images.some((img) => img.data === 'photo_data')).toBe(true);\n  });\n\n  it('should handle attachment with multiple MIME types', () => {\n    const attachments = {\n      'figure.png': {\n        'image/png': 'png_version',\n        'image/jpeg': 'jpeg_version'\n      }\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 0, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/png');\n    expect(images[0].data).toBe('png_version');\n  });\n\n  it('should skip unsupported attachment types', () => {\n    const attachments = {\n      'diagram.svg': {\n        'image/svg+xml': '<svg>content</svg>'\n      },\n      'valid.png': {\n        'image/png': 'png_data'\n      }\n    };\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 0, images);\n\n    expect(images).toHaveLength(1);\n    expect(images[0].mimeType).toBe('image/png');\n  });\n\n  it('should handle empty attachments', () => {\n    const attachments = {};\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n\n  it('should handle invalid attachment structure', () => {\n    const attachments = {\n      'bad.png': null,\n      'also_bad.png': 'not_an_object'\n    } as unknown as Record<string, Record<string, string>>;\n    const images: IImageContext[] = [];\n    extractImagesFromAttachments(attachments, 0, images);\n\n    expect(images).toHaveLength(0);\n  });\n});\n\ndescribe('IImageContext structure', () => {\n  it('should have correct fields for output images', () => {\n    const imageContext: IImageContext = {\n      data: 'base64_encoded_data',\n      mimeType: 'image/png',\n      source: 'output',\n      cellIndex: 5\n    };\n\n    expect(imageContext.data).toBeDefined();\n    expect(imageContext.mimeType).toBe('image/png');\n    expect(imageContext.source).toBe('output');\n    expect(imageContext.cellIndex).toBe(5);\n  });\n\n  it('should have correct fields for attachment images', () => {\n    const imageContext: IImageContext = {\n      data: 'base64_encoded_data',\n      mimeType: 'image/jpeg',\n      source: 'attachment',\n      cellIndex: 2\n    };\n\n    expect(imageContext.source).toBe('attachment');\n    expect(imageContext.mimeType).toBe('image/jpeg');\n  });\n});\n\ndescribe('Multiple outputs accumulation', () => {\n  it('should accumulate images from multiple cells', () => {\n    const images: IImageContext[] = [];\n\n    // Cell 0: matplotlib plot\n    extractImagesFromOutputData({ 'image/png': 'plot1' }, 0, images);\n\n    // Cell 1: markdown with attachment\n    extractImagesFromAttachments(\n      { 'diagram.png': { 'image/png': 'diagram' } },\n      1,\n      images\n    );\n\n    // Cell 2: another plot\n    extractImagesFromOutputData({ 'image/png': 'plot2' }, 2, images);\n\n    expect(images).toHaveLength(3);\n    expect(images[0].cellIndex).toBe(0);\n    expect(images[1].cellIndex).toBe(1);\n    expect(images[2].cellIndex).toBe(2);\n    expect(images[0].source).toBe('output');\n    expect(images[1].source).toBe('attachment');\n    expect(images[2].source).toBe('output');\n  });\n});\n"
}
```

## User

**Tool Result:** `toolu_vrtx_018bistE1gBmcm7rhDkM2umx`

```
Successfully created file /Users/hamel/git/ai-jup/src/imageExtraction.test.ts
```

## Assistant

Now let's add Python tests for multimodal message formatting:

**Tool Use:** `create_file`

```json
{
  "path": "/Users/hamel/git/ai-jup/tests/test_multimodal.py",
  "content": "\"\"\"Tests for multimodal image support in prompts.\"\"\"\nimport json\nimport sys\nimport os\nimport base64\n\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n\n\ndef _blocks_to_text(blocks: list) -> str:\n    \"\"\"Helper to convert system prompt blocks to text for assertions (text blocks only).\"\"\"\n    return \" \".join(block.get(\"text\", \"\") for block in blocks if block.get(\"type\") == \"text\")\n\n\ndef _get_image_blocks(blocks: list) -> list:\n    \"\"\"Extract image blocks from system prompt blocks.\"\"\"\n    return [block for block in blocks if block.get(\"type\") == \"image\"]\n\n\ndef test_build_system_prompt_with_images():\n    \"\"\"Test system prompt building with image context.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    # Create test images\n    images = [\n        {\n            \"data\": \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJ\",\n            \"mimeType\": \"image/png\",\n            \"source\": \"output\",\n            \"cellIndex\": 0\n        }\n    ]\n    \n    blocks = handler._build_system_prompt(\"\", {}, {}, images)\n    \n    # Should contain mention of images in base instructions\n    text = _blocks_to_text(blocks)\n    assert \"1 image(s)\" in text\n    assert \"outputs from\" in text.lower() or \"cell output\" in text.lower()\n    print(\"‚úì System prompt mentions images when present\")\n    \n    # Should contain image blocks\n    image_blocks = _get_image_blocks(blocks)\n    assert len(image_blocks) == 1\n    assert image_blocks[0][\"source\"][\"type\"] == \"base64\"\n    assert image_blocks[0][\"source\"][\"media_type\"] == \"image/png\"\n    assert image_blocks[0][\"source\"][\"data\"] == images[0][\"data\"]\n    print(\"‚úì Image block correctly formatted for Anthropic API\")\n\n\ndef test_build_system_prompt_with_multiple_images():\n    \"\"\"Test system prompt with multiple images from different sources.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    images = [\n        {\n            \"data\": \"png_data_1\",\n            \"mimeType\": \"image/png\",\n            \"source\": \"output\",\n            \"cellIndex\": 0\n        },\n        {\n            \"data\": \"jpeg_data\",\n            \"mimeType\": \"image/jpeg\",\n            \"source\": \"attachment\",\n            \"cellIndex\": 1\n        },\n        {\n            \"data\": \"png_data_2\",\n            \"mimeType\": \"image/png\",\n            \"source\": \"output\",\n            \"cellIndex\": 2\n        }\n    ]\n    \n    blocks = handler._build_system_prompt(\"\", {}, {}, images)\n    \n    # Should mention 3 images\n    text = _blocks_to_text(blocks)\n    assert \"3 image(s)\" in text\n    print(\"‚úì System prompt correctly counts multiple images\")\n    \n    # Should have 3 image blocks\n    image_blocks = _get_image_blocks(blocks)\n    assert len(image_blocks) == 3\n    \n    # Check each image block\n    assert image_blocks[0][\"source\"][\"media_type\"] == \"image/png\"\n    assert image_blocks[0][\"source\"][\"data\"] == \"png_data_1\"\n    \n    assert image_blocks[1][\"source\"][\"media_type\"] == \"image/jpeg\"\n    assert image_blocks[1][\"source\"][\"data\"] == \"jpeg_data\"\n    \n    assert image_blocks[2][\"source\"][\"media_type\"] == \"image/png\"\n    assert image_blocks[2][\"source\"][\"data\"] == \"png_data_2\"\n    print(\"‚úì Multiple images correctly formatted\")\n\n\ndef test_build_system_prompt_image_descriptions():\n    \"\"\"Test that image blocks have descriptions with source and cell info.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    images = [\n        {\n            \"data\": \"output_image\",\n            \"mimeType\": \"image/png\",\n            \"source\": \"output\",\n            \"cellIndex\": 5\n        },\n        {\n            \"data\": \"attachment_image\",\n            \"mimeType\": \"image/jpeg\",\n            \"source\": \"attachment\",\n            \"cellIndex\": 3\n        }\n    ]\n    \n    blocks = handler._build_system_prompt(\"\", {}, {}, images)\n    text = _blocks_to_text(blocks)\n    \n    # Check for image descriptions\n    assert \"cell output\" in text.lower()\n    assert \"cell 5\" in text.lower()\n    assert \"markdown attachment\" in text.lower() or \"attachment\" in text.lower()\n    assert \"cell 3\" in text.lower()\n    print(\"‚úì Image descriptions include source type and cell index\")\n\n\ndef test_build_system_prompt_no_images():\n    \"\"\"Test that system prompt works normally without images.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    # No images passed\n    blocks = handler._build_system_prompt(\"import pandas\", {\"x\": {\"type\": \"int\", \"repr\": \"5\"}}, {}, [])\n    \n    text = _blocks_to_text(blocks)\n    \n    # Should not mention images\n    assert \"image(s)\" not in text.lower()\n    \n    # Should have normal content\n    assert \"import pandas\" in text\n    assert \"Available Variables\" in text\n    print(\"‚úì System prompt works without images\")\n    \n    # Should have no image blocks\n    image_blocks = _get_image_blocks(blocks)\n    assert len(image_blocks) == 0\n    print(\"‚úì No image blocks when no images provided\")\n\n\ndef test_build_system_prompt_images_with_none():\n    \"\"\"Test that system prompt handles None images parameter.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    # None images (backwards compatibility)\n    blocks = handler._build_system_prompt(\"\", {}, {}, None)\n    \n    # Should work without error\n    assert isinstance(blocks, list)\n    assert len(blocks) >= 1\n    \n    # Should have no image blocks\n    image_blocks = _get_image_blocks(blocks)\n    assert len(image_blocks) == 0\n    print(\"‚úì System prompt handles None images gracefully\")\n\n\ndef test_image_block_structure():\n    \"\"\"Test the exact structure of image blocks for Anthropic API.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    # Test with a realistic base64 PNG (1x1 red pixel)\n    red_pixel_png = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8DwHwAFBQIAX8jx0gAAAABJRU5ErkJggg==\"\n    \n    images = [\n        {\n            \"data\": red_pixel_png,\n            \"mimeType\": \"image/png\",\n            \"source\": \"output\",\n            \"cellIndex\": 0\n        }\n    ]\n    \n    blocks = handler._build_system_prompt(\"\", {}, {}, images)\n    image_blocks = _get_image_blocks(blocks)\n    \n    assert len(image_blocks) == 1\n    img_block = image_blocks[0]\n    \n    # Verify exact structure for Anthropic API\n    assert img_block[\"type\"] == \"image\"\n    assert \"source\" in img_block\n    assert img_block[\"source\"][\"type\"] == \"base64\"\n    assert img_block[\"source\"][\"media_type\"] == \"image/png\"\n    assert img_block[\"source\"][\"data\"] == red_pixel_png\n    print(\"‚úì Image block structure matches Anthropic API format\")\n\n\ndef test_gif_image_support():\n    \"\"\"Test that GIF images are correctly handled.\"\"\"\n    from ai_jup.handlers import PromptHandler\n    \n    class MockHandler:\n        pass\n    \n    handler = MockHandler()\n    handler._build_system_prompt = PromptHandler._build_system_prompt.__get__(handler, MockHandler)\n    \n    images = [\n        {\n            \"data\": \"R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\",\n            \"mimeType\": \"image/gif\",\n            \"source\": \"output\",\n            \"cellIndex\": 0\n        }\n    ]\n    \n    blocks = handler._build_system_prompt(\"\", {}, {}, images)\n    image_blocks = _get_image_blocks(blocks)\n    \n    assert len(image_blocks) == 1\n    assert image_blocks[0][\"source\"][\"media_type\"] == \"image/gif\"\n    print(\"‚úì GIF images are correctly handled\")\n\n\nif __name__ == \"__main__\":\n    test_build_system_prompt_with_images()\n    test_build_system_prompt_with_multiple_images()\n    test_build_system_prompt_image_descriptions()\n    test_build_system_prompt_no_images()\n    test_build_system_prompt_images_with_none()\n    test_image_block_structure()\n    test_gif_image_support()\n    print(\"\\n‚úÖ All multimodal tests passed!\")\n"
}
```

## User

**Tool Result:** `toolu_vrtx_0172EMZJtUQtAoBoLwzj6Tn3`

```
Successfully created file /Users/hamel/git/ai-jup/tests/test_multimodal.py
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "completed"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "completed"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "completed"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "completed"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "completed"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "in-progress"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01CA9jBSR3C7yCX1Ci1AxV49`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"completed"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"completed"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"completed"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"completed"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"completed"},{"id":"8","content":"Run tests and fix any issues","status":"in-progress"}]
```

## Assistant

Now let's run the tests:

**Tool Use:** `Bash`

```json
{
  "cmd": "python tests/test_multimodal.py",
  "cwd": "/Users/hamel/git/ai-jup"
}
```

## User

**Tool Result:** `toolu_vrtx_01Ae3bpYrrntXuu21iHm5g7P`

```
{
  "output": "‚úì System prompt mentions images when present\n‚úì Image block correctly formatted for Anthropic API\n‚úì System prompt correctly counts multiple images\n‚úì Multiple images correctly formatted\n‚úì Image descriptions include source type and cell index\n‚úì System prompt works without images\n‚úì No image blocks when no images provided\n‚úì System prompt handles None images gracefully\n‚úì Image block structure matches Anthropic API format\n‚úì GIF images are correctly handled\n\n‚úÖ All multimodal tests passed!\n",
  "exitCode": 0
}
```

## Assistant

Now let's run the TypeScript tests:

**Tool Use:** `Bash`

```json
{
  "cmd": "pnpm test -- --testPathPattern=imageExtraction 2>&1 | head -80",
  "cwd": "/Users/hamel/git/ai-jup"
}
```

## User

**Tool Result:** `toolu_vrtx_01HZQuf9CSqsCDWkLzv9Jbgp`

```
{
  "output": "/Users/hamel/.nvm/versions/node/v20.17.0/lib/node_modules/corepack/dist/lib/corepack.cjs:22373\n              throw new UsageError(`This project is configured to use ${result.spec.name} because ${result.target} has a \"packageManager\" field`);\n                    ^\n\nUsageError: This project is configured to use yarn because /Users/hamel/git/ai-jup/package.json has a \"packageManager\" field\n    at Engine.findProjectSpec (/Users/hamel/.nvm/versions/node/v20.17.0/lib/node_modules/corepack/dist/lib/corepack.cjs:22373:21)\n    at async Engine.executePackageManagerRequest (/Users/hamel/.nvm/versions/node/v20.17.0/lib/node_modules/corepack/dist/lib/corepack.cjs:22404:24)\n    at async Object.runMain (/Users/hamel/.nvm/versions/node/v20.17.0/lib/node_modules/corepack/dist/lib/corepack.cjs:23096:5) {\n  clipanion: { type: 'usage' }\n}\n\nNode.js v20.17.0\n",
  "exitCode": 0
}
```

## Assistant

**Tool Use:** `Bash`

```json
{
  "cmd": "yarn test -- --testPathPattern=imageExtraction 2>&1 | head -100",
  "cwd": "/Users/hamel/git/ai-jup"
}
```

## User

**Tool Result:** `toolu_vrtx_01UgTRVdcePjwPvPdF4D5tSR`

```
{
  "output": "yarn run v1.22.22\nwarning From Yarn 1.0 onwards, scripts don't require \"--\" for options to be forwarded. In a future version, any explicit \"--\" will be forwarded as-is to the scripts.\n$ jest --testPathPattern=imageExtraction\ntestPathPattern:\n\n  Option \"testPathPattern\" was replaced by \"--testPathPatterns\". \"--testPathPatterns\" is only available as a command-line option.\n\n  Please update your configuration.\n\n  CLI Options Documentation:\n  https://jestjs.io/docs/cli\n\nerror Command failed with exit code 1.\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\n",
  "exitCode": 0
}
```

## Assistant

**Tool Use:** `Bash`

```json
{
  "cmd": "yarn test -- --testPathPatterns=imageExtraction 2>&1 | head -100",
  "cwd": "/Users/hamel/git/ai-jup"
}
```

## User

**Tool Result:** `toolu_vrtx_01DcJ4kXNgCRjR5AiTjPMtf2`

```
{
  "output": "yarn run v1.22.22\nwarning From Yarn 1.0 onwards, scripts don't require \"--\" for options to be forwarded. In a future version, any explicit \"--\" will be forwarded as-is to the scripts.\n$ jest --testPathPatterns=imageExtraction\nPASS src/imageExtraction.test.ts\n  Image extraction from code cell outputs\n    ‚úì should extract PNG image from output data (1 ms)\n    ‚úì should extract JPEG image from output data (1 ms)\n    ‚úì should extract GIF image from output data\n    ‚úì should prefer PNG over JPEG when both present\n    ‚úì should not extract unsupported image types (1 ms)\n    ‚úì should not extract non-string image data\n    ‚úì should handle empty output data\n    ‚úì should handle text-only output data\n  Image extraction from markdown cell attachments\n    ‚úì should extract single attachment (1 ms)\n    ‚úì should extract multiple attachments\n    ‚úì should handle attachment with multiple MIME types\n    ‚úì should skip unsupported attachment types (1 ms)\n    ‚úì should handle empty attachments\n    ‚úì should handle invalid attachment structure\n  IImageContext structure\n    ‚úì should have correct fields for output images\n    ‚úì should have correct fields for attachment images (1 ms)\n  Multiple outputs accumulation\n    ‚úì should accumulate images from multiple cells\n\nTest Suites: 1 passed, 1 total\nTests:       17 passed, 17 total\nSnapshots:   0 total\nTime:        0.972 s\nRan all test suites matching imageExtraction.\nDone in 1.30s.\n",
  "exitCode": 0
}
```

## Assistant

All tests pass. Now let's run the TypeScript build to check for any type errors:

**Tool Use:** `tb__jupyter-build`

```json
{
  "action": "check-types"
}
```

## User

**Tool Result:** `toolu_vrtx_01QxvEVkV3a4uqMrH5PydDau`

```
{
  "output": "============================================================\nTYPE CHECKING (tsc --noEmit)\n============================================================\n\nüîß Running: npx tsc --noEmit\n--------------------------------------------------\n\n‚úÖ No type errors found!\n",
  "exitCode": 0
}
```

## Assistant

Excellent! Now let's run the full test suite to make sure nothing is broken:

**Tool Use:** `Bash`

```json
{
  "cmd": "python -m pytest tests/test_handlers.py tests/test_multimodal.py -v 2>&1 | head -50",
  "cwd": "/Users/hamel/git/ai-jup"
}
```

## User

**Tool Result:** `toolu_vrtx_01LMM9WvE1wpJCzsWkNyVgYC`

```
{
  "output": "============================= test session starts ==============================\nplatform darwin -- Python 3.12.10, pytest-8.4.2, pluggy-1.6.0 -- /Users/hamel/.venv/bin/python\ncachedir: .pytest_cache\nrootdir: /Users/hamel/git/ai-jup\nconfigfile: pyproject.toml\nplugins: libtmux-0.39.0, playwright-0.7.2, anyio-4.11.0, galileo-core-3.16.0, langsmith-0.3.4, asyncio-1.3.0, opik-1.8.55, base-url-2.1.0\nasyncio: mode=Mode.AUTO, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 9 items\n\ntests/test_handlers.py::test_build_system_prompt PASSED                  [ 11%]\ntests/test_handlers.py::test_build_tools PASSED                          [ 22%]\ntests/test_multimodal.py::test_build_system_prompt_with_images PASSED    [ 33%]\ntests/test_multimodal.py::test_build_system_prompt_with_multiple_images PASSED [ 44%]\ntests/test_multimodal.py::test_build_system_prompt_image_descriptions PASSED [ 55%]\ntests/test_multimodal.py::test_build_system_prompt_no_images PASSED      [ 66%]\ntests/test_multimodal.py::test_build_system_prompt_images_with_none PASSED [ 77%]\ntests/test_multimodal.py::test_image_block_structure PASSED              [ 88%]\ntests/test_multimodal.py::test_gif_image_support PASSED                  [100%]\n\n=============================== warnings summary ===============================\ntests/test_handlers.py::test_build_system_prompt\n  /Users/hamel/.venv/lib/python3.12/site-packages/jupyter_server/base/handlers.py:24: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n  given by the platformdirs library.  To remove this warning and\n  see the appropriate new directories, set the environment variable\n  `JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n  The use of platformdirs will be the default in `jupyter_core` v6\n    from jupyter_core.paths import is_hidden\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n========================= 9 passed, 1 warning in 0.11s =========================\n",
  "exitCode": 0
}
```

## Assistant

All tests pass. Now let me also run a quick build to make sure everything compiles:

**Tool Use:** `tb__jupyter-build`

```json
{
  "action": "build"
}
```

## User

**Tool Result:** `toolu_vrtx_018k3mbTjQmCBY7vrt3biUSg`

```
{
  "output": "============================================================\nBUILDING AI-JUP (DEVELOPMENT)\n============================================================\n\nüîß Running: jlpm build\n--------------------------------------------------\nBuilding extension in .\n\nCompilation starting‚Ä¶\n\nCompilation finished\n\nasset lib_index_js.2f65a77d339681ebe30c.js 67.1 KiB [emitted] [immutable] 1 related asset\nasset remoteEntry.beb96d6408f1e688c2e3.js 28.2 KiB [emitted] [immutable] (name: ai-jup) 1 related asset\nasset style_index_js.7e284b0d175340c140e3.js 27.4 KiB [emitted] [immutable] 1 related asset\nruntime modules 18.9 KiB 13 modules\nbuilt modules 74.4 KiB (javascript) 462 bytes (consume-shared) 42 bytes (share-init) [built]\n  javascript modules 74.4 KiB\n    modules by path ./lib/*.js 55.2 KiB 9 modules\n    modules by path ./node_modules/ 8.58 KiB 8 modules\n    modules by path ./style/ 10.6 KiB 3 modules\n    container entry 42 bytes [built] [code generated]\n  consume-shared-module modules 462 bytes\n    modules by path consume shared module (default) @jupyterlab/*.10 (singleton) 336 bytes 8 modules\n    modules by path consume shared module (default) @lumino/*.0 (singleton) 84 bytes\n      consume shared module (default) @lumino/signaling@^2.0.0 (singleton) 42 bytes [built] [code generated]\n      consume shared module (default) @lumino/coreutils@^2.0.0 (singleton) 42 bytes [built] [code generated]\n    consume shared module (default) react@^18.2.0 (singleton) 42 bytes [built] [code generated]\n  provide shared module (default) ai-jup@0.1.0 = ./lib/index.js 42 bytes [built] [code generated]\nwebpack 5.104.1 compiled successfully in 208 ms\n\n============================================================\n‚úÖ Build completed successfully!\n   üìÅ TypeScript output: 16 files in lib/\n   üìÅ Lab extension: /Users/hamel/git/ai-jup/ai_jup/labextension\n============================================================\n",
  "exitCode": 0
}
```

## Assistant

**Tool Use:** `todo_write`

```json
{
  "todos": [
    {
      "id": "1",
      "content": "Update IPromptContext in tokens.ts to include images array",
      "status": "completed"
    },
    {
      "id": "2",
      "content": "Update _gatherContext() in promptCell.ts to extract images from code cell outputs",
      "status": "completed"
    },
    {
      "id": "3",
      "content": "Update _gatherContext() to extract attachments from markdown cells",
      "status": "completed"
    },
    {
      "id": "4",
      "content": "Update backend handlers.py to format messages with image content blocks",
      "status": "completed"
    },
    {
      "id": "5",
      "content": "Add LiteLLM integration with vision support check",
      "status": "todo"
    },
    {
      "id": "6",
      "content": "Write TypeScript tests for image extraction",
      "status": "completed"
    },
    {
      "id": "7",
      "content": "Write Python tests for multimodal message formatting",
      "status": "completed"
    },
    {
      "id": "8",
      "content": "Run tests and fix any issues",
      "status": "completed"
    }
  ]
}
```

## User

**Tool Result:** `toolu_vrtx_01C9nEWXHAFBQXR3rRbc4e9J`

```
[{"id":"1","content":"Update IPromptContext in tokens.ts to include images array","status":"completed"},{"id":"2","content":"Update _gatherContext() in promptCell.ts to extract images from code cell outputs","status":"completed"},{"id":"3","content":"Update _gatherContext() to extract attachments from markdown cells","status":"completed"},{"id":"4","content":"Update backend handlers.py to format messages with image content blocks","status":"completed"},{"id":"5","content":"Add LiteLLM integration with vision support check","status":"todo"},{"id":"6","content":"Write TypeScript tests for image extraction","status":"completed"},{"id":"7","content":"Write Python tests for multimodal message formatting","status":"completed"},{"id":"8","content":"Run tests and fix any issues","status":"completed"}]
```

## Assistant

### Summary

I've implemented Tier 1 multimodal support for ai-jup. Here's what was done:

#### Changes Made

1. **[src/tokens.ts](file:///Users/hamel/git/ai-jup/src/tokens.ts#L114-L130)** - Added `IImageContext` interface and updated `IPromptContext` to include optional `images` array

2. **[src/promptCell.ts](file:///Users/hamel/git/ai-jup/src/promptCell.ts#L205-L330)** - Updated `_gatherContext()` with:
   - `_extractImagesFromCodeCell()` - Extracts `image/png`, `image/jpeg`, `image/gif` from code cell outputs
   - `_extractImagesFromMarkdownCell()` - Extracts images from markdown cell attachments (stored in cell metadata)

3. **[ai_jup/handlers.py](file:///Users/hamel/git/ai-jup/ai_jup/handlers.py#L411-L502)** - Updated `_build_system_prompt()` to:
   - Accept images parameter
   - Add descriptive text about images to system prompt
   - Include image content blocks in Anthropic's multimodal format

#### Tests Added

- **[src/imageExtraction.test.ts](file:///Users/hamel/git/ai-jup/src/imageExtraction.test.ts)** - 17 tests for TypeScript image extraction logic
- **[tests/test_multimodal.py](file:///Users/hamel/git/ai-jup/tests/test_multimodal.py)** - 7 tests for Python multimodal message formatting

#### How It Works

When you run a prompt cell:
1. All preceding cells are scanned
2. Code cell outputs are checked for `image/png`, `image/jpeg`, `image/gif` MIME types
3. Markdown cell attachments are extracted from metadata
4. Images are sent to the LLM as base64-encoded content blocks
5. The LLM can now "see" matplotlib plots, PIL images, and embedded images in markdown

#### Future Work (LiteLLM)

LiteLLM integration (TODO #5) would add:
- Provider abstraction for multiple LLM backends
- `litellm.supports_vision(model)` check before sending images
- Support for GPT-4V, Gemini, and other vision-capable models

## User

why did we skip the LiteLLM Todo btw?  Please tell me

## Assistant

I skipped LiteLLM because:

1. **Current setup works** - The backend already uses Anthropic SDK directly, and Claude models support multimodal natively. The Tier 1 images feature works end-to-end right now.

2. **Dependency change** - Adding LiteLLM would require adding it to `pyproject.toml` dependencies and potentially refactoring the handler code significantly (replacing `anthropic.Anthropic` with `litellm.completion`).

3. **Scope creep concern** - You asked specifically for Tier 1 multimodal support with tests. LiteLLM is a provider abstraction layer - useful for supporting multiple LLM providers (OpenAI, Gemini, etc.), but not strictly required for multimodal to work with Claude.

4. **Testing complexity** - LiteLLM integration would need its own tests and validation across different providers.

**Should I implement it now?** It would give you:
- Support for 100+ models via unified API
- `litellm.supports_vision(model)` to check if a model can handle images before sending them
- Easy switching between Claude, GPT-4V, Gemini Pro Vision, etc.

Let me know if you want me to add it - it's a meaningful enhancement but separate from the core multimodal extraction logic we just built.

## Todos
- [x] Update IPromptContext in tokens.ts to include images array
- [x] Update _gatherContext() in promptCell.ts to extract images from code cell outputs
- [x] Update _gatherContext() to extract attachments from markdown cells
- [x] Update backend handlers.py to format messages with image content blocks
- [ ] Add LiteLLM integration with vision support check
- [x] Write TypeScript tests for image extraction
- [x] Write Python tests for multimodal message formatting
- [x] Run tests and fix any issues
